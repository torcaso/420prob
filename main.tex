\documentclass[12pt]{article}
%\pagestyle{empty}
\setlength{\topmargin}{-1 in}
\setlength{\textheight}{9.5 in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.25in}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,xcolor,rotating,mathrsfs}
%\usepackage{amsmath}

%\setlength{\evensidemargin}{\oddsidemargin}
\begin{document}%\large

\begin{center}May 3, 2023\end{center}

\vskip 2 in

\begin{center}{\Large\bf Probability course notes}\end{center}
\bigskip
\begin{center}by\end{center}
\begin{center}Fred Torcaso\end{center}

\newpage



\begin{center}{\bf \Large CONTENTS.}\end{center}

\vskip .5 in

\noindent {\bf I. Probability at the experiment level and Combinatorics.}

$
\begin{array}{lr}
1.\mbox{ Definitions, notations, basic examples} & \qquad\qquad\qquad\ \ \ \ \ \pageref{sec:experimentlevel} \\
2.\mbox{ Computing probabilities, the classical probability measure} & \pageref{sec:computingprobabilities}\\
3.\mbox{ The basic counting rule, representing sample points} & \pageref{sec:basiccountingrule}\\
4.\mbox{ Sampling with replacement, examples} & \pageref{sec:samplingwithreplacement}\\
5.\mbox{ Orderings, permutations of objects, examples} & \pageref{sec:orderings}\\
6.\mbox{ Anagrams} & \pageref{sec:anagrams}\\
\quad \mbox{ Exercises} & \pageref{problemset1}\\
7.\mbox{ Combinations, binomial coefficients} & \pageref{sec:combinations}\\
\quad \mbox{ Exercises} & \pageref{problemset2}\\
8.\mbox{ Multinomial coefficients} & \pageref{multinomialcoeff1}\\
\quad \mbox{ Exercises} & \pageref{problemset3}\\
9.\mbox{ Stars-and-bars counting} & \pageref{starsandbars2}\\
\quad \mbox{ Exercises} & \pageref{problemset4}\\
\end{array}
$
\bigskip

\noindent {\bf II. Probability measures, Axioms and consequences.}

$
\begin{array}{lr}
1.\mbox{ Motivation for axioms}\hskip 2.55 in & \qquad\qquad\qquad\ \ \ \ \pageref{axiommotivation} \\
2.\mbox{ Axioms of Probability} & \pageref{axioms1}\\
3.\mbox{ Properties of probability measures} & \pageref{pmproperties}\\
\ \ \mbox{ Complementary rule} & \pageref{complementaryrule}\\
\ \ \mbox{ Monotonicity} & \pageref{monotonicity}\\
\ \ \mbox{ Subadditivity, Boole's inequality} & \pageref{subadd1}\\
\ \ \mbox{ Inclusion-exclusion rules, Boole-Bonferroni inequalities} & \pageref{inclusionexclusionrules}\\
\ \ \mbox{ DeMorgan's rules} & \pageref{demorgan}\\
4.\mbox{ Continuity of probabililty measures.} & \pageref{s:continuityprobmeas}\\
5.\mbox{ Conditional probability} & \pageref{conditionalprob1}\\
6.\mbox{ Bayes rule} & \pageref{s:bayesrule}\\
7.\mbox{ Independence} & \pageref{independence1}\\
\end{array}
$
\bigskip

\newpage


\noindent {\bf III. Random variables, discrete random variables.}

$
\begin{array}{lr}
1.\mbox{ Random variables, concepts, notations, definitions}\qquad \qquad \qquad \quad \quad \quad\ \ \ \  & \pageref{rvs1} \\
2.\mbox{ Types of random variables} & \pageref{typesofrvs1}\\
3.\mbox{ Discrete random variables} & \pageref{d:discreterv}\\
4.\mbox{ Some useful discrete probability distributions} & \\
\ \ \mbox{ Bernoulli$(p)$} & \pageref{d:bernoullip}\\
\ \ \mbox{ Hypergeometric} & \pageref{d:hypergeom}\\
\ \ \mbox{ binomial$(n,p)$} & \pageref{d:binomialnp}\\
\ \ \mbox{ Poisson$(\lambda)$} & \pageref{d:poissonlambda}\\
\ \ \mbox{ geometric$(p)$} & \pageref{d:geometricp}\\
\ \ \mbox{ negative binomial$(r,p)$} & \pageref{d:negbinom}\\
5.\mbox{ Expected values of discrete rvs} & \pageref{d:expectedvaluediscrete}\\
6.\mbox{ Properties of expected values} & \pageref{expectedvalueproperties}\\
7.\mbox{ Expected values: existence vs. non-existence} & \pageref{expectedvaluesexistence}\\
8.\mbox{ Law of the Unconscious Statistician} & \pageref{lotus}\\
9.\mbox{ Variance} & \pageref{variance}\\
10.\mbox{ Moment generating functions - part 1} & \pageref{mgf-part1}\\
11.\mbox{ Cumulative distribution functions and properties} & \pageref{cdfs}\\
\end{array}$
\bigskip

\noindent {\bf IV. Continuous random variables.}

$
\begin{array}{lr}
1.\mbox{ Continuous random variables, probability density functions}\ \ \  & \pageref{continuousrvs} \\
2.\mbox{ The exponential distribution} & \pageref{expdist}\\
3.\mbox{ Expected values of continuous random variables} & \pageref{expectedvaluecontinuous}\\
4.\mbox{ The uniform distribution} & \pageref{d:uniformabdist}\\
5.\mbox{ Moment generating functions - part 2} & \pageref{mgf-part2}\\
6.\mbox{ Other moment quantities: {\em z}-score, centered moments, skewness, kurtosis} \ \ & \pageref{othermomemtquantities}\\
7.\mbox{ A digression: Euler's Gamma function} & \pageref{d:eulergamma}\\
8.\mbox{ The Gamma-family of pdfs} & \pageref{gammapdf}\\
%9.\mbox{ Application: The Poisson process} & \pageref{poissonprocess1}\\
9.\mbox{ The Normal (Gaussian) distribution} & \pageref{normaldist}\\
10.\mbox{ Global properties of the Normal distribution} & \pageref{propertiesofnormal}\\
11.\mbox{ Local properties of the Normal distribution} & \pageref{normallocalproperties}\\
12.\mbox{ The CDF method -- univariate case} & \pageref{cdfmethod}\\
\end{array}$

\newpage


\noindent {\bf V. Jointly distributed random variables.}

$
\begin{array}{lr}
1.\mbox{ Jointly discrete, joint pmfs}\ \, \hskip 3.25 in & \pageref{jointlydistributed} \\
2.\mbox{ Marginal pmf} & \pageref{marginalpmf}\\
3.\mbox{ The multivariate hypergeometric distribution} & \pageref{multivariatehypergeom}\\
4.\mbox{ The multinomial distribution} & \pageref{multinomialdist}\\
5.\mbox{ Jointly continuous, joint pdfs} & \pageref{jointlycontinuous}\\
6.\mbox{ Marginal pdf} & \pageref{s:marginalpdf}\\
7.\mbox{ Independence of random variables} & \pageref{independentervs}\\
8.\mbox{ Convolution} & \pageref{s:convolution}\\
9.\mbox{ Law of the Unconscious Statistician re-visited} & \pageref{s:lotus}\\
10.\mbox{ Moment generating functions - part 3} & \pageref{mgfpart3}\\
11.\mbox{ Conditional distributions} & \pageref{s:conditionaldistributions}\\
12.\mbox{ Method of Jacobians} & \pageref{methodofjacobians}\\
13.\mbox{ Ordered statistics} & \pageref{orderstatistics}\\
14.\mbox{ Symmetry and exchangeability} & \pageref{exchangeability}\\
15.\mbox{ DeFinetti's theorem} & \pageref{definettistheorem}\\
16.\mbox{ Application: P\'{o}lya's urn} & \pageref{polyaurn}\\
\end{array}$

\bigskip
\noindent {\bf VI. First- and second-order results.}

$
\begin{array}{lr}
1.\mbox{ Expectations: linearity of expectation} & \hskip 1.8225 in \pageref{linearityofexpectation}\\
2.\mbox{ Covariance and its properties} & \pageref{covariance}\\
3.\mbox{ Variance of a sum of random variables} & \pageref{varianceofasum}\\
4.\mbox{ The Cauchy--Schwarz inequality and correlation} & \pageref{cauchyschwarz}\\
5.\mbox{ Conditional expectation} & \pageref{conditionalexpectation}\\
6.\mbox{ Law of total expectation} & \pageref{lawoftotalexpectation}\\
7.\mbox{ Conditional variance} & \pageref{conditionalvariance}\\
8.\mbox{ Law of total variance} & \pageref{lawoftotalvariance}\\
9.\mbox{ The bivariate Normal distribution} & \pageref{bivariatenormaldist}\\
10.\mbox{ The multivariate Normal distribution} & \pageref{multivariatenormaldist}\\
\end{array}
$

\bigskip

\noindent {\bf VII. Inequalities and limit theorems.}

$
\begin{array}{lr}
1.\mbox{ The Markov inequality} \hskip 3.6 in & \pageref{markovinequality}\\
2.\mbox{ The Chebyshev inequality} & \pageref{chebyshevinequality}\\
3.\mbox{ The weak law of large numbers} & \pageref{wlln}\\
4.\mbox{ Monte-carlo method*} & \pageref{montecarlomethod}\\
5.\mbox{ The central limit theorem} & \pageref{clt}\\
6.\mbox{ The strong law of large numbers*} & \pageref{slln}\\
\end{array}
$

\vskip 1 in

\noindent * can be skipped on first reading.

\newpage

\noindent These notes were developed over five semesters (Spring 2021 through Spring 2023)
for the course {\em Introduction to Probability} at the Johns Hopkins University.
This course is a comprehensive treatment of applied probability taught at the senior level.


\newpage

\begin{center}{\bf \Large I. Probability at the experiment level and Combinatorics.}\end{center}

\vskip 1 in

\noindent The point of this section is to carefully and rigorously explain
the mathematics of counting in a systematic manner.
Equal emphasis will be on modeling, counting and
computing probability in the case of experiments
leading to a finite number of
equally likely outcomes.  Many examples are provided.\\

\noindent In future sections we will need to come back to these roots. We will also later
develop tools that, when combined with the combinatorics we learn here, can greatly increase our
abilities to compute probability at the experiment level.




\newpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{center}{\bf 1. Probability - introduction.}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent {\bf Experiment level probability.}\label{sec:experimentlevel}\\


\noindent An {\bf \em experiment}\label{d:experiment} is a repeatable process of observation where every possible outcome
is known in advance.\\

\noindent The individual outcomes that can happen are called {\bf \em sample points}\label{d:samplepoint}.  We denote a generic
sample point by the lower-case Greek letter omega $\omega$.\\

\noindent The collection of all sample points is called the {\bf \em sample space}\label{d:samplespace} denoted by $\Omega$. Thus,
$\omega\in \Omega$ means $\omega$ is one of the possible outcomes (sample points) of the experiment.\\

\vskip .5 in

\noindent {\bf Basic experiments:}\\
Please understand that in the examples below specific choices were made to {\em model}
the
sample spaces, i.e., as a way to
{\bf\em specify}\label{specify} what the experimenter observes as the
outcome.\medskip

\noindent {\bf 1.1.} Flip a coin once: $\Omega = \{h,t\}$.

\noindent {\bf 1.2.} Flip a coin twice: $\Omega = \{hh,ht,th,tt\}$.

\noindent {\bf 1.3.} Flip a coin thrice: $\Omega = \{hhh,hht,hth,htt,thh,tht,tth,ttt\}$, etc.

\noindent {\bf 1.$n$.} Flip a coin $n>1$ times: $\Omega = \{x_1x_2\cdots x_n: x_i\in\{h,t\}\mbox{ for all }i\}$.

\noindent We may think of these experiments as repeated trials of flipping a coin once.\medskip

\noindent {\bf 2.1.} Roll a single 6-sided die: $\Omega = \{1,2,3,4,5,6\}$.

\noindent {\bf 2.2.}\label{2dicesamplespace} Roll a 6-sided die twice ($xy$ means $x$ on 1st roll, $y$ on 2nd roll):

$\begin{array}{rrrrrl}\Omega=\{11,&12,&13,&14,&15,&16,\\
21,&22,&23,&24,&25,&26,\\
31,&32,&33,&34,&35,&36,\\
41,&42,&43,&44,&45,&46,\\
51,&52,&53,&54,&55,&56,\\
61,&62,&63,&64,&65,&66\}\end{array}$

\noindent Again, we may wish to think of this experiment as two trials of rolling the die once.\medskip

\noindent {\bf 3.1.}\label{experiment31} There are 4 slips of paper (numbered 1,2,3,4) in a hat.
The experiment is to
draw all slips of paper one at a time noting the number on each.

$\begin{array}{rrrrrl}\Omega=\{1234,&1243,&1324,&1342,&1423,&1432,\\
2134,&2143,&2314,&2341,&2413,&2431,\\
3124,&3142,&3214,&3241,&3412,&3421,\\
4123,&4132,&4213,&4231,&4312,&4321\}.\end{array}$\label{4!permutations}

\noindent {\bf 3.2.} There are 4 slips of paper (numbered 1,2,3, and 4) in a hat.
The experiment is to
draw {\em two} slips of paper one at a time noting the number on each.

$\begin{array}{rrl}\Omega=\{12,&13,&14,\\
21,&23,&24,\\
31,&32,&34,\\
41,&42,&43\}.\end{array}$\label{4fallingfactorial2}\medskip


\noindent {\bf 4.} There are 4 slips of paper (numbered 1,2,3, and 4) in a hat.
The experiment is to draw
{\em two} slips of paper at once noting which two you selected. We neglect order.

$\Omega = \Big\{ \{1,2\},\{1,3\},\{1,4\},\{2,3\},\{2,4\},\{3,4\}\Big\}$.



\newpage

\noindent An {\bf \em event} is a subset of $\Omega$.
We usually think of an event as a particular subcollection
of $\Omega$ containing sample points that are all of interest to the experimenter.\\

\noindent When an experiment is performed, an $\omega\in \Omega$ is produced. If $A$ is an event, we say
{\bf \em $A$ occurs} to mean $\omega\in A$, i.e., the $\omega$ that chance produced is a member of $A$.

\includegraphics*[-130,0][200,120]{venndiagram.jpg}
\begin{center}{\bf Figure.} A Venn diagram visualization of an event $A$ in $\Omega$.\end{center}


\noindent We imagine the box ($\Omega$) filled with all its sample points.
The event in yellow ($A$) corrals a certain group of these sample points.\\

\vskip .5 in



\noindent {\bf Some interesting events:}\\

\noindent In experiment 1.3,
the event $H_i$ the $i$th toss is a head:

$H_1=\{hhh,hht,hth,htt\}$ \quad $H_2=\{hhh,hht,thh,tht\}$ \quad $H_3=\{hhh,hth,thh,tth\}$;

\noindent The event $D$ the first and last toss differ in parity:
$D=\{hht,htt,thh,tth\}$.\\


\noindent In experiment 2.2,
the event $S_7$ the sum total is 7: $S_7 = \{16,25,34,43,52,61\}$;

\noindent The event $A$ a six on first roll: $A = \{61,62,63,64,65,66\}$;

\noindent The event$^*$ $B$ a six is rolled: $B = \{16,26,36,46,56,66,61,62,63,64,65\}$;

\noindent $^*$Notice in this example that a six is rolled means {\em at least} one six is rolled.

\noindent The event $C_1$ the red die (or the first die) shows a six:
$C_1 = \{61,62,63,64,65,66\}$.

\noindent The event $C_2$ the green die (or the second die) shows a six:
$C_1 = \{16,26,36,46,56,66\}$.\\

\noindent In experiment 3.1,
the event $I$ that even numbers are in increasing order and odd numbers are in increasing order :

$I = \{1234, 1243, 1324, 2134,2143,2413\}$.\\

\noindent In experiment 3.2, the event $T$ the sum of the numbers drawn is divisible by 3:

$B = \{12,21,24,42\}$.\\


\noindent In experiment 4,
the event $F$ the number 4 is selected: $T = \Big\{\{1,4\},\{2,4\},\{3,4\} \Big\}$.


\newpage

\noindent {\bf Computing probabilities.}\label{sec:computingprobabilities}\\

\noindent When given an experiment, we may want to understand how likely a particular event
is to occur.  That is, if $A$ is an event, then we wish to assign a number $P(A)$ that tells
us how likely the event is.
We will learn that
the computation of $P(A)$ will depend
on the experiment and the probability model chosen for it.  More on this later.  But, for now,
we introduce a historically significant way to compute $P(A)$
under the assumptions
that the sample space is comprised of {\bf\em finite and equally-likely outcomes}.\\

\vskip .5 in


\noindent {\bf The classical probability measure.}\label{classicalprobmeasure1}\\
\noindent Under the assumption that $|\Omega|$ is finite and all $\omega\in \Omega$ have the same chance of occurring, then
for any event $A\subseteq \Omega$,
$$P(A) = \dfrac {|A|}{|\Omega|}.$$
That is, $P(A)$ is just the proportion of possible outcomes that belong to $A$.
Therefore, in the case of finite and equally-likely outcomes, computing $P(A)$ boils down to just two counting problems:
count the number of things in $\Omega$, count the number of things in $A$, and take the ratio.  \medskip

\noindent In most problems that we will encounter we will want to count things by {\em avoiding} an actual count.
The secret to counting effectively will be in how we {\em model} our sample points and how we {\em think} about them.
The mathematical discipline of counting without actually doing a count is called {\bf \em combinatorics}.


\newpage


\noindent An important step to counting the number of sample points is to mathematically model your experiment that makes counting
sample points {\em easier}; for instance, in situations when we can model our experiment as a (finite) sequence of two or more stages,
where on each stage we know the number of ways that stage can be completed.\\

\vskip .5 in



\noindent {\bf The basic counting rule.}\label{sec:basiccountingrule}\\
Suppose we have a procedure consisting of $r$ stages numbered 1 thru $r$.  Stage 1 can be performed in $n_1$ ways, and, for each $k=2,\dots,r$, stage $k$ can be completed in $n_k$ ways regardless
of the way stage $j$ was completed for all $j<k$. Then the procedure can be completed in $n_1\times n_2\times\cdots\times n_r$ ways.\\

\vskip .5 in


\noindent {\bf Basic example 1.}\\
Suppose at a certain deli a {\em sandwich} is

(1) a choice of bread (one of Flatbread, Rye, or White),

(2) a choice of protein (one of Roast beef, Salami, Tuna fish, or Turkey), and

(3) a choice of cheese (one of no cheese, American, or Provolone)

\noindent There are 3 stages to make a sandwich. There are $n_1=3$ ways we can choose the bread. There are $n_2=4$ ways to choose the protein.
There are $n_3=3$ ways to choose the cheese. The number of choices at each stage does not depend on {\em which} choice was made in previous stages.
Then, there are $3\times 4\times 3 = 36$ possible sandwiches.\vskip -1.5 in
\includegraphics*[-75,0][250,300]{Sandwiches.jpg}

\noindent Pictorially, we can think of each box as an ordered 3-tuple $(x,y,z)$, where $x\in \{\mbox{Flatbread, Rye, White}\}$,
$y\in \{\mbox{Roast beef, Salami, Tuna Fish, Turkey} \}$, and $z\in \{ \mbox{No cheese, American, Provolone} \}$.\\


\noindent We can also visualize all sandwiches through a {\bf\em tree diagram} (see next page).


\newpage

\noindent The tree below has the property that all nodes within a fixed stage have the same number of arcs leaving it. This is the paradigm of the basic counting rule.

\includegraphics*[-100,0][250,400]{sandwichtree.jpg}
\begin{center}{\bf Figure.} A tree diagram of all possible sandwiches.\end{center}

\vskip .5 in


\noindent Each path from the root node to a leaf represents one sandwich.  Therefore, the number of leaves {\em is} the number of sandwiches. \\

\noindent (Stage 1) The root has 3 children.

\noindent (Stage 2) Each of these children has 4 children.

\noindent (Stage 3) Each of these children have 3 children (the leaves).

\noindent So, there
are $3\times 4\times 3$ leaves (sandwiches).\\

\noindent The next example sets up a situation where the basic counting rule cannot be applied.



\newpage




\noindent {\bf Basic example 2.}\label{electionexample}\\
Suppose 5 people Amir, Betty, Cristoff, Dahlia, and Ernesto are senior members of a club from which we need to elect a
(1) president, (2) a vice-president, and (3) a treasurer. No one can hold more than one office.  How many elections are possible?\\

\noindent An election result is a choice of president, vice-president, and treasurer.  Therefore, we think of the procedure as 3 stages:
stage 1 (say, choose a president) can be completed in 5 ways.
Stage 2 (say, choose a vice-president) can be completed in 4 ways - note that this stage can always be completed in 4 ways {\em regardless} of which person was chosen in the first stage;
finally, stage 3 (choose a treasurer) can always be completed in 3 ways once we've used up the two people for president and vice-president.
Therefore, there are $5\times 4\times 3 = 60$ possible election results. See figure below.

\includegraphics*[-110,0][300,150]{basicexample2pic2.jpg}
\begin{center}{\bf Figure.} An illustration of the basic counting rule generating a sample point.\end{center}

\vskip .5 in


\noindent {\bf Remark.} (A warning about blindly using the basic counting rule.)\\
Suppose that Amir refuses to hold office if Betty is elected president.  How many election results are possible now?
Although there may be 5 choices for president, the number of choices for vice-president (and treasurer) depends on whether or not Betty was elected president
(if Betty is elected president, then there are only 3 choices for vice-president: {\em we must remove} Amir; if Betty is not elected president, there are 4 choices for vice-president now). So, strictly speaking, this question does not fit the paradigm of the basic counting rule: the number of choices in a stage {\em depends} on
how a previous stage was completed.\\

%as there are not always the same number of ways of completing an operation at each stage.\\

\noindent Nevertheless, if Betty is elected president (one way to complete this), then there are 3 choices for vice-president and then 2 choices for treasurer as we must remove Amir from the pool.  If Betty is not elected president (4 ways to complete this), then there are any of 4 people to be vice-president (Betty doesn't mind holding office with Amir), and then 3 ways to elect a treasurer. Thus, $1\times 3\times 2 + 4\times 4\times 3 = 54$ election results.\\




\newpage





\noindent {\bf Remark.}\\
Had we modeled the procedure as choosing a treasurer in stage 1, a vice-president in stage 2, and a president in stage 3, then, of course, there are still $5\times 4\times 3=60$ unrestricted election results; the only difference is the ordered 3-tuple $(x,y,z)$ has $x$ as {\em treasurer} instead of president like before, $y$ as vice-president, and $z$ now as the president.  Having chosen to model the procedure in this fashion might have made answering the question in the previous remark more difficult; however, it can make answering the following question easier:

Suppose that Dahlia refuses to be vice-president if Ernesto is treasurer (but all

other possibilities are fine). How many election results?

\noindent With this alternate model for the procedure, when Ernesto is chosen as treasurer, there are only 3 choices of vice-president (Dahlia must be removed from the pool) and there are 3 choices for president (as Dahlia is okay with the presidency when Ernesto is the treasurer); on the other hand, if Ernesto is not treasurer (4 ways to complete this), then there are 4 ways to choose the
vice-president and 3 ways to choose the president.  Therefore,
there are $1\times 3\times 3 + 4\times 4\times 3 = 57$ possible election results.\\

\noindent Perhaps an easier way to have solved the last problem was to recognize that out of the total
$5\times 4\times 3 = 60$ unrestricted election results we just need to remove
all those ordered 3-tuples of the form $(e,d,z)$, where $e$ is Ernesto, $d$ is Dahlia, and $z$ can be any of the 3 remaining people as president, and clearly there are 3 such ``bad" election results. So, we remove these 3 bad ones from the 60: $60-3=57$.\\

\vskip .5 in



\noindent {\bf Advice:}\\
The choice of model can help simplify an approach to a problem. It is always beneficial to visualize yourself
actually performing the experiment!
The specific questions can help us choose the model we want to use to answer these questions.\\





% and a set of questions that we must answer are given,

\noindent {\bf Example and discussion.}\label{30ballsexample}\\
\noindent We have an urn with 30 balls:
16 are red, 8 are green, and 6 are yellow.
We draw three balls from this urn without replacing so that at each stage (draw) any of the existing balls in the urn is equally likely to be chosen.
Find the probability that the first and third draws are both green. How about both the same color.\\

\noindent {\em Where do we start?}\\







\noindent Well\dots visualize yourself performing this experiment as though you were the person asking these questions.
Maybe start by drawing a picture of the urn on a piece of paper.
Something like this:

\includegraphics*[-150,0][100,80]{urnwith30balls.jpg}

\newpage

\noindent Since we are interested in a probability here,
we should first settle on a sample space $\Omega$.
Since the event of interest is green on first and third draw, it should be clear that we need a sample space where we know the color ball at each draw.
So treating the draws as stages we might model the sample points as ordered 3-tuples $(x,y,z)$,
where $x$ is the color drawn first, $y$ the color drawn second, and $z$ the color drawn third. But now we have an issue:
there are more red balls than green, so it appears that it is more likely to select red at any draw than either of the other colors. E.g., $(r,r,r)$ is more likely than, say, $(g,g,g)$.  But, equally-likely here should mean that any of the 30 balls has the same chance of being selected first, then any of the remaining 29 balls should have the same chance of being selected second, and any of the remaining 28 balls should have the same chance of being selected third.  So, let's mark the 16 red balls as $r_1,r_2,\dots,r_{16}$, the 8 green balls as $g_1,g_2,\dots,g_8$ and the 6 yellow balls as $y_1,y_2,\dots,y_6$, and think of the urn as
$$U:=\{r_1,r_2,\dots,r_{16},g_1,g_2,\dots,g_8,y_1,y_2,\dots,y_6\}.$$
We get around the issue now by modeling $\omega$ as $(x,y,z)$ where $x\in U$, $y\in U-\{x\}$, and $z\in U-\{x,y\}$. The sample space
of all these $\omega$ will be equally-likely. {\em What's $|\Omega|$?} \\

\noindent This fits the paradigm of the basic counting rule: the answer is $|\Omega|=30\times 29\times 28$.\\

\noindent Now we ask: with this model of $\Omega$, {\em is it easy to count the number of sample points in the event $A$ that the first and third draw are both green?}\\

\noindent Try to envision what sample points look like in $A$.  Also, think about the process that leads to an $\omega\in A$:
namely, pick a green first, pick anything remaining second, and pick a green third. Formally,
by setting $G=\{g_1,g_2,\dots,g_8\}$,
$$A = \Big\{(x,y,z):x\in G, y\in U-\{x\},z\in G-\{x,y\}\Big\}.$$
\noindent {\em Does this fit the paradigm of the basic counting rule?}  Not quite.  It's very much like the election results problem. The number of green marbles that remain in the urn at stage 3 depends on the color of the ball chosen at stage 2.  After one of the 8 green balls is chosen at stage 1, there are 29 remaining balls (7 are green and 22 are not green).  If one of the 7 greens is selected at stage 2, then there are 6 greens that can be selected at stage 3.  If one of the 22 non-greens is chosen at stage 2, then there are 7 greens that can be selected at stage 3.  Thus,
$$|A| = 8\cdot 7\cdot 6 + 8\cdot 22\cdot 7,$$
and our probability is
$$P(A) = \frac {8\cdot 7\cdot 6 + 8\cdot 22\cdot 7}{30\cdot 29\cdot 28}.$$

\bigskip

\noindent There's now something worth noting in this calcuation: $8\cdot 7\cdot 6 + 8\cdot 22\cdot 7 = 8\cdot 7\cdot (6+22) = 8\cdot 7\cdot 28$ so that the probability above becomes
$$P(A) = \frac {8\cdot 7\cdot 28}{30\cdot 29\cdot 28} = \frac {8\cdot 7}{30\cdot 29},$$
which counts the number of sample points where the first and {\em second} are green, i.e., there are the same number of sample points in $\Omega$ where the
first and second are green as there are sample points where the first and third are green.
This peculiarity will be discussed in a bit (page \pageref{exchangeability2}) -- it is called {\bf\em exchangeability}\label{exchangeability1}. But, with this it follows that the probability the first and third are green is the same as the probability the first and second are green which is:
$$\frac {8\cdot 7}{30\cdot 29}.$$\\


\noindent I'll leave you to think about why the event $C$ that the first and third are the same color has probability
\begin{eqnarray*}
P(C) &=&
\frac {(16\cdot 15\cdot 14 + 16\cdot 14\cdot 15)+(8\cdot 7\cdot 6 + 8\cdot 22\cdot 7)+(6\cdot 5\cdot 4 + 6\cdot 24\cdot 5)}{30\cdot 29\cdot 28}\\
&=&
\frac {(16\cdot 15)+(8\cdot 7)+(6\cdot 5)}{30\cdot 29}.\end{eqnarray*}
The second equality here emphasizes that the probability is really the same as the probability the first two balls drawn are the same color.\\





\newpage

\noindent {\bf Important special cases of the basic counting rule.}\\

\noindent {\bf 1.} {\bf Sampling with replacement.}\label{sec:samplingwithreplacement}\\
From $n$ distinct objects, how many sequences of length $k\ge 1$ can be made where we allow repetition of objects.

There are $\underbrace{n\times n\times \cdots \times n}_{k\ \mbox{\small times}} = n^k$ possible selections.\\

\vskip .5 in

\noindent {\bf Examples.}\medskip

\noindent $\bullet$ If you toss a coin $k$ times, how many sequences are possible?\\

\noindent Solution: Here $n=2$, $\{h,t\}$, therefore, there are $2^k$ possible sequences.\\

\noindent $\bullet$ If you roll a 6-sided die $k$ times$^*$, how many sequences are possible?\\

\noindent Solution: Here $n=6$, $\{1,2,3,4,5,6\}$, therefore, there are $6^k$ possible die rolls.\\
$^*$ We keep track of the order of the rolls or, equivalently, we think of the $k$ dice as different
colors and observe the up-face as well as which color it appeared on.  For example, if $k=2$, then
we can think that the first roll as the result on the red die and the second roll as the result on the green die.
In this way, when we roll these (distinguishable) multicolored dice {\em simultaneously},
the sequence $(6,1)$ means 6 on red, and 1 on green and clearly this is a different outcome than $(1,6)$.
Later on we will discuss counting in the situation where we roll
several {\em indistinguishable} dice simultaneously. In this case we will be counting {\bf\em multisets}\label{multisets1}
and we use an idea called {\bf\em stars and bars}\label{starsandbars1} type counting.\\

\noindent $\bullet$ How many ways can a person answer a 9-question True/False exam (assuming they
answer each question)?\\

\noindent Solution: Here, $n=2$, $\{T,F\}$, moreover, the stages are `answer to question 1', `answer to question 2',\dots,`answer to question 9', Therefore, we can think of
the sample points as ordered $9$-tuples, where each entry is either True or False. So, $2^9$ possible answer sheets.\\

\noindent ({\em continued})  What if leaving questions blank is allowed?\\

\noindent Solution: Now, $n=3$, $\{T,F,\mbox{blank}\}$, and therefore, there
are $3^9$ possible answer sheets.\\


\noindent $\bullet$ 15 employees sign up for exactly one of 6 possible jobs.  How many different sign-ups are possible?\\

\noindent Before providing the solution, think about actually performing this experiment.  What would an outcome of this experiment look like?
We are trying to count {\em sign-ups}.  What's a sign-up?  Well\dots we model it!  I'm thinking that the 15 employees are lined up, and, when they reach the front of the line, they declare their job choice.  So, from this point of view, I model the sample point as an ordered 15-tuple where each entry is one of 6 jobs.  Thus, $n=6$ and $k=15$. There are $6^{15}$ sign-ups.




\newpage




\noindent {\bf 2.} {\bf The number of ways to order $n$ distinct objects.}\label{sec:orderings}\\
$$n! := n\times (n-1)\times (n-2)\times \cdots \times 3\times 2\times 1.$$
\noindent By convention we set $0!\equiv 1$.  The expression $n!$ is pronounced {\bf\em $n$ factorial}\label{factorial}.\\

\vskip .5 in

\noindent {\bf 3.} {\bf Sampling {\em without} replacement.}\label{sec:permutations}\\
From $n$ distinct objects, how many sequences of length $1\le k\le n$ can be made where there are no repeated entries?

There are
$$\underbrace{n\times (n-1)\times (n-2)\times \cdots \times (n-(k-1))}_{k\ \mbox{\tiny factors}} =: (n)_k = {}_nP_{k} = \frac {n!}{(n-k)!}$$
sequences (orderings, arrangements).\\

\noindent When $k=0$, by convention, we would have only the empty sequence, so $(n)_0\equiv 1$. But, there are {\em no} sequences of length greater than $n$, therefore, we define $(n)_k=0$ when $k>n$, and $(n)_n=n!$.  The expression $(n)_k$ is pronounced {\bf\em $n$ falling factorial $k$}\label{fallingfactorial}.\\



%\noindent These arrangements are sometimes referred to as {\bf\em permutations} of $n$ objects taken $k$ at a time.\\

\vskip .5 in

\noindent {\bf Examples.}\\

\noindent $\bullet$ There are 4 slips of paper (numbered 1,2,3,4) in a hat.
The experiment is to
draw all slips of paper one at a time (without replacing) noting the number on each.\\

\noindent Solution: There are $4!$ (orderings) ways to pull out all the slips of paper.
Since $4!=24$ is rather small we can list out all $\omega\in \Omega$. See experiment 3.1 on page \pageref{4!permutations}. \\

\noindent ({\em continued}) What if we draw only 2 slips of paper instead?\\

\noindent Solution: $n=4$ distinct objects, forming sequences of length $k=2$. Therefore, $(4)_2 = 4\cdot 3 = 12$.  Again, this number is rather small and we list out the
12 possibilities in experiment 3.2 on page \pageref{4fallingfactorial2}.\\



\noindent $\bullet$ In a standard deck of 52 cards: 13 ranks ($2,3,4,5,6,7,8,9,10,$ Jack, Queen, King, Ace) in each of 4 suits $(\clubsuit, \diamondsuit, \heartsuit, \spadesuit)$.  How many ways can these cards be ordered? \\

\noindent Solution: $52!$.  This is a {\em very} large number!  For fun: type `How large is $52!$' in a Google search engine.
Although it would be impossible list out all $52!$ $\omega$'s,
it shouldn't be difficult to understand the process that leads to them.\\


\newpage





\noindent $\bullet$ 30 balls: 16 red $\{r_1,r_2,\dots,r_{16}\}$, 8 green $\{g_1,g_2,\dots,g_8\}$, and 6 yellow $\{y_1,y_2,\dots,y_6\}$.
We draw {\em all} the balls one at a time without replacing.  How many orderings can we observe? \\

\noindent Solution: There are $30!$ ordering of these 30 distinct balls.\\

\noindent ({\em continued.}) If, instead, we draw only 3 balls (without replacing) how many sequences can we observe? \\

\noindent Solution: There are $(30)_3=30\cdot 29\cdot 28$ orders of 3 balls we can observe drawn from these 30.\\

\noindent ({\em continued}) Now, a more {\bf\em advanced question}\label{30ballsadvancedquestion}: How many orderings have all the balls of like color together?  For example,
$$\underbrace{g_3,g_7,\dots, g_5}_{\mbox{all green}},
\underbrace{r_6,r_{14},\dots,r_2}_{\mbox{all red}}, \underbrace{y_5,y_1,\dots, y_2}_{\mbox{all yellow}}$$
is one such ordering.\\

\noindent Solution: Clearly, $30!$ {\em is way too much} since a sequence among the $30!$ can have the colors alternating, for instance.
So, let's think about the process that leads to a sample point in this event.  We want all the balls of the same color grouped next to each other.
In the sample point demonstrated above we have green, followed by red, followed by yellow {\em and} within each color we have an ordering of the balls of that color.
But, it seems clear that in the above sample point, by taking the exact same ordering within each color but simply moving the blocks of colors around, we would get a different sequence. So, we look at a fixed ordering of colors, say like the one above: green, red, yellow. And ask: for this ordering of colors how many orderings of the balls are there.
The process to order the balls is 3 stages. In stage 1 (green) we select an order of green balls -- there are $8!$ possible. For each such ordering of the green balls, stage 2 (red) has us select an order of the red balls -- there are $16!$ such; and, for each ordering or green and red balls, stage 3 (yellow) has us select an ordering for the yellow balls -- there are $6!$ such ordering. The basic counting rule tells us that there are $16!8!6!$ orderings of the balls when the color sequence is green, red, yellow.
But now, every ordering of the 3 colors yields the same number of ordering of the balls.  And, since there are $3!$ ways to order the 3 colors and $16!8!6!$ ways to order the balls of like color amongst themselves, it follows there are $3!16!8!6!$ ordering of the balls that have all balls of like color together.\\


\newpage

\noindent {\bf Example and discussion.} (a first illustration of the idea of exchangeability)\label{exchangeability2}\\
Suppose we have 5 chips numbered 1 through 5 in a hat, and we plan to select 3 of them without replacement one at a time.
All the possible sample points are listed in the following figure (we assume they are all equally likely):

$$\begin{array}{rrrrr}
123\quad &213\quad &312\quad &412\quad &512\quad \\
124\quad &214\quad &314\quad &413\quad &513\quad \\
125\quad &215\quad &315\quad &415\quad &514\quad \\
132\quad &231\quad &321\quad &421\quad &521\quad \\
134\quad &234\quad &324\quad &423\quad &523\quad \\
135\quad &235\quad &325\quad &425\quad &524\quad \\
142\quad &241\quad &341\quad &431\quad &531\quad \\
143\quad &243\quad &342\quad &432\quad &532\quad \\
145\quad &245\quad &345\quad &435\quad &534\quad \\
152\quad &251\quad &351\quad &451\quad &541\quad \\
153\quad &253\quad &352\quad &452\quad &542\quad \\
154\quad &254\quad &354\quad &453\quad &543\quad \end{array}$$
\begin{center}{\bf Figure.} The sample points $\omega$ of $\Omega$ listed out.\end{center}

\noindent In the above representation, the sample point $324$, for instance, means chip number 3 was drawn first, chip number 2 drawn second,
and chip number 4 drawn third.  I ask:
\begin{center}What's the probability that chip number 3 is drawn first?\end{center}
\noindent Intuitively, the answer is just $\frac 15$ since any of the 5 numbers is equally likely to have been chosen first; i.e., we simply view the experiment as stopping after the first draw. Alternatively, we can work with the sample space $\Omega$ and note $|\Omega|=5\cdot 4\cdot 3$, while the event of interest has $1\cdot 4\cdot 3$ sample points, and the classical probability measure dictates the probability as $\frac {1\cdot 4\cdot 3}{5\cdot 4\cdot 3}=\frac 15$.\\

\noindent Now, I ask:
\begin{center}What's the probability that chip number 3 is drawn {\em third}?\end{center}
Intuitively, the answer should also be $\frac 15$ because before the experiment is actually performed the number 3 should be equally likely to occur in any of the 3 positions; in fact, each of the 5 numbers should be equally likely to occur in any of the 3 positions.\\

%each of the 5 numbers should have the same chance of appearing in the
%third position (in fact, the same chance of appearing at {\em any} position).  \\

\noindent Interestingly, there is another way to view this. What would happen to our original sample space if, in each sample point above, we simply {\em exchange} the first and third entry.  Then, in this {\em exchanged} sample space, the first entry of a sample point can be viewed as the chip number that was drawn third (and, the third entry as the chip number drawn first).  But, here's the amazing thing: the exchanged sample space and the original sample space are {\em exactly the same set!}  The next figure has the sample points in the exchanged $\Omega$ listed out $\omega$ for $\omega$ (convince yourself the two lists are the same):

\newpage



$$\begin{array}{rrrrr}
321\quad &312\quad &213\quad &214\quad &215\quad \\
421\quad &412\quad &413\quad &314\quad &315\quad \\
521\quad &512\quad &513\quad &514\quad &415\quad \\
231\quad &132\quad &123\quad &124\quad &125\quad \\
431\quad &432\quad &423\quad &324\quad &325\quad \\
531\quad &532\quad &523\quad &524\quad &425\quad \\
241\quad &142\quad &143\quad &134\quad &135\quad \\
341\quad &342\quad &243\quad &234\quad &235\quad \\
541\quad &542\quad &543\quad &534\quad &435\quad \\
251\quad &152\quad &153\quad &154\quad &145\quad \\
351\quad &352\quad &253\quad &254\quad &245\quad \\
451\quad &452\quad &453\quad &354\quad &345\quad \end{array}$$
\begin{center}{\bf Figure.} The sample points of the exchanged $\Omega$ listed out.\end{center}

\noindent Now, the point is this: Because these sample spaces are the same, we can view the original sample space -- just the way it is shown at the start of this example -- as having first entry showing what was drawn third.  And, now, it should be clear that there are just as many sample points in the event that chip number 3
drawn third as there are sample points in the
event that chip number 3 drawn first, i.e., these two events have the same cardinality. Moreover, since
all sample points are equally likely, the probabilities of these events must be the same as well!\\

\noindent As a thought exercise, it should be easy to see that had this experiment been to draw chips {\em with replacement}, then the same is true; namely,
if the experiment had been to draw 3 chips with replacement after each draw, then the probability the first drawn is 3 would be the same as the probability the third drawn is 3.
But, this is not really all that interesting.\\


\noindent {\bf Remark.}\\
The ideas just presented remain true when we have $n$ chips numbered 1 through $n$ and we draw $k$ chips without replacement,
where $1\le k\le n$.  But, also, the ideas can be generalized significantly.  We have a sample space $\Omega$ of all possible
sequences of length $k$ where entries are any of $n$ objects without repetition.
Instead of exchanging the first and last entry of each sequence in $\Omega$ as we did in the example,
we can, in fact, apply any fixed permutation to each sample point in $\Omega$ and recover the {\em same} sample space.
For instance, one choice is the permutation (mapping) that reverses the order of the each $\omega\in \Omega$:
$$\pi : (x_1,x_2,x_3,x_4,x_5,x_6) \mapsto (x_6,x_5,x_4,x_3,x_2,x_1).$$
Another possibility is the permutation that simultaneously swaps the first and third entry {\em and} the second and fifth entry:
$$\pi : (x_1,x_2,x_3,x_4,x_5,x_6) \mapsto (x_3,x_5,x_1,x_4,x_2,x_6).$$
In either of these instances the ``permuted" sample space and the original sample space will be identical!  If you've had discrete math or any exposure to proof writing, you should try to prove this for yourself.



\newpage

\noindent {\bf Example.}\\
We have 8 blue marbles ($b_1,b_2,b_3,b_4,b_5,b_6,b_7,b_8$) and 5 red marbles ($r_1,r_2,r_3,r_4,r_5$). We draw
6 marbles without replacement, where, on each draw, a marble is selected equally likely among the remaining marbles.
Compute the probability that the last marble you draw is red.  Compute the probability that the third marble is blue and the fifth marble is red.\\

\noindent Under the conditions of this experiment our sample space $\Omega$ is finite and equally likely, $|\Omega|=(13)_{6}$ by the basic counting rule.\\

\noindent Let $A$ be the event where the sixth marble drawn is red.
Again, intuitively, $P(A) = \frac 5{13}$ since a red marble is equally likely to appear in any position and there are 5 red out of 13 total marbles; but,
given the previous remark, we can apply the permutation that
reverses the order of every $\omega\in \Omega$, get a sample space that is the same as the original sample space so that
$|A|$ is the same size as the event that the first marble drawn is red. Consequently, $P(A) = \frac 5{13}$.\\

\noindent Let $B$ be the event where the third is blue and the fifth is red.  To compute $P(B)$ we recognize that applying the permutation that simultaneously swaps positions 3 and 1 and swaps positions 5 and 2
to every $\omega\in\Omega$ gives the original sample space back again.  But then, $|B|$ is the same size as the event that has the first marble drawn being blue and the second
marble drawn red, and the cardinality of this event is, by the basic counting rule,
$$8\cdot 5\cdot 11\cdot 10\cdot 9\cdot 8.$$
So, $P(B) = \frac {8\cdot 5\cdot 11\cdot 10\cdot 9\cdot 8}{13\cdot 12\cdot 11\cdot 10\cdot 9\cdot 8} = \frac {8\cdot 5}{13\cdot 12},$
which, not surprisingly, is the same as the probability that the first marble is blue and the second is red by stopping the experiment after the second draw.\\

\vskip .5 in

\noindent {\bf Anagrams.}\label{sec:anagrams}\\

\noindent An {\bf\em anagram}\label{d:anagram} is an ordering of the ``letters" in a ``word".\\

\noindent {\bf Example.}\\
\noindent How many anagrams of the word {\bf MATH} are possible?  List them all.\\

\noindent Solution: This is an old question. In fact, this is exactly the same as experiment 3.1 on page \pageref{experiment31}:
since there are 4 distinct letters in this word,
the number of orderings should be just $4!=24$. So, if we do things correctly we should have 24 orderings.  Here goes:
$$
\begin{array}{cccccc}
MATH & MAHT & MTAH & MTHA & MHAT & MHTA \\
AMTH & AMHT & ATMH & ATHM & AHMT & AHTM \\
TMAH & TMHA & TAMH & TAHM & THMA & THAM \\
HMAT & HMTA & HTMA & HTAM & HAMT & HATM \end{array}$$


\newpage

\noindent The reason the last example was ``easy" was because the 4 letters in MATH are {\em distinct} so that we are able to recognize the counting as just sampling all $k=n=4$ letters without replacement.  The same reasoning explains why there are $7!$ anagrams of the word DISPARE.  \\

\noindent We now investigate what happens when the letters are not all distinct.\\

\noindent {\bf Example and discussion.}\\
\noindent How many anagrams of the word {\bf PEEP} are possible?\\

\noindent An immediate issue you may see is that when we swapped the first and last letter
in MATH we obtained a distinct ordering HATM, but if we swap the first and last letter in
the word PEEP we obtain the {\em same} word.  So, in this problem, it appears $4!$ would be
over-counting because $4!$ would be the number of anagrams had the letters in the word PEEP
been all distinct, which they are not. \\

\noindent To count the actual number of anagrams we first artificially
make these 4 letters in PEEP distinct, say, by putting subscripts on the letters: P$_1$E$_1$E$_2$P$_2$,
then there {\em are} $4!$ anagrams of this ``word".  Here are 24 anagrams grouped by common spelling:
$$
\begin{array}{cccccc}
P_1E_1E_2P_2 & P_1E_1P_2E_2 & P_1P_2E_1E_2 & E_1P_1E_2P_2 & E_1P_1P_2E_2 & E_1E_2P_1P_2 \\
P_1E_2E_1P_2 & P_1E_2P_2E_1 & P_1P_2E_2E_1 & E_2P_1E_1P_2 & E_2P_1P_2E_1 & E_2E_1P_1P_2 \\
P_2E_1E_2P_1 & P_2E_1P_1E_2 & P_2P_1E_1E_2 & E_1P_2E_2P_1 & E_1P_2P_1E_2 & E_1E_2P_2P_1 \\
\underbrace{P_2E_2E_1P_1}_{\mbox{\tiny all words spell PEEP}} & \underbrace{P_2E_2P_1E_1}_{\mbox{\tiny all words spell PEPE}} & \underbrace{P_2P_1E_2E_1}_{\mbox{\tiny all words spell PPEE}} & \underbrace{E_2P_2E_1P_1}_{\mbox{\tiny all words spell EPEP}} & \underbrace{E_2P_2P_1E_1}_{\mbox{\tiny all words spell EPPE}} & \underbrace{E_2E_1P_2P_1}_{\mbox{\tiny all words spell EEPP}} \end{array}$$

\noindent When grouped this way it is clear that there are always $4$ words in each column that all
spell the same word. I.e., in each column the 2 P's and the 2 E's are in the same relative positions, and therefore, if we remove the subscripts then all words in a column are the same.
Therefore, the number of columns, i.e., $\frac {4!}{4}=6$, {\em is} the number of distinct
spellings of the word PEEP (without subscripts). So, there are 6 anagrams of PEEP.\\

\noindent One way to have seen that there are always going to be 4 orderings of the letters P$_1$, E$_1$, E$_2$, P$_2$ that all spell the same word
is as follows.  Think of the process that keeps the 2 P's (P$_1$ and P$_2$) and the 2 E's (E$_1$ and E$_2$) in the same relative positions.
For instance,
fix the 2 positions for the P's (the remaining 2 positions will have to be for the E's) for a word.  The process is 2 stages: stage 1 has us put P$_1$ and P$_2$ into the 2 fixed positions (there are $2!$ ways to do this), and, for each way that stage 1 is completed, stage 2 has us place to 2 E's (also $2!$ ways).  Thus, there are $2!2!=4$ orderings
of P$_1$E$_1$E$_2$P$_2$ that have the same spelling.\\




\newpage

\noindent The last discussion can lead us to provide a proof for\dots\\

\noindent {\bf Counting the number of anagrams.}\label{countinganagrams}\\
If an $n$-letter word is comprised of $k\ge 1$ types of {\em indistinguishable} letters of
which there are $n_i$ of type $i$ ($i=1,\dots,k$), where
$n_1+n_2+\cdots+n_k=n$, then there are
$$\frac {n!}{n_1!n_2!\cdots n_k!}$$
anagrams.\\

\vskip .5 in


\noindent {\bf Examples.}\\

\noindent $\bullet$ How many anagrams of INDEPENDENCE are there?\\

\noindent SOLUTION:

This is an $n=12$ letter word.

There are $k=6$ types of letters (I,N,D,E,P,C),

$n_1=1$ I, $n_2=3$ N's, $n_3=2$ D's, $n_4=4$ E's, $n_5=1$ P, and $n_6=1$ C.

Therefore, there are $\dfrac {12!}{1!3!2!4!1!1!}=\dfrac {12!}{4!3!2!}$ anagrams.\\

\noindent ({\em continued}) How many anagrams of INDEPENDENCE begin NE (in this order)?\\

\noindent SOLUTION:

Think of the process that leads to such an anagram.
Each such anagram will begin NE followed by the remaining letters in some order. If you try to visualize the list of all anagrams that look like this,
then it should not be hard to see that the number of them will be the same as if we just {\em erase} the NE at the beginning of each. We'd be now left
with all the anagrams of a 10-letter word with 1 I, 2 N's, 2 D's, 3 E's, 1 P, and 1 C.   Therefore, there are
$\dfrac {10!}{1!2!2!3!1!1!}=\dfrac {10!}{3!2!2!}$ anagrams.\\

\noindent ({\em continued}) How many anagrams will have all the vowels contiguous (i.e., grouped together)?\\

\noindent SOLUTION:

Here's a slick way: since all the vowels will be grouped together,
let's temporarily create the super-letter V, i.e., V represents IEEEE.  Now, let's remove the vowels from
INDEPENDENCE (leaves us with NDPNDNC) and put the {\em one} super-letter V in the vowels absence: VNDPNDNC,  an 8-letter word with
1 V, 3 N's 2 D's, 1 P, and 1 C, and there are $\dfrac {8!}{1!3!2!1!1!} = \dfrac {8!}{3!2!}$ such anagrams.  Now, in these anagrams, if we replace
the V with IEEEE in its place, we'd have all the anagrams of INDEPENDENCE with the vowels grouped together in the order IEEEE.
But, of course, these vowels can be re-ordered in $\dfrac {5!}{1!4!}=5$ ways. So, the number of anagrams is $5\cdot\dfrac {8!}{3!2!}$.\\


\newpage


\noindent $\bullet$ How many anagrams of 1111000 are there?\\

\noindent SOLUTION:

A 7-letter word with 4 1's and 3 0's. Thus, $\dfrac {7!}{4!3!}$ anagrams.\\

\bigskip

\noindent $\bullet$ We toss a coin 7 times and observe the sequence of heads and tails we get. How many sequences have exactly 4 heads?\label{7cointosses4heads}\\

\noindent SOLUTION:

This is just the previous question (do you see?). Answer: $\dfrac {7!}{4!3!}$.\\

\bigskip

\noindent $\bullet$\label{30ballsexample2} We have 30 balls: 16 red, 8 green, and 6 yellow.  We line up all 30 balls left to right.  How many color sequences can we observe?\\

\noindent SOLUTION:

Notice the subtle difference in this problem compared with what we were dealing with on page \pageref{30ballsexample}.
In this problem, if we swap two balls of the same color we'll get the same color sequence.  We are now treating balls of the same color as {\em indistinguishable}.
For instance, the color sequence
$$GGGGGGGGRRRRRRRRRRRRRRRRYYYYYY$$
has all the greens appearing first, followed by all the reds, followed by all the yellows, and,
if we permute the $G$'s amongst their relative positions, we end up with the same color sequence.
We think of the color sequence as a word of length 30
with 16 R's, 8 G's, and 6 Y's, then every color sequence corresponds to an anagram of this word.
Therefore, there are
$$\dfrac {30!}{16!8!6!}$$
\noindent color sequences.\\

\bigskip


\noindent $\bullet$ 4 Germans, 4 Danes, 3 Americans, and 2 Canadians enter a road race. Everyone finishes and there are no ties.
How many ways can these people finish the race if we record only their nationalities?\\

\noindent SOLUTION:

There are $13!$ finishes possible if the 13 people were treated distinct. However, in this problem since we only record their racer's nationality, all Germans are indistinguishable, all Danes are indistinguishable, etc. We are trying to count the number of possible orderings of these nationalities, i.e., which nationalities finish 1st, 2nd, 3rd, and so on.
But, this is just the number of anagrams of the word GGGGDDDDAAACC. There are
$$\dfrac {13!}{4!4!3!2!}$$
\noindent possible finishes.









\newpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% COMBINATORICS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% COMBINATORICS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% COMBINATORICS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% COMBINATORICS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}{\bf \Large EXERCISES.}\label{problemset1}\end{center}

%\vskip .25 in

\noindent {\bf 1.} A 7-sided die is rolled 14 times. How many sequences of outcomes are there?\medskip

%\noindent {\bf 2.}  Repeat Question 10 of Permutations if values can be repeated.\\

\noindent {\bf 2.} Noelle has a basket of 8 toys for her dog. She pulls a toy for the dog to play with, then takes that toy away from the dog and puts it back into the basket.
She then grabs one of the those toys again uniformly at random. She repeats this a total of 6 times. In how many ways can Noelle draw the toys?\medskip

\noindent {\bf 3.} Feller's \textit{An Introduction to Probability Theory and Its Applications, Volume 2} has 670 pages. Dr. Torcaso opens the book, flips to a random page, and then closes the book. This is repeated 10 times. How many different sequences of pages can Dr. Torcaso obtain?\medskip

\noindent {\bf 4.} How many subsets of $\{1,2,\dots,n\}$ exclude the subset $\{1,2,\dots, k\}$, where $1 \leq k \leq n$?\medskip

\noindent {\bf 5.} There are $8$ soccer players on a Hall of Fame candidate list. You can select {\em any} number of the people as your choice(s) to enter the Hall of Fame as you like (zero is possible as well as all 8). How many different selections are there?\medskip

%\noindent {\bf 7.} Repeat Question 10 of Multinomial Coefficients under the assumptions that the shirts in each individual line of Billabong are identical and that Gabe is no longer cleanly, so he may wear shirts as many times as he wants (ew).\\

\noindent {\bf 6.} Paul has to go to the Bradford laundry room. There are 15 washers. He does his 5 loads of laundry one-at-a-time, and he can select any of the 15 washers each time. Find the number of ways Paul can do his 5 loads of laundry.\medskip

\noindent {\bf 7.} Adam is playing poker. On each turn, since he is aggressive and doesn't know how to fold, he either checks or raises. There are 6 bets in a given hand. Find the amount of sequences of actions that Adam can perform in one hand.\medskip

\noindent {\bf 8.} There are 20 sectors on a standard dartboard. Six people throw darts at random at this board. If we record what sector each person landed in, how many recordings are possible?\medskip

\noindent {\bf 9.} Suppose a sandwich is

choice of bread (choose one: 1=rye, 2=wheat, 3=white, 4=kaiser roll)

choice of protein (choose one: 1=roast beef, 2=turkey, 3=tuna salad)

choice of cheese (choose one: 1=American, 2=Swiss, 3=no cheese)

choice of lettuce (choose one: 1=yes, 2=no)

choice of tomato (choose one: 1=yes, 2=no)

\noindent (a) How many sandwiches on rye are possible?\\
(b) How many roast beef sandwiches have cheese?\\
(c) If a friend says they will buy you a sandwich (so that all possible sandwiches are equally likely), what's the probability you get a roast beef sandwich?\\
(d) (separate question) What's the probability your friend put's cheese and lettuce on your sandwich?\\
(e)* If a sandwich is allowed to not have a protein (for example, two slices of bread with nothing in-between can be considered a sandwich now), how many sandwiches are possible now?\\
(f)** If a sandwich is now allowed to have more than one type of protein (as well as no protein) and is allowed to have more than one type of cheese (as well as no cheese), how many sandwiches are possible now?\\
(g) The deli worker refuses to put mustard on Tuna Salad.  How many sandwiches are possible? Try to count this in two ways.\medskip


\newpage


\noindent {\bf 10.} A manager has 165 players from which they are trying to fill a roster of 11 different positions.
How many rosters are possible?\\


\noindent {\bf 11.} A basketball manager has 8 players 6$^{\prime}$ in height or taller
and 6 players under 6$^{\prime}$ tall on the bench.  The manager wants to create a starting roster
(5 different positions) consisting of a center, a power forward, a small forward, a shooting guard, and a point guard.\\
(a) If there are no restrictions on how the manager can make the starting team of 5 players, how many starting rosters are possible?\\
(b) If only players 6$^{\prime}$ or taller can be rostered for the center and two forward positions, and only players under 6$^{\prime}$ can be rostered for the guard positions,
how many starting rosters are possible now?\\
(c) (separate question) If only players 6' or taller can be rostered for the center and two forward positions (and no height restriction on guards), how many starting
rosters are possible?\\



\noindent {\bf 12.} There are 8 horses in a race at Pimlico. The first horse to finish is ranked 1, the second horse to finish is ranked 2, and so on.
All horses finish the race, there are no ties. \\
(a) How many rankings are possible?\\
(b) The rank 1, 2, and 3 positions are sometimes called the {\em win, place,} and {\em show} positions, respectively.  How many win, place, show results are possible?\\



\noindent {\bf 13.} A child is putting away 12 different Lego blocks. In how many different orders can they be put into the container?\\



\noindent {\bf 14.} We have a standard deck of 52 cards well-shuffled.  We turn over the top 5 cards one at a time.\\
(a) How many arrangements are possible?\\
(b) Find the probability all cards are red (the diamond $\diamondsuit$ and heart $\heartsuit$ suits are red, other suits are black).\\
(c) Find the probability colors alternate.\\
(d) (separate question) Suppose when we turn over the 5 cards we replace each card we turn over into the deck and re-shuffle before drawing the next, then how many possible arrangements are there now? \\

\noindent {\bf 15.} Suppose $k$ and $n$ are positive integers with $1\le k\le n$.
How many orderings of the $n$ integers $1$ through $n$ have the first $k$ entries from the set $1$ through $k$?\\



\noindent {\bf 16.} 10 people sit at a round table. How many distinct arrangements of seats are there? Any arrangements that can be obtained by rotating the people at the table around but not changing the order of the seats are considered identical. Also, if all such seating arrangements were equally likely, what's the probability Fred is seated next to Carrie?\\

\noindent {\bf 17.} A lottery card consists of 6 distinct numbers from $1$ to $90$ inclusive.
If the order is relevant in determining a winner, how many different lottery cards are there?\\


\newpage



\noindent {\bf 18.} Gary creates a workout plan at a gym. There are 28 different machines, and the order in which he selects machines influences his workout.
How many different ways can Gary create a workout consisting of 7 machines if \\
(a) Gary can repeat a machine at any point in the workout?\\
(b) Gary cannot repeat a machine in the workout?\\
(c) Gary can repeat a machine in the workout but just not consecutively?\\







\noindent {\bf 19.} Consider the word BOOLAHUBBOO.\\
(a) How many anagrams are possible?\\
(b) How many of these anagrams end BOOBOO?\\
(c) How many anagrams have all the B's grouped together?\\
(d) How many anagrams have all the B's grouped together and all the vowels grouped together?\\

\noindent {\bf 20.} I have 3 one dollar bills, 2 five dollar bills and 5 ten dollar bills.  How many ways can I
distribute these 10 bills to 10 children so that each child gets one bill?
Treat the bills of equal value as indistinguishable please.\\


\noindent {\bf 21.}
There are 9 people who will toss a ball around. The only rule is that you cannot toss to yourself.
 One of them picks up a ball, tosses to another,
who tosses to another (we're allowed to toss back to the person who tossed to them), and this repeats. E.g., if $a$ picks up the ball, tosses to $b$,
who tosses to $a$, who tosses to $c$, we would observe the ordered $4$-tuple $(a,b,a,c)$ of {\em three} tosses.\\
(a) Let $\Omega$ be the set of all such lists when we have 9 people and 7 tosses. Compute $|\Omega|$.\\
(b) Assume the sample space $\Omega$ of all 7-toss possibilities are equally likely.  What's the probability that $f$ picks up the ball then tosses to $a,b,$ or $c$?\\
(c) Give an example of a permutation which, when applied to all $\omega\in \Omega$, will {\em not} give the same sample space back again.\\
(d)* (challenging) With 9 people and 7 tosses, how many possible lists can be observed where Fred is the last to catch the ball?\\





\newpage




\noindent Up to now, we've only considered experiments that generated
sample points modeled through stages as a process, and we've found that the basic counting rule
has an enormous number of applications in this regard.  However, there are some experiments
where the order in which stages are performed is not needed.\\

\noindent Here's a fairly abstract but prototypical example of what I'm talking about.
Consider a set of $n$ distinct objects. Without loss of generality we can think of the set
as
$$\{x\in {\mathbb Z}:1\le x\le n\} = \{1,2,3,\dots, n\}.$$
How many subsets of size $k$ ($0\le k\le n$) are there from this set?\\



\noindent {\bf Combinations.}\label{sec:combinations}\\
There are
$${n\choose k} = \frac {(n)_k}{k!} = \frac {n!}{k!(n-k)!}$$
subsets of size $k$ from the $n$-element set.  The symbol ${n\choose k}$ is
pronounced {\bf\em $n$ choose $k$} and, in mathematics, it is referred to as a {\bf\em binomial coefficient}\label{d:binomialcoeff}.\\

\noindent $\displaystyle {n\choose k}$ counts how many ways we can sample $k$ objects without replacing, ignoring order.\\

\vskip .3 in

\noindent {\bf Example and discussion.}\\
How many subsets of size $3$ are there from a 5-element set?\\

\noindent SOLUTION: There are ${5\choose 3} = \frac {5!}{3!2!}= \frac {5\cdot 4\cdot (3!)}{3!2!} = \frac {5\cdot 4}{2\cdot 1} = 10$.
Here are the 10 subsets listed out:
$${\small \begin{array}{ccccccccccc}
\{1,2,3\} & \!\! \{1,2,4\} & \!\! \{1,2,5\} & \!\! \{1,3,4\} & \!\! \{1,3,5\} & \!\! \{1,4,5\} &\!\!  \{2,3,4\} & \!\! \{2,3,5\} & \!\! \{2,4,5\} & \!\! \{3,4,5\}\end{array}}$$

\noindent Had we decided to count the subsets by first including order we would end up with
the following $(5)_3=60$ ordered 3-tuples:
$${\small \begin{array}{ccccccccccc}
(1,2,3) & (1,2,4) & (1,2,5) & (1,3,4) & (1,3,5) & (1,4,5) & (2,3,4) & (2,3,5) & (2,4,5) & (3,4,5) \\
(1,3,2) & (1,4,2) & (1,5,2) & (1,4,3) & (1,5,3) & (1,5,4) & (2,4,3) & (2,5,3) & (2,5,4) & (3,5,4) \\
(2,1,3) & (2,1,4) & (2,1,5) & (3,1,4) & (3,1,5) & (4,1,5) & (3,2,4) & (3,2,5) & (4,2,5) & (4,3,5) \\
(2,3,1) & (2,4,1) & (2,5,1) & (3,4,1) & (3,5,1) & (4,5,1) & (3,4,2) & (3,5,2) & (4,5,2) & (4,5,3) \\
(3,1,2) & (4,1,2) & (5,1,2) & (4,1,3) & (5,1,3) & (5,1,4) & (4,2,3) & (5,2,3) & (5,2,4) & (5,3,4) \\
(3,2,1) & (4,2,1) & (5,2,1) & (4,3,1) & (5,3,1) & (5,4,1) & (4,3,2) & (5,3,2) & (5,4,2) & (5,4,3) \\
\end{array}}$$
But, if we only cared about the entries and not the order they are in, then computing the number of ordered 3-tuples would be {\em way} over-counting!
How much is $(5)_3$ over-counting by?  \\

\noindent It seems every choice of 3 distinct elements from $\{1,2,3,4,5\}$ is repeated $3!=6$ times in the table, when we only need {\em one}!
Just look at the columns of the table. Every column has the same 3 entries, and we know there must be $3!$ of them.
So $(5)_3$ is over-counting by $3!$ and, therefore, there are $\frac {(5)_3}{3!}$ subsets.\\

\noindent An extension of this argument to $k$-element subsets of an $n$-element set should now be straightforward.










\newpage

\noindent {\bf Remark.} (A slight digression)\\
\noindent Here's an interesting follow-up to the last example.
Suppose our experiment is to draw 3 numbers without replacement
from a hat having numbers 1,2,3,4,5. We draw so that each number is equally likely to be any of the remaining numbers in the hat.

\begin{center}What's the probability we draw the numbers in decreasing order?\end{center}

\noindent Looking at the equally likely sample space, (we list those sample points again here:)
$${\small \begin{array}{ccccccccccc}
\!\! \!\! \!\! (1,2,3) & \!\! (1,2,4) & \!\! (1,2,5) & \!\! (1,3,4) & \!\! (1,3,5) & \!\! (1,4,5) & \!\! (2,3,4) & \!\! (2,3,5) & \!\! (2,4,5) & \!\! (3,4,5) \\
\!\! \!\! \!\! (1,3,2) & \!\! (1,4,2) & \!\! (1,5,2) & \!\! (1,4,3) & \!\! (1,5,3) & \!\! (1,5,4) & \!\! (2,4,3) & \!\! (2,5,3) & \!\! (2,5,4) & \!\! (3,5,4) \\
\!\! \!\! \!\! (2,1,3) & \!\! (2,1,4) & \!\! (2,1,5) & \!\! (3,1,4) & \!\! (3,1,5) & \!\! (4,1,5) & \!\! (3,2,4) & \!\! (3,2,5) & \!\! (4,2,5) & \!\! (4,3,5) \\
\!\! \!\! \!\! (2,3,1) & \!\! (2,4,1) & \!\! (2,5,1) & \!\! (3,4,1) & \!\! (3,5,1) & \!\! (4,5,1) & \!\! (3,4,2) & \!\! (3,5,2) & \!\! (4,5,2) & \!\! (4,5,3) \\
\!\! \!\! \!\! (3,1,2) & \!\! (4,1,2) & \!\! (5,1,2) & \!\! (4,1,3) & \!\! (5,1,3) & \!\! (5,1,4) & \!\! (4,2,3) & \!\! (5,2,3) & \!\! (5,2,4) & \!\! (5,3,4) \\
\!\! \!\! \!\! {\bf (3,2,1)} & \!\! {\bf (4,2,1)} & \!\! {\bf (5,2,1)} & \!\! {\bf (4,3,1)} & \!\! {\bf (5,3,1)} & \!\! {\bf (5,4,1)} & \!\! {\bf (4,3,2)} & \!\! {\bf (5,3,2)} & \!\! {\bf (5,4,2)} & \!\! {\bf (5,4,3)} \\
\end{array}}$$

\noindent every choice of 3 numbers we could have drawn has exactly $3!$ orderings.  Moreover, among the $3!$ equally likely orderings, only 1 of them will be in decreasing order ({\bf\em bold-faced} in table above).  Therefore, the probability we draw the numbers in decreasing order is $\frac 1{3!}$. \\

\noindent Alternatively, there are ${5\choose 3}$ ordered 3-tuples in decreasing order out of a sample space of $(5)_3$ equally likely sample points. Thus, the probability is also
$\dfrac {{5\choose 3}}{(5)_3}$.  You should verify this expression reduces to $\dfrac 1{3!}$.  It's interesting to note that this probability only depends on the size of the subset and not the number of elements we are choosing them from.\\


\vskip 1 in

\noindent {\bf Remark.}\\
\noindent There are {\em many} experiments -- even ones that are modeled in stages where we may want to count sample points (or the number of ways a stage can be completed) -- that ignore order, i.e., count subsets. Here are some examples.\\

\vskip .5 in

\noindent {\bf Example.}\\
\noindent A committee of size 3 is to be made from Amir, Betty, Cristoff, Dahlia, and Ernesto. How many committees are possible?\\

\noindent SOLUTION: Unlike the situation where 3 of them were being elected into distinguishable positions (president, vice-president, treasurer), here we have the
3 members serving a common committee -- we can think of the positions as indistinguishable.  So, we are counting subsets. There are ${5\choose 3}$ committees.\\


\newpage


\noindent {\bf Standard deck of 52 cards.}\\
\includegraphics*[-80,0][400,140]{standard-deck.jpg}
\begin{center}{\bf Figure\footnote{Source of Figure: https://www.milefoot.com/math/discrete/counting/cardfreq.htm}.} 13 ranks:{\small A,2,3,\dots J,Q,K};\ 4 suits:
{\small Club}($\clubsuit$), {\small Spade}($\spadesuit$), {\small Heart}($\heartsuit$), {\small Diamond}($\diamondsuit$)\end{center}




\vskip .5 in



\noindent A {\bf\em $k$-card hand} or, simply, {\bf\em hand} of $k$ cards is a subset of size $k$ from from the deck.\\

\vskip .5 in

\noindent {\bf Example.}\\
\noindent From a standard deck of 52 cards, how many 5-card hands are possible? \\

\noindent SOLUTION: There are ${52\choose 5}$ such hands. As an integer this is 2,598,960 hands.

\vskip .5 in

\noindent {\bf Remark.} (Be careful when picking models to work with cards!)\\
A problem might tell you {\em loosely} that someone is dealing you {\em a hand} when they might really mean they are dealing you {\em a sequence of} 5 cards.
We should really understand the questions being asked before blindly using the ${52\choose 5}$ sample space.
For example, if someone asks for the probability the first card is a King while the last card is a club, then this would
require a sample space that keeps track of the order in which cards are dealt; otherwise, this would not be an event in (i.e., a subset of) the ${52\choose 5}$ sample space.
And, what the problem called a {\em hand} should really be thought of as an ordered 5-tuple of cards dealt without replacement.
In this case, we should (and, possibly, need to) use the sample space that has $(52)_5$ equally likely sample points.

However, we may have a choice in deciding whether or not to have a sample space keep track of order.  For example, consider the events

$A$: we are dealt 5 red cards, and

$B$: we are dealt 3 black and 2 red cards.

\noindent For these events, we do not need to know the order the cards were dealt
to determine whether or not the event occurred. In these cases, we are justified (but not obliged) to model
our sample points {\em ignoring} the order in which cards were dealt -- and doing so will typically
make the computations {\em much} easier!  We can use the ${52\choose 5}$ sample space $\Omega$. This $\Omega$ will still have equally likely sample points.\\


\newpage






\noindent {\bf Example.} Consider the event $A$ from the previous remark.\\
What's the probability we are dealt 5 red cards?\\

\noindent SOLUTION: There are 26 red cards (and 26 black cards) in a standard deck. The occurrence (or nonoccurrence)
of the event can be determined if we ignore the order of the deal, therefore, we can
use the ${52\choose 5}$ {\em equally likely} sample space.
Since there are ${26\choose 5}$ 5-card hands that are all red, the probability is
$\dfrac {{26\choose 5}}{{52\choose 5}} = \dfrac {65780}{2598960}\approx 0.0253$.

Note: in this example, I'll concede that we could have also (easily)
computed this probability as $\dfrac {(26)_5}{(52)_5} = 0.0253\dots$ where we kept track of the order in which the red cards were dealt,
and although it was just as easy, it wasn't necessary.  But please read the next example.\\

%\bigskip

\noindent {\bf Example.} Consider the event $B$ from the previous remark.\label{3black2redcardsexample}\\
What's the probability we are dealt 3 black and 2 red cards?\\

\noindent SOLUTION: Since the event of getting 3 black and 2 red cards doesn't
depend on the order in which these cards were dealt, I choose to work with the ${52\choose 5}$ sample space.  We need to count the number of subsets
that have 3 black and 2 red cards.  This is only a little tricky.  Here's the idea.  Label the 26 black cards $b_1,b_2,\dots,b_{26}$ and the 26 red cards $r_1,r_2,\dots,r_{26}$.
Every subset of size 5 from the 52 that have 3 black and 2 red looks like $\{b_i,b_j,b_k\}\cup \{r_m,r_n\}$, where the indices $i,j,k$ are distinct and the indices $m,n$ are distinct, i.e.,
in stage 1 we select a subset of 3 black cards from 26 (there are ${26\choose 3}$ ways to complete this) and, for each such selection of the black cards, in stage 2 we select 2 cards from the 26 red cards (which can be completed in ${26\choose 2}$ ways). The basic counting rule says there are ${26\choose 3}\cdot {26\choose 2}$ 5-card hands with 3 black and 2 red. The probability is\vskip -.1 in
$$\dfrac {{26\choose 3}\cdot {26\choose 2}}{{52\choose 5}} \approx 0.325.$$
Just to foreshadow, in the last two probability computations we used what's called the {\bf\em hypergeometric distribution}\label{hypergeometric1} (see page \pageref{d:hypergeom}).\\

\noindent {\em What if we chose to keep track of order, i.e., work with the $(52)_5$ sample space instead?}\\
\noindent Now, we're counting ordered 5-tuples of 3 black and 2 red cards.
Careful: the answer
is not $(26)_3(26)_2$.  This {\em under-counts} by a bit, since it only considers the case of a fixed ordering for the positions of black and red cards; for instance, $bbbrr$, or $brbrb$, etc.  Each re-ordering will give $(26)_3(26)_2$ 5-tuples. So, we count the number of ways to re-order the positions, the basic counting rule gives us our answer.
There are $\frac {5!}{3!2!}$ ways to re-order the positions of the black and red cards: this is just the number of anagrams of a 5-letter word having 3 B's and 2 R's.  Therefore, there are
$(26)_3 (26)_2\cdot \frac {5!}{3!2!}$ ways to receive 3 black and 2 red 5-tuples, and the probability is
$$\dfrac {(26)_3(26)_2\cdot \dfrac {5!}{3!2!}}{(52)_5} = \dfrac {\dfrac {(26)_3}{3!}\dfrac{(26)_2}{2!}}{\dfrac {(52)_5}{5!}} =: \dfrac {{26\choose 3}{26\choose 2}}{{52\choose 5}}.$$
{\bf Moral of the story}: some problems are more straightforward when we can ignore order.




\newpage






\noindent {\bf Continuing with cards: two 5-card poker hand examples\dots}\\

\noindent {\bf Example.}\\
What's the probability of getting a {\em full-house}?   FYI: a {\bf\em full-house} in poker is 3 cards of a rank and 2 cards of another (different) rank.\\

\noindent So, for instance, $\{\mbox{A}\clubsuit,\mbox{A}\diamondsuit,\mbox{A}\spadesuit, \mbox{K}\diamondsuit,\mbox{K}\heartsuit\}$ is a full-house -- Aces over Kings.\\

\noindent SOLUTION: Again, we ignore the order in which cards were dealt since a full-house will be the same full-house if dealt in a different order.  Now, try to
envision a process that describes the sample points in this event.  Here's a start:
select 3 cards of one rank, and then, after this, select 2 cards of another rank -- a two-stage procedure! This procedure would do it, except\dots \\

{\em What rank do I select the 3 cards from?  What rank do I select the 2 cards from?}\\

\noindent In a full-house, the two ranks we select will happen to be {\em distinguishable}, i.e.,
if the two ranks selected are Ace and King, then we can distinguish between
the full-house having 3 Aces and 2 Kings from the full-house that has 3 Kings and 2 Aces, i.e.,
$$\{\mbox{A}\clubsuit,\mbox{A}\diamondsuit,\mbox{A}\spadesuit, \mbox{K}\diamondsuit,\mbox{K}\heartsuit\}\ne \{\mbox{K}\clubsuit,\mbox{K}\diamondsuit,\mbox{K}\spadesuit, \mbox{A}\diamondsuit,\mbox{A}\heartsuit\}.$$
\noindent Order of selecting ranks matters here!
So, when selecting the two ranks from the 13, we may declare the first to correspond to, say, the 3 card rank, and the second (chosen from the remaining 12 ranks) to correspond to the 2 card rank -- another two-stage procedure.  Finally, the process that leads to a full-house is a 4-stage procedure that fits the paradigm of the basic counting rule:

Stage 1: select rank for 3 cards -- ${13\choose 1} = 13$ choices. Once we've performed stage 1

Stage 2: select rank for 2 cards from remaining ranks -- ${12\choose 1}=12$ choices. Then

Stage 3: select 3 cards from a rank -- ${4\choose 3}$ choices. Once we completed stage 3

Stage 4: select 2 cards from another rank -- ${4\choose 2}$ choices\dots

\noindent and, the number of full-houses is
$$13\cdot 12\cdot {4\choose 3}\cdot {4\choose 2}$$
and the probability of a full-house is
$$ \dfrac {13\cdot 12\cdot {4\choose 3}\cdot {4\choose 2}}{{52\choose 5}} = \dfrac {3744}{2598960} \approx 0.00144.$$





\vskip .5 in

\noindent {\bf Thought exercise.}\\
\noindent We employed the basic counting rule to both the 3 black/2 red example on page \pageref{3black2redcardsexample} and the full-house example
above.  Why was the counting ``more involved" in the full-house example compared to the 3 black/2 red example?\\






\newpage

\noindent {\bf Example.}\\
What's the probability we are dealt a two-pair hand?  FYI: {\bf\em two-pair} in poker is 2 cards in a rank,
2 cards in another (different) rank (these are the {\em pair ranks}), and 1 card from the remaining cards that do not have the pair ranks.\\

\noindent So, for instance $\{\mbox{A}\heartsuit, \mbox{A}\spadesuit, \mbox{K}\diamondsuit, \mbox{K}\heartsuit, 10\heartsuit\}$ is a two-pair with pair ranks Aces and Kings.\\

\noindent SOLUTION: Since a two-pair hand is the same regardless of the order it was dealt,
I choose to work with a sample space whose sample points exclude the order the cards are dealt.
In this sample space, what's the process that leads to a two-pair?

Following what we did in the full-house example\dots
Here's a 3-stage process:

\noindent In stage 1, select two cards of a rank; once this is done, in stage 2, we can select two cards of a different rank than in stage 1; and, once this is done, in stage 3, we can select 1 card from the remaining cards that do not have the pair ranks. But, again,\\

{\em What rank do I select in stage 1?  What rank do I select in stage 2?}\\

\noindent In two-pair, unlike a full-house, the ranks we choose are {\em indistinguishable}!  For example,
the procedure that selected $\{\mbox{A}\heartsuit,\mbox{A}\spadesuit\}$ in stage 1, then selected $\{\mbox{K}\diamondsuit, \mbox{K}\heartsuit\}$ in stage 2, and then
selected $\{10\heartsuit\}$ in stage 3 cannot be distinguished from the procedure that selected $\{\mbox{K}\diamondsuit, \mbox{K}\heartsuit\}$ in stage 1, then selected
$\{\mbox{A}\heartsuit,\mbox{A}\spadesuit\}$ in stage 2, and then selected $\{10\heartsuit\}$ in stage 3.  These are actually the {\em same} hand.

So, we arrive at the following 4-stage process:

Stage 1: select a {\em subset} of 2 ranks -- ${13\choose 2}$ choices. Then

Stage 2: select 2 cards from one of the ranks -- ${4\choose 2}$ choices. Then

Stage 3: select 2 cards from the other rank -- ${4\choose 2}$ choices. Then

Stage 4: select 1 card from the $52-8=44$ cards that do not have the pair ranks -- 44 choices.\\
The basic counting principle says there are
$${13\choose 2}\cdot {4\choose 2}\cdot {4\choose 2}\cdot 44$$
two-pair hands, and the probability is
$$\dfrac{{13\choose 2}\cdot {4\choose 2}\cdot {4\choose 2}\cdot 44}{{52\choose 5}} \approx 0.0475.$$




\vskip .5 in

\noindent {\bf Exercise for students.}\\
\noindent Try re-doing the full-house and two-pair examples by working with the sample space that keeps track of order.  You might just convince yourself
that not keeping track of order is relatively {\em much simpler} and we are much less likely to make arithmetic and reasoning mistakes.







\newpage

\noindent Now for a deeper example that combines some older counting ideas, and a {\em new} idea\dots\\

\noindent {\bf Example and discussion.}\\
Consider the 30 balls examples from pages \pageref{30ballsexample} and \pageref{30ballsexample2}.  We have 30 balls comprised of
16 red, 8 green, and 6 yellow balls.  We randomly line up all the balls left to right in such a way that all line-ups are equally likely.
Compute the probability that there are no two yellow balls next to each other.\\

\noindent Since this is a probability question we should fix a sample space. It seems natural to take the sample space $\Omega$ of all possible color sequences. We learned
that $|\Omega|=\frac {30!}{16!8!6!}$ (see page \pageref{30ballsexample2}).  Please think about why $\Omega$ is equally likely.\\


\noindent Since the sample space of all color sequences is equally likely, we will need to count the number of color sequences that have no two yellow balls adjacent.
We now think about the process that leads to such a color sequence.  Here's an idea:
there's no restriction on the red and green balls here, so we imagine lining up just
these 24 balls (in any fashion) with enough ``space" between them to potentially fit yellow balls anywhere within the sequence as well as the start and the end of the sequence.
Here's a picture of what I'm talking about:
$${\tiny \_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,R\_\_\,G\_\_\,G\_\_\,G\_\_\,G\_\_\,G\_\_\,G\_\_\,G\_\_\,G\_\_}$$

\noindent The picture has $24+1=25$ blanks that correspond to the potential places we can put yellow balls. We want to place yellow balls into the blanks in a way where there is {\em at most one} yellow in a blank (because, if we put two or more yellows in a blank we'd have adjacent yellows).  Therefore, since there are 25 blanks, i.e., (distinct potential positions for the yellows), we just need to select any 6 of them to put our {\em indistinguishable} yellows into. This can be done in ${25\choose 6}$ ways.

And, now we can count how many colors sequences have no two yellows adjacent via the basic counting rule:

Stage 1: select 6 out the 25 possible positions to put the letter $Y$, ${25\choose 6}$ ways. Then

Stage 2: select an anagram of the word containing the $R$'s and $G$'s, $\frac {24!}{16!8!}$ ways.\\
Therefore, there are
$$\displaystyle {25\choose 6}\cdot \dfrac {24!}{16!8!}$$
color sequences with no two yellows adjacent, and the probability is
$$\dfrac{\displaystyle {25\choose 6}\cdot \dfrac {24!}{16!8!}}{\dfrac {30!}{16!8!6!}}=\dfrac {\displaystyle {25\choose 6}}{\displaystyle {30\choose 6}}\approx 0.298.$$

\vskip .5 in
\noindent {\bf Remark. (Cute finding)}\\
Notice the first equality in the probability computation {\em suggests} the intuitive answer to this question:
The $Y$'s can occupy ${30\choose 6}$ possible places in the color sequence equally likely. ${25\choose 6}$ of these colors sequences have no two $Y$'s adjacent when ignoring the $R$'s and $G$'s. The ratio is our answer!




\newpage



%\noindent One more example, this time involving dice.\\

\noindent {\bf Example.}\\
Roll a fair 6-sided die 5 times. Compute the probability of throwing a full-house.\\

\noindent SOLUTION: We'll take $\Omega$ to be set of all ordered 5-tuples of the numbers 1,2,3,4,5,6 with replacement.
There are $|\Omega|=6^5$ equally likely sample points.
We are now going to count the number of full-houses -- sample points like
$$(1,3,3,1,3),\quad (4,4,4,2,2),\quad (6,5,6,5,5),\ \mbox{etc.}$$
\noindent In this example we are choosing to keep track of order unlike the similar situation of a hand dealt from a deck of cards.
So, we should now think about the process that leads to a full-house in the current situation.  Clearly, even with dice rolls,
a full-house will have
3 of one rank and 2 of another (different) rank.
So, similar to the card example,

Stage 1: select a rank for the triple, ${6\choose 1}=6$ ways to complete. Once this is done,

Stage 2: select a rank from the remaining 5 ranks for the double, ${5\choose 1}=5$ ways. Then

Stage 3: select the 3 positions from the 5 possible for our triple, ${5\choose 3}=10$ ways. Then

Stage 4: select 2 of the remaining 2 positions for our double, ${2\choose 2}=1$ way.


\noindent The basic counting rule says there's ${6\choose 1}\cdot {5\choose 1}\cdot {5\choose 3}\cdot {2\choose 2}$ possible full-houses. Therefore, the probability of a full-house (with 5 dice) is
$$\dfrac {{6\choose 1}\cdot {5\choose 1}\cdot {5\choose 3}\cdot {2\choose 2}}{6^5} = \dfrac {6 \cdot 5 \cdot 10 \cdot 1}{7776} = \frac {300}{7776}\approx 0.03858.$$
The reason this calculation was essentially different from the card example is that this experiment is sampling {\em with} replacement, whereas the card example is sampling {\em without} replacement.

In the last example I'll mention that had we ignored the order of the dice rolls then not all throws will be equally likely. We'll discuss this more in the Stars-and-bars counting section on page \pageref{starsandbars2}.\\

%\vskip .1 in

%\noindent A result that we make use of {\em many} times in this course is\dots \\

\noindent {\bf the binomial theorem.}\label{d:binomialtheorem1}\\
For integer $n\ge 1$ and for any real constants $a$ and $b$,
$$\sum_{k=0}^{n} {n\choose k}a^kb^{n-k} = (a+b)^n.$$

\vskip .25 in
\noindent Notice the symmetry in the results: since $(a+b)^n=(b+a)^n$, we also have
$$\sum_{k=0}^{n} {n\choose k}a^kb^{n-k} = \sum_{k=0}^{n} {n\choose k}a^{n-k}b^{k} = (a+b)^n.$$

\vskip .25 in

\noindent Taking $a=b=1$ in the binomial theorem gives us the following:\\

\noindent {\bf Corollary to the binomial theorem.}\\
$$\displaystyle \sum_{k=0}^n {n\choose k} = 2^n.$$






\newpage

\noindent {\bf Remark.} ({\bf Important interpretation of the corollary.})\label{subsetsof123}\\
\noindent The total number of subsets of an $n$-element set is $2^n$:\\
$$\sum_{k=0}^n {n\choose k} = \underbrace{{n\choose 0}}_{\stackrel{\#\mbox{\small size $0$}}{\mbox{\small subsets}}} +
\underbrace{{n\choose 1}}_{\stackrel{\#\mbox{\small size 1}}{\mbox{\small subsets}}} + \cdots +
\underbrace{{n\choose n-1}}_{\stackrel{\#\mbox{\small size $n-1$}}{\mbox{\small subsets}}} +
\underbrace{{n\choose n}}_{\stackrel{\#\mbox{\small size $n$}}{\mbox{\small subsets}}} = 2^n,$$
and, since every subset of an $n$-element set will have exactly one of these sizes, the expression above represents the total number of subsets.\\




\noindent For example, if $A=\{1,2,3\}$ (here, of course, $|A|=3$), then according to the corollary there must be $2^3=8$ subsets of $A$.
Here they are:
$$\varnothing, \{1\}, \{2\}, \{3\}, \{1,2\}, \{1,3\}, \{2,3\}, \{1,2,3\}.$$
\noindent There is ${3\choose 0}=1$ subset with 0 members; namely, the null set $\varnothing$.

\noindent There are ${3\choose 1}=3$ subsets with 1 member; namely, $\{1\},\{2\},\{3\}$.

\noindent There are ${3\choose 2}=3$ subsets with 2 members; namely, $\{1,2\},\{1,3\},\{2,3\}$.

\noindent There is ${3\choose 3}=1$ subset with 3 members; namely, $A$ itself: $\{1,2,3\}$,\\
\noindent and, of course, $1+3+3+1=8=2^3$.\\

\vskip .5 in


\noindent {\bf Example.}\\
Suppose $A$ is a finite set with $|A|=7$, and select a subset $B$ {\bf \em uniformly at random}\label{uniformlyatrandom1}, which means
all subsets of $A$ are equally likely to be chosen.
Compute the probability that $|B|=4$.\\

\noindent SOLUTION: There are $2^7$ possible subsets to be selected, each equally likely.  The event that a subset is size 4 has cardinality ${7\choose 4}$.
Therefore, the probability is
$$\dfrac {{7\choose 4}}{2^7} = \dfrac {35}{128} \approx 0.2734.$$

\bigskip

\noindent An alternate solution to this problem using older ideas goes like this: \\

\noindent List the 7 members of $A$,
and flip a fair coin $n=7$ times. If the $i$th is a head, put the $i$th member of $A$ in the subset; else don't.
Then, each sequence having exactly 4 heads corresponds to
a distinct $4$-element subset of $A$.  From the example on page \pageref{7cointosses4heads}
there are $\dfrac {7!}{4!3!}$ sequences having exactly 4 heads.  Moreover, there are $2^7$ possible sequences which are equally likely because the coin is fair.
Thus, we arrive at the same probability: $$\dfrac {\dfrac{7!}{4!3!}}{2^7}.$$\\
This last calculation is related to the {\bf\em binomial distribution}\label{binomdist1} (see page \pageref{d:binomialnp}) which we discuss at length in the discrete random variables section.


\newpage


\noindent {\bf Example.}\\
Suppose we have $n$ people. We define a {\em club} to be any subset of
these people, where one of them is identified as the {\em leader} along with the remaining members (possibly empty).
How many clubs are possible?\\

\noindent Just to clarify, a club must be a subset of size {\em at least} 1 since a subset of size 0 would have no leader.
A club can be a subset of size 1: a leader with no other members. Finally, a subset with the same members having different leaders identified are considered different clubs.
So, for instance, if we have $n=4$ people: $a,b,c,d$, then $\{a,b,c\}$ with $a$ as leader and $\{a,b,c\}$ with $b$ as leader are two {\em different} clubs.\\

\noindent SOLUTION:  Here's one solution. We, again, consider a process to get a {\em club} as defined above.  One process is as follows:

Stage 1: select the leader from the $n$ people, ${n\choose 1}=n$ ways to complete this. Then

Stage 2: select a subset from the remaining $n-1$ members to round out the club, $2^{n-1}$ ways.\\
\noindent The basic counting rule says there are
\begin{equation}n2^{n-1}\label{eq:clubcount1}\end{equation}
clubs.\\

\noindent Here's {\em another} solution where we first fix the size $k$ of the club, count the number of clubs of size $k$, and then sum from $k=1$ to $k=n$ to get all the clubs.
To this end, fix $k$ between $1$ and $n$ (inclusive). Here's a process that leads to a club of size $k$:

Stage 1: select a subset of size $k$ from the $n$ possible, ${n\choose k}$ ways. Then

Stage 2: identify one the people in the subset in stage 1 as the leader, ${k\choose 1}=k$ ways.\\
\noindent The basic counting rule says there are $k{n\choose k}$ clubs of size $k$. Therefore, there are
\begin{equation}\sum_{k=1}^n k{n\choose k}\label{eq:clubcount2}\end{equation}
\noindent clubs.\\

The answers (\ref{eq:clubcount1}) and (\ref{eq:clubcount2}) look different, but if we did the counting right, they should be the same.\\

\vskip .8 in

\noindent {\bf Exercise for the student$^{\dagger}$.}\\
Show that
$$\sum_{k=1}^n k{n\choose k} = n2^{n-1}.$$

\noindent $^{\dagger}$ This is a nice (and, if you are doing it right) short calculation. Seriously, do this {\bf\em now}!\\
Hint: you'll need to recognize to use the corollary to the binomial theorem.









\newpage







\noindent  We now collect some important mathematical results involving binomial coefficients.\\

\noindent $\bullet$ {\bf FACT 1}: For any integers $n\ge 1$ and $0\le k\le n$,
$$\displaystyle {n\choose k} = {n\choose n-k}.$$

\noindent Here's a very simple algebraic proof:
$$\displaystyle {n\choose n-k} = \dfrac {n!}{(n-k)![n-(n-k)]!} = \dfrac {n!}{(n-k)!k!} = {n\choose k}.$$

\bigskip

\noindent Here's a very simple combinatorial proof:

\noindent From a set of $n$ people, we plan to select $k$ winners and the remaining $n-k$ will be losers.
${n\choose k}$ represents the number of subsets of $k$ winners.  Since every subset of $k$ winners we select corresponds to a unique subset of $n-k$ losers, the subsets of $k$ winners is in one-to-one correspondence with the subsets of $n-k$ losers. Therefore, ${n\choose k}={n\choose n-k}$.\\

\vskip .5 in


\noindent {\bf FACT 2}: ({\bf Pascal's identity})\label{pascalsidentity}\\
\noindent For any integers $n\ge 1$ and $0<k<n$ (i.e., $1\le k\le n-1$),
$${n\choose k} = {n-1\choose k} + {n-1\choose k-1}.$$

\noindent Here's an algebraic proof:\\
Let $n$ and $k$ be integers as described in the statement.
\begin{eqnarray*}
{n-1\choose k} + {n-1\choose k-1}& = & \frac {(n-1)!}{k!(n-1-k)!} + \frac {(n-1)!}{(k-1)!(n-k)!}\\
& = & \frac {(n-1)!(n-k)}{k!(n-1-k)!(n-k)} + \frac {(n-1)!\,k}{(k-1)!(n-k)!\,k}\\
& = & \frac {(n-1)!\,n-(n-1)!\,k}{k!(n-k)!} + \frac {(n-1)!\,k}{k!(n-k)!}\\
& = & \frac {n!}{k!(n-k)!} = \displaystyle {n\choose k}.\\
\end{eqnarray*}


\vskip .5 in



\newpage






\begin{center}{\bf \Large EXERCISES.}\label{problemset2}\end{center}


%\noindent {\bf Elementary exercises/applications.}\medskip

\noindent {\bf 1.} In how many ways can a coach create a tee-ball team of 9 players from a collection of 15 players?\\

\noindent {\bf 2.} Harry buys 5 cookies from Insomnia Cookie. In how many ways can he create the box so that all the flavors are distinct if there are 40 different flavors?\\

\noindent {\bf 3.} A barber is creating a fresh fade. He has to choose 4 different scissors from a collection of 16 scissors. \\
In how many ways can the barber select his scissors?\\

\noindent {\bf 4.} George is selecting 6 different fencing uniforms to take with him on a trip. He has 24 uniforms total. How many different selections of uniforms can George take with him?\\

\noindent {\bf 5.} Vincent has all 9 trophies from High Tide in his room. He wants to move 4 of them to Simon's room. In how many ways can Vincent select 4 trophies to give to Simon?\\

\noindent {\bf 6.} In Spikeball, there are 8 valid starting positions around the circular net that one can stand in. Four (4) people are playing Spikeball. In how many ways can they select 4 spots out of these 8 to be occupied at the start if we only care about the positions of the people relative to each other?\\

\noindent {\bf 7.} Miguel has 20 different earrings that he wears regularly. In how many ways can he select two to wear on a given day?\\

\noindent {\bf 8.} Brandon is running a Hot Dog Joint that has 32 distinct orders. In how many ways can a customer purchase 8 distinct items from the menu?\\

\noindent {\bf 9.} How many binary sequences (sequences only consisting of $0$ and $1$) of length $14$ have exactly $6$ ones?\\

\noindent {\bf 10.} Kevin works at the Applied Physics Laboratory. There are 21 projects being worked on, and he must choose 4 to supervise. In how many ways can Kevin do this?\\

%\noindent {\bf Problems.}\medskip

\noindent {\bf 11.} For each integer $n\ge 1$, simplify $\sum_{k=0}^n (-1)^k{n\choose k}$.  What does this result say combinatorially about a set of size $n$?\\

\noindent {\bf 12.} Compute this sum: $\displaystyle \sum_{k=0}^n \frac 1{k!(n-k)!}$.  You should get a function of $n$ alone.\\

\noindent {\bf 13.} If $(2x-1)^{10}$ is written out as a polynomial in $x$ of degree 10, determine the coefficient of $x^{8}$ in this expansion.  Then do $x^{5}$.\\



\newpage




\noindent {\bf 14.} A dissertation defense committee at Johns Hopkins is a group of 5 people: one is the student's dissertation advisor,
3 are eligible members of the faculty in the advisor's department, and an eligible faculty from outside the advisor's department.
Rhee Lee Smart is trying to form her
dissertation committee. Her dissertation advisor is Justin Case from the Department of Civil Disobedience (DOCD).  There are 600 other eilgible university faculty that can serve but only 8 of these belong to DOCD.  How many dissertation committees can Rhee form?\\


\noindent {\bf 15.} From a pack a 20 m\&m's there are 5 red, 4 blue, 3 green, 6 yellow, and 2 orange.  Assume the candies are well-mixed.  Only simplify if it's something nice.\\
{\em These are separate questions unless noted otherwise}.\\
(a) We grab a handful of 4 m\&m's from this pack.  What's the probability that you grab exactly 2 red m\&m's?\\
(b) The plan is to line up all 20 m\&m's.  What's the chance that no two red m\&m's are adjacent?\\
(c) (continued from part (b)) What's the chance the exactly two red m\&m's are adjacent?\\

\noindent {\bf 16.} Roll a fair 6-sided die 5 times. Compute the probability you throw a two-pair.\\

\noindent {\bf 17.} I deal you 8 cards from a (well-shuffled) standard deck of 52. What's the probability that you get exactly 2 of each suit?\\



\noindent {\bf Somewhat challenging (?) problems.}\medskip

\noindent {\bf 18.} $A$ is a finite set with $n>1$ members.  We plan to select a subset of $A$ at random but {\em not} uniformly at random.
In fact, we are told that if $|\omega|=k$, then $P(\{\omega\}) = \dfrac k{n2^{n-1}}$.  Notice $P(\{\varnothing\})=0$.\\
(a) $S_k$ is the event that we select a subset of size $k$ from $A$, $0\le k\le n$.  Compute $P(S_k)$.\\
(b) Show that $\displaystyle \sum_{\omega\in \Omega} P(\omega)= 1$.\\


\noindent {\bf Remark.} It might help to first do this problem in the special case where $A=\{1,2,3\}$ and
$\Omega = \left\{ \varnothing, \{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}\right\}$.\\

\noindent {\bf 19.} We turn over the cards of a well-shuffled
standard deck of 52 cards one by one. What's the probability that
all the Kings are turned over before the first Ace?  What's the probability that exactly two
Kings are turned over before the first Ace?\\




%\newpage

%\noindent {\bf Digression.}\\

%\noindent Let $A$ be a finite set.\\

%\noindent {\bf Partitions.}\\
%\noindent A {\bf\em partition}\label{d:partition} of $A$ is a {\em set} of {\em nonempty} pairwise disjoint subsets that exhaust $A$.\\


%\noindent Subsets are {\bf\em pairwise disjoint}\label{d:pairwisedisjoint}
%means that no two of the subsets have a common member.  The subsets {\bf\em exhaust $A$}\label{d:exhaust}
%means that if we union all the subsets we get $A$.  Here's a Venn diagram illustration of this:
%
%\includegraphics*[-100,0][200,140]{exhaustsA.jpg}
%\begin{center}{\bf Figure.} The subsets $A_1,A_2,A_3,A_4,A_5$ are pairwise disjoint and exhaust $A$\end{center}
%
%\noindent So, a partition is a grouping of all the members of $A$ into nonempty subsets, where each member belongs to one subset.
%We may call a subset belonging to a partition a {\bf\em part}\label{d:part}. The {\bf\em partition size}\label{d:partitionsize} is the number of parts.\\
%
%\vskip .5 in
%
%\noindent {\bf Example.}\\
%Suppose $A=\{a,b,c,d\}$.  List all the partitions of $A$.  How many parts does each have?  How many partitions of each partition size are there?\\
%
%\noindent SOLUTION:
%
%\noindent There are 11 total partitions.  Here they are:\medskip
%
%1 partition with 1 part:
%$$\underbrace{\Big\{\{1,2,3\}\Big\}}_{\mbox{\small 1 part}}, \ \ \underbrace{\Big\{\{1,2\},\{3\}\Big\}, \ \ \Big\{\{1,3\},\{2\}\Big\}, \ \ \Big\{\{1\},\{2,3\}\Big\}}_{\mbox{\small 2 parts}}, \ \ \underbrace{\Big\{\{1\},\{2\},\{3\}\Big\}}_{\mbox{\small 3 parts}}$$
%\noindent There's 1 partition with 1 part, 3 partitions with 2 parts, and 1 partition with 3 parts.\\
%
%\qquad \qquad $\Big\{\{1,2,3,4\}\Big\}$\medskip
%
%7 partitions with 2 parts:
%
%\qquad \qquad $\Big\{\{1\},\{2,3,4\}\Big\},\ \ \Big\{\{2\},\{1,3,4\}\Big\},\ \ \Big\{\{3\},\{1,2,4\}\Big\},\ \ \Big\{\{4\},\{1,2,3\}\Big\}$\smallskip
%
%\qquad \qquad $\Big\{\{1,2\},\{3,4\}\Big\},\ \ \Big\{\{1,3\},\{2,4\}\Big\},\ \ \Big\{\{1,4\},\{2,3\}\Big\}$\medskip
%
%6 partitions with 3 parts:
%
%\qquad \qquad $\Big\{\{1\},\{2\},\{3,4\}\Big\},\ \ \Big\{\{1\},\{3\},\{2,4\}\Big\},\ \ \Big\{\{1\},\{4\},\{2,3\}\Big\},$\smallskip
%
%\qquad \qquad $\Big\{\{2\},\{3\},\{1,4\}\Big\},\ \ \Big\{\{2\},\{4\},\{1,3\}\Big\},\ \ \Big\{\{3\},\{4\},\{1,2\}\Big\}$\medskip
%
%1 partition with 4 parts:
%
%\qquad \qquad $\Big\{\{1\},\{2\},\{3\},\{4\}\Big\}$.



%\newpage


%\noindent {\bf Remark.}\\
%\noindent Because we defined a partition to be a {\em set} the order the parts appear within it is irrelevant.
%
%
%
%Depending on the
%application we may or may not want to keep track of the order of the subsets in this decomposition.

\newpage


\noindent {\bf Counting with multinomial coefficients.}\label{multinomialcoeff1}\\

\noindent Consider the following problem:\\

\noindent We have $n$ distinct objects.

\noindent We have $r$ distinct ``boxes" labeled $1$ thru $r$.

\noindent We want to put the $n$ objects into the boxes with the following restrictions:

1. Each object is put into only one box.

2. The order in which objects are put into boxes is irrelevant.

3. We have prescribed nonnegative integers $n_1,n_2,\dots,n_r$ such that
$$n_1+n_2+\cdots+n_r=n,$$

that tell us how many objects to put in each ``box": box $i$ gets $n_i$ of the objects.

\noindent We ask:

\begin{center}{\em How many ways can we assign the objects to these boxes?}\end{center}

\noindent Let's start with, say, box 1.
We need to put $n_1$ objects into box 1 and order is irrelevant.
So, select a subset of $n_1$ objects for box 1.
Now, there are $n-n_1$ objects remaining.
Now, say, go to box 2, which is to
receive $n_2$ objects.
Select a subset of $n_2$ objects from the $n-n_1$ remaining for box 2, and so on.\\

\noindent This scheme creates an $r$-stage procedure that fits the paradigm of the basic counting rule:

Stage 1: select $n_1$ for box 1.

Stage 2: select $n_2$ from those remaining (after the first stage) for box 2.

Stage 3: select $n_3$ from those remaining (after the first 2 stages) for box 3.

$\vdots$

Stage $r-1$: select $n_{r-1}$ from those remaining (after first $r-2$ stages) for box $r-1$.

Stage $r$: there are now $n_r$ remaining, just put these in $r$.\\

\noindent Here's the resulting count:\medskip



\begin{center}$\underbrace{{n\choose n_1}}_{\frac {n!}{n_1!(n-n_1)!}}\cdot \underbrace{{n-n_1\choose n_2}}_{\frac {(n-n_1)!}{n_2!(n-n_1-n_2)!}}\cdot
\underbrace{{n-n_1-n_2\choose n_3}}_{\frac {(n-n_1-n_2)!}{n_3!(n-n_1-n_2-n_3)!}}\cdots
\underbrace{{n-n_1-n_2-\cdots-n_{r-2}-n_{r-1}\choose n_{r}}}_{\frac {(n-n_1-\cdots - n_{r-1})!}{n_r!(n-n_1-\cdots-n_{r-1}-n_r)!}\ =\ 1}.$\end{center}

\noindent You may notice that each binomial coefficient in this product from the
second one onward has the property that the factorial in the numerator appears as the denominator in
the previous binomial coefficient.  So, there's a significant amount of cancellation (a telescoping product, in fact), and I leave this\dots\\

\noindent {\bf Exercise for the student.}\\
Show that the product of binomial coefficients above reduces to$^*$ $\displaystyle \frac {n!}{n_1!n_2!\cdots n_r!}.$\\

\noindent $^*$ This is the number of anagrams of an $n$-letter word with $n_i$ letters of type $i$, $i$ from 1 to $r$.\\




\newpage





\noindent {\bf Notation.}\\
\noindent We may use the symbol
$$\displaystyle {n\choose n_1,n_2,\dots,n_r}$$
to represent $\displaystyle \frac {n!}{n_1!n_2!\cdots n_r!},$
and refer to it as a {\bf\em multinomial coefficient}\label{multinomialcoeff2}.\\


\vskip .3 in

\noindent {\bf Remark.}\label{multinomialremark1} (the multinomial coefficient is counting {\em ordered} $r$-tuples)\\
It's worth pointing out the sample points that the multinomial coefficient is counting are {\em ordered} $r$-tuples of subsets with
specified sizes for each entry, where the $r$ subsets are pairwise disjoint and
union to the entire set of $n$ objects.
In this context we also assume these subsets are {\em nonempty}, because
if a box is to receive no objects we can just remove the box from the experiment.\medskip

\noindent I'll also mention that because we can distinguish between the boxes, if 2 subsets of the same size in a $r$-tuple are swapped,
then this leads to a different assignment.\\


\vskip .3 in

\noindent {\bf Example.}\\
\noindent We have 6 balls numbered $1$ thru $6$ that are to be put into 3 boxes, where box 1 gets $3$, box 2 gets $2$, and box 3 gets $1$ ball.
How many ways can this be done?\\

\noindent SOLUTION:\\

There are $\displaystyle {6\choose 3,2,1} = \frac {6!}{3!2!1!}=60$ ways.\\

\noindent In this example, the boxes are clearly distinguishable from each other because each box is getting a different number of balls:
box 1 is the one getting 3 balls, box 2 is the one getting 2 balls, and box 3 is the one getting 1 ball.\\

\vskip .3 in

\noindent {\bf Example.}\\
\noindent A teacher has 6 children, $a,b,c,d,e,f$, in a kindergarten class.  She plans to assign each child a task.  There are
3 tasks:

task 1: move desks in a circle

task 2: throw out the trash

task 3: erase the blackboard.

\noindent The teacher plans to assign 3 to task 1, 2 children to task 2, and 1 child to task 3.\\
\noindent How many ways can the teacher assign the children to a task (each child gets exactly one task)?\\

\noindent SOLUTION: This is the same as the previous example, $\displaystyle {6\choose 3,2,1}$. ({\em Do you see why?})\\

\noindent Again, in this problem we can clearly distinguish between the tasks(boxes) since each task requires different number of children.



\newpage



\noindent {\bf Remark.}\\
The multinomial coefficient $\displaystyle {n\choose n_1,n_2,\dots,n_r}$
counts the number of ways we can assign subsets of objects to distinct boxes --
putting subsets of objects to boxes - where each box gets a prescribed number of objects: $n_i$ to box $i$.
But, from the formula at the top of page \pageref{multinomialcoeff2}, the multinomial coefficient
is also the number of anagrams of an $n$-letter word, where there are $n_i$ {\em indistinguishable} letters of type $i$ --
we are mapping the $n$ positions(objects) to $r$ letters(boxes) .  Here's the connection between these two approaches\dots \\

\noindent In the last example, let's call the tasks $D$ (for ``move the desks"), $T$ (for ``throw out the trash"), and $B$ (for ``erase the blackboard"), and fix a line-up of the
6 children left to right, say we choose $(a,b,c,d,e,f)$ as our line-up.  Create a 6-letter word with 3 $D$'s, 2 $T$'s, and $1$ $B$. For example, the word $DDDTTB$, thought of as
$$\dfrac{D}{\small a}\dfrac{D}{\small b}\dfrac{D}{\small c}\dfrac{T}{\small d}\dfrac{T}{\small e}\dfrac{B}{\small f},$$
is telling us that $a,b$ and $c$ are being assigned to job $D$, $d$ and $e$ are being assigned to job $T$, and $f$ is being assigned to job $B$.\\

\noindent Below we demonstrate the correspondence: the ordered 3-tuple is sending subsets of children to tasks and the anagram is mapping of children to tasks.  The two approaches are just different ways to think about the same thing. \\

\begin{align*}
\Big( \{a,b,c\},\{d,e\},\{f\}\Big) &=: \{a,b,c\}\mapsto D, \{d,e\}\mapsto T, \{f\}\mapsto B\\
DDDTTB &=: a\mapsto D, b\mapsto D, c\mapsto D, d\mapsto T, e\mapsto T, f\mapsto B\\
&  \\
\Big( \{b,d,f\},\{c,e\},\{a\}\Big) &=: \{b,d,f\}\mapsto D, \{c,e\}\mapsto T, \{a\}\mapsto B\\
BDTDTD &=: a\mapsto B, b\mapsto D, c\mapsto T, d\mapsto D, e\mapsto T, f\mapsto D\\
&  \\
\Big( \{b,c,e\},\{a,f\},\{d\}\Big)&=: \{b,c,e\}\mapsto D, \{a,f\}\mapsto T, \{d\}\mapsto B\\
TDDBDT &=: a\mapsto T, b\mapsto D, c\mapsto D, d\mapsto B, e\mapsto D, f\mapsto T\\
&\vdots \\
\end{align*}





\newpage







\noindent {\bf Example.}\\
In the card game {\em Bridge} there are 4 people called North, South, East, West and a standard deck of 52 cards.
A {\em Bridge deal} is when we deal 13 cards to each of these 4 people -- so that there are four (4) 13-card hands being dealt.
For this problem assume the order in which the 13 cards are dealt to each person is irrelevant.  Moreover, if, for instance, everyone took their hand and
rotated to the person on the right then that would be considered a {\em different} Bridge deal.  How many Bridge deals are possible?\\

\noindent SOLUTION:\\
\noindent Since we distinguish the players hands, the way we view this is like this:
the dealer deals a 13-card hand to, say, North; then, from the remaining $52-13=39$ cards,
she deals a 13-card hand to, say, South; and, so on. A 4-stage procedure!
We have
$${52\choose 13}\cdot {39\choose 13}\cdot {26\choose 13}\cdot {13\choose 13}.$$
Similarly, 52 distinct objects, 4 boxes and 13 objects to each box. Answer: $\displaystyle {52\choose 13,13,13,13}$.\\

\vskip .5 in

\noindent {\bf ADVICE:}
\noindent For a Bridge deal it is sometimes helpful to think of the sample points as the ordered 4-tuple of 13-card hands:
$$\Big(\underbrace{\{\mbox{\small 13-card hand}\}}_{\mbox{\small North's hand}}, \underbrace{\{\mbox{\small 13-card hand}\}}_{\mbox{\small South's hand}},
\underbrace{\{\mbox{\small 13-card hand}\}}_{\mbox{\small East's hand}}, \underbrace{\{\mbox{\small 13-card hand}\}}_{\mbox{\small West's hand}} \Big).$$



\vskip .5 in


\noindent {\bf Example.}\\
How many Bridge deals have North receiving all the Aces?\\

\noindent SOLUTION: \\
Every hand North receives will now look like $\{\mbox{A}\clubsuit,\mbox{A}\diamondsuit,\mbox{A}\heartsuit,\mbox{A}\spadesuit\}\cup \{\mbox{9-card hand}\}$, and there are no
restrictions on the 13-card hands of the other three players. If we imagine listing out all of North's hands, you will see that the only differences are the subset of size 9 that are chosen from the 48 non-Ace cards available.  So counting the number of ways North can receive all the Aces is the same as taking
the 4 Aces out of the original deck and dealing North
only 9 cards from the 48, and 13 cards for each of the other players. The answer is
$$\displaystyle {4\choose 4}{48\choose 9}\cdot {39\choose 13}\cdot {26\choose 13}\cdot {13\choose 13}={48\choose 9,13,13,13}.$$
Alternatively, counting the number of hands where North receives all the Aces can be thought of as a 2-stage process: select the 4 Aces, ${4\choose 4}=1$ way to do this; then, select the other 9 cards from 48, ${48\choose 9}$. The rest of the counting is the same as above.


\newpage

\noindent All the examples thus far in this section have fit the paradigm of the multinomial coefficient: $n$ distinct objects going into
$r$ distinct boxes with $n_i$ objects for box $i$ ($i=1,2,\dots,r$.  Here's an example where we can't distinguish between the boxes\dots



\vskip .5 in

\noindent {\bf Example and discussion.}\\
\noindent A teacher has 6 children, $a,b,c,d,e,f$, in a kindergarten class.  She plans to create 3 teams of size 2.
How many ways can the teacher create the teams?\\

\noindent SOLUTION:

\noindent There is now a subtlety that we should discuss.   Had we said the 3 teams were the A-team, the B-team, and the C-team, then these teams are clearly labeled.
We'd have to form the assignment by saying which 2 children are the A-team, which 2 are the B-team, and which 2 are the C-team, and we can distinguish between assignments  because the teams are clearly labeled.\\

\noindent But, in our current problem, the teams are {\em not} labeled; moreover, they are all the same size, so we cannot distinguish between the teams by their size either.
The teams are now {\em indistinguishable}! Thus, for instance,
$$\begin{array}{ccc}
\Big( \{a,d\},\{b,f\},\{c,e\}\Big), & \Big( \{a,d\},\{c,e\},\{b,f\}\Big), & \Big( \{b,f\},\{a,d\},\{c,e\}\Big),\\
 & & \\
\Big( \{b,f\},\{c,e\},\{a,d\}\Big), & \Big( \{c,e\},\{a,d\},\{b,f\}\Big), & \Big( \{c,e\},\{b,f\},\{a,d\}\Big)\ \end{array}$$
all represent the same 3 teams. i.e., they are all the same assignment.  So, in the current problem we need to ignore the order in which we place the subsets in the assignment.
Counting the number of assignments as if these were really an A-team, a B-team, and a C-team would be {\em over-counting} by a factor of $3!$ since, for any ordered 3-tuple, any rearrangement of the subsets will lead to the {\em same} assignment -- and there are $3!$ ways to order the 3 distinct subsets within the 3 positions.
Consequently, there are
$$\dfrac{\displaystyle {6\choose 2,2,2}}{3!} = 15$$
ways for the teacher to create 3 teams of size 2 each from the 6 children.\\


\vskip .5 in

\noindent {\bf Remark.}\\
\noindent If the ``boxes" all get differing numbers of objects in each, then we can distinguish between the boxes by the number of objects in them.
When some (or all) of the subsets being assigned to the boxes are the same size, however, we should be a bit careful and think about the situation.
Usually, whether or not boxes are distinguishable is clear from context. Hopefully the last example demonstrated this.  \\






%\noindent For illustration purposes I list out the 12 sample points here:\\
%
%$\Big( \{1,2\},\{3\}, \{4\} \Big), \quad \Big( \{1,2\},\{4\}, \{3\} \Big), \quad \Big( \{1,3\},\{2\}, \{4\} \Big), \quad \Big( \{1,3\},\{4\}, \{2\} \Big),$
%
%%$\Big( \{1,4\},\{2\}, \{3\} \Big), \quad \Big( \{1,4\},\{3\}, \{2\} \Big), \quad \Big( \{2,3\},\{1\}, \{4\} \Big), \quad \Big( \{2,3\},\{4\}, \{1\} \Big),$
%
%$\Big( \{2,4\},\{1\}, \{3\} \Big), \quad \Big( \{2,4\},\{3\}, \{1\} \Big), \quad \Big( \{3,4\},\{1\}, \{2\} \Big), \quad \Big( \{3,4\},\{2\}, \{1\} \Big)$.\\
%\noindent and, clearly, there are 12 such assignments.\\
%
%\noindent In each ordered 3-tuple above, the first entry has the chips for box 1, the second entry those for box 2, and the last entry those for box 3.
%The boxes are assumed distinguishable -- maybe the boxes have numbers $1,2$ and $3$ stamped on them or, maybe, they are different color/shaped boxes -- so that
%%when a chip number enters it we will know which box that chip belongs to.  The sample points
%$$\Big( \{1,2\},\{3\}, \{4\} \Big)\quad \mbox{and}\quad \Big( \{1,2\},\{4\}, \{3\} \Big)$$
%are considered different, because, in the former, chip 3 is in box 2 while chip 4 is in box 3, but, in the latter, this is reversed.\\
%
%\noindent In this example, it should be clear that box 1 is definitely distinguished from boxes 2 and 3 since it's the only box that gets 2 objects.
%





\newpage




\noindent {\bf Example.}\\
\noindent We divide up a standard deck of 52 cards into 4 piles of 13 cards.  How many divisions are possible?\\

\noindent SOLUTION: \\
\noindent This is almost the same question as the Bridge deals
problem from earlier, except now the piles are indistinguishable.
Visualize yourself doing this, then ask yourself: If, after dividing the deck up twice and comparing the resulting divisions (sample points), I find that
the piles each have exactly the same cards in each, do I {\em really} want to say these are different piles?  The answer should be {\em probably not}!  There is nothing in this
question that suggests we need to distinguish the piles from each other.  The answer would then be
$$\dfrac {\displaystyle {52\choose 13,13,13,13}}{4!}$$
divisions of the deck into 4 equal-sized piles.\\

\vskip .5 in


%\noindent {\bf Example.}\\
%\noindent There are $3^{15}$ ternary strings, i.e., sequences of length 15 where each entry is $0,1,$ or $2$ repetition allowed.  How many ternary strings have exactly 5 each of %0's, 1's, and 2's?\\


%\noindent SOLUTION:\\
%I think the clearest solution to this problem is to visualize one such ternary string having 5 each of $0,1,$ and $2$, say, this one:
%$$012012012012012.$$
%Then recognize that every anagram of this word gives another ternary sequence with 5 each of these same digits. Therefore, there are
%$$\dfrac {15!}{5!5!5!}$$
%ternary sequences with 5 0's, 5 1's, and 5 2's.\\








%\newpage

\noindent {\bf Example and discussion.}\\
We need to assign 10 people to 4 distinct jobs -- each person gets one job. Jobs 1, 2, and 3 require 2 people each while job 4 requires 4 people.
How many assignments of jobs to people are there?\\

\noindent SOLUTION: There are $\displaystyle {10\choose 2,2,2,4} = \frac {10!}{2!2!2!4!} = 18900$ assignments possible.\\

\noindent Let's think for a minute about the objects that we are counting.  If we call the 10 people
$a,b,c,d,e,f,g,h,i,j$, then we can think of an assignment as an ordered 4-tuple of subsets like this
$$\Big(\underbrace{\{d,g\}}_{\mbox{\small job 1}},\underbrace{\{a,h\}}_{\mbox{\small job 2}},\underbrace{\{b,i\}}_{\mbox{\small job 3}},\underbrace{\{c,e,f,j\}}_{\mbox{\small job 4}} \Big).$$
The subsets of people assigned to each job are disjoint -- no one is assigned more than one job -- and, their union is all 10 people -- everyone is assigned a job.\\

\noindent Suppose the first 3 jobs (having 2 people in each) are really the same job, say, cleaning windows.  What changes with the answer above?\\


\noindent Now, the same subsets of people assigned to jobs $1,2,$ and $3$ in a different order is the same assignment.  So, we get
$$\dfrac {\displaystyle \displaystyle {10\choose 2,2,2,4}}{3!}$$
instead.



\newpage


\begin{center}{\bf \Large EXERCISES.}\label{problemset3}\end{center}

%\vskip .5 in

\noindent {\bf 1.} How many ways can 30 distinct objects be divided into 3 distinct subsets of respective sizes 16, 8 and 6?\\

\noindent {\bf 2.} 11 girls are to be assigned soccer positions (one girl to each position):

1 forward, 5 midfielders, 4 defenders, and 1 goalie.

\noindent How many different assignments are possible?\\
\noindent How many assignments having Angela as a defender are possible?\\

\noindent {\bf 3.} Gary is creating a workout. The order of the exercises he performs is irrelevant. Out of the 28 machines, in how many ways can he select 4 machines to do each day of the week with no repeats?\\

\noindent {\bf 4.} Jan is packing up his 18 math textbooks into 3 boxes. In how many ways can he do this if the 3 boxes hold $4,6,$ and $8$ books, respectively?\\

\noindent {\bf 5.} In how many ways can 15 students be split into 3 groups to work on a project?  Assume there is at least one person in each group.\\

\noindent {\bf 6.} How many distinct arrangements of FREDTORCASOPROB are there?\\

\noindent {\bf 7.} A stack of 18 dinner plates is created with 4 red, 4 blue, 5 green, and 5 yellow plates. How many distinct stacks are possible?\\

\noindent {\bf 8.} Harry orders 1 of each of the 40 flavors Insomnia cookie has and then puts them into 8 boxes of 5 cookie capacities. In how many ways can Harry arrange the cookies?\\

\noindent {\bf 9.} Dan buys 15 Pokemon Card packs, of which 6 are Shining Fates, 4 are Roaring Skies, 3 are Sword and Shield, and 2 are Celebrations. He wants to open all 15 packs. In how many distinguishable orders can he do this?\\

\noindent {\bf 10.} A 6-sided die is rolled $6n$ times. How many sequences of rolls have exactly $n$ of each of the 6 values?\\

\noindent {\bf 11.} Gabe places an order from Billabong. He purchases 12 shirts: 3 Wrangler Series, 3 Simpsons Collaboratory, 3 50th Anniversary, and 3 Basic. He wants to wear these 12 shirts on 12 consecutive days. In how many distinct ways can Gabe wear the shirts in a 12-day period, assuming he is cleanly and, therefore, wears each shirt exactly once over this period?\\





\newpage





\noindent {\bf Stars-and-bars}\label{starsandbars2}\\

\noindent The experiment here is a slight modification to the sampling with replacement scheme, and we'll get sample points that we haven't quite seen yet.\\

\bigskip

\noindent {\bf The stars-and-bars experiment.}\\
\noindent We have a set of $r$ distinct labels.  We sample $n$ of these labels with replacement, but {\em ignore the order in which they were drawn}.\medskip

\vskip .2 in

\noindent {\bf Basic example.}\\
\noindent If the $r=3$ labels were $\{1,2,3\}$ and we sample $n=2$ with replacement -- ignoring the order in which we selected them, then I'll write the possible sample points like this:
$$\{1,1\}, \{2,2\},\{3,3\},\{1,2\}, \{1,3\}, \{2,3\}.$$
Here, $\{1,1\}$ means we drew label 1 twice, $\{1,3\}$ means we drew label 1 and label 3 (each once), etc.
There are 6 sample points total. Please convince yourself that we're not missing any.  I stress that, since we are ignoring order, the sample
point $\{1,2\}$, for instance, is the same as $\{2,1\}$ and it only needs to be represented once. \medskip

\vskip .2 in

\noindent {\bf Remark.}\\
\noindent WARNING: Please do {\em not} view the sample points in this example as sets, because, if we did, then $\{1,1\}=\{1\}$ and we'd lose the interpretation that
the label 1 appears $n_1=2$ times.
In this context, we view the sample points above as {\em multisets}.  As the example illustrates, we can think of a multiset as an {\em unordered $n$-tuple of labels 1 thru $r$, where
repetition is allowed}.\\

\vskip .2 in

\noindent {\bf Multisets.}\\
A {\bf\em multiset}\label{d:multiset} is a set
that allows labels to appear more than once.
The {\bf\em multiplicity}\label{d:multiplicity} (or {\bf\em index}\label{d:index})
of a label in a multiset is the number of times the label appears in it.  We may use the notation $n_i$ to represent the multiplicity of label $i$ ($i=1,2,\dots,r$).
We allow $n_i=0$, which will mean the label $i$ does not appear in the multiset ($i$ wasn't selected).
The {\bf\em size} of a multiset is the total number of labels in it including multiplicity;
in the stars-and-bars experiment above we are drawing $n$ labels, so the resulting size of the multiset will be $n$.
In this way, two multisets are {\bf\em equal} if each label in the multisets have the same multiplicity, which will imply they are the same size.
For instance, among the following multisets:
$$\{1,1,1,2,2,4\},\quad \{1,2,4\},\quad \{1,1,1,2,2,3,4\},\ \mbox{and}\ \{4,2,2,1,1,1\}$$
only the first and the last ones are equal since they are the only ones that represent the same unordered $6$-tuple, and also because
$n_1=3$, $n_2=2$, $n_3=0$, and $n_4=1$ for each of them. Notice this also implies that they will have size
$n_1+n_2+n_3+n_4=6$.\\


\noindent Just to clarify: the stars-and-bars experiment above can be viewed as
producing sample points that are unordered $n$-tuples of $r$ distinct objects (repetition allowed).\\


\newpage

%\noindent {\bf Question.}\\
%\noindent Given $n$ and $r$ how many multisets are possible?\\
%
%\noindent {\bf Answer.}
%$$\dfrac {(n+r-1)!}{n!(r-1)!} = {n+r-1\choose n} = {n+r-1\choose r-1}.$$

\noindent There is an alternate way of viewing the stars-and-bars experiment that
will lead to an equivalent concept to a multiset.  This equivalent way of viewing the sample space will
provide us with a formula to count the total number of multisets for a given $n$ and $r$.\\


\noindent {\bf Alternate way to represent sample points in the stars-and-bars-experiment.}\\
\noindent Let $n$ and $r$ be positive integers.  Without loss of generality, let's assume the set of $r$ labels
we are selecting from is $\{1,2,\dots,r\}$.  When we perform the stars-and-bars experiment, we get a multiset of size $n$
of (possibly repeated) labels.  Instead of recording the multiset, we could, equivalently, record the ordered $r$-tuple of the multiplicities of each label,
i.e., record $(n_1,n_2,\dots, n_r)$.  For a fixed $n$ and $r$, there's a one-to-one correspondence between multisets
and these ordered $r$-tuples $(n_1,n_2,\dots, n_r)$ of nonnegative integers with $n_1+n_2+\cdots + n_r=n$.
The ordered $r$-tuple is an ordered {\bf\em integer partition of $n$}.\\


\noindent {\bf Illustration and important FACT.}\label{starsbarsillustration}\\
Take $n=3$ and $r=4$.  These numbers are relatively small so I can attempt to list all the resulting multisets, but I think you can see
how listing these will become tedious had $n$ and $r$ been larger.  To the right of each multiset I put the corresponding integer partition.
Ignore, for the moment, the stars-and-bars representations (I will discuss on page \pageref{starsbarsremark} but in the meantime if you see a pattern that's great!).
$$\begin{array}{ccccc}\underline{\mbox{\small multiset}} &  & \underline{\mbox{\small integer partition}} &  & \underline{\mbox{\small stars-and-bars}}\\
\{1,1,1\} &\longleftrightarrow & (3,0,0,0) &\longleftrightarrow &\star\star\star|||\\
\{2,2,2\} &\longleftrightarrow & (0,3,0,0) &\longleftrightarrow &|\star\star\star||\\
\{3,3,3\} &\longleftrightarrow & (0,0,3,0) &\longleftrightarrow &||\star\star\star|\\
\{4,4,4\} &\longleftrightarrow & (0,0,0,3) &\longleftrightarrow &|||\star\star\star\\
\{1,1,2\} &\longleftrightarrow & (2,1,0,0) &\longleftrightarrow &\star\star|\star||\\
\{1,1,3\} &\longleftrightarrow & (2,0,1,0) &\longleftrightarrow &\star\star||\star|\\
\{1,1,4\} &\longleftrightarrow & (2,0,0,1) &\longleftrightarrow &\star\star|||\star\\
\{2,2,3\} &\longleftrightarrow & (0,2,1,0) &\longleftrightarrow &|\star\star|\star|\\
\{2,2,4\} &\longleftrightarrow & (0,2,0,1) &\longleftrightarrow &|\star\star||\star\\
\{3,3,4\} &\longleftrightarrow & (0,0,2,1) &\longleftrightarrow &||\star\star|\star\\
\{1,2,2\} &\longleftrightarrow & (1,2,0,0) &\longleftrightarrow &\star|\star\star||\\
\{1,3,3\} &\longleftrightarrow & (1,0,2,0) &\longleftrightarrow &\star||\star\star|\\
\{1,4,4\} &\longleftrightarrow & (1,0,0,2) &\longleftrightarrow &\star|||\star\star\\
\{2,3,3\} &\longleftrightarrow & (0,1,2,0) &\longleftrightarrow &|\star|\star\star|\\
\{2,4,4\} &\longleftrightarrow & (0,1,0,2) &\longleftrightarrow &|\star||\star\star\\
\{3,4,4\} &\longleftrightarrow & (0,0,1,2) &\longleftrightarrow &||\star|\star\star\\
\{1,2,3\} &\longleftrightarrow & (1,1,1,0) &\longleftrightarrow &\star|\star|\star|\\
\{1,2,4\} &\longleftrightarrow & (1,1,0,1) &\longleftrightarrow &\star|\star||\star\\
\{1,3,4\} &\longleftrightarrow & (1,0,1,1) &\longleftrightarrow &\star||\star|\star\\
\{2,3,4\} &\longleftrightarrow & (0,1,1,1) &\longleftrightarrow &|\star|\star|\star\end{array}$$
Hopefully, this illustration convinces you of the following\\

\noindent {\bf FACT:} the total number of multisets of size $n$ from a set of $r$ labels
{\em is the same as} the total number of nonnegative integer $r$-tuple $(n_1, n_2,\dots, n_r)$ solutions to the equation
$$n_1+n_2+\cdots + n_r=n.$$
Counting one thing is the same as counting the other.



\newpage

\noindent The fact on the previous page is important because the ordered $r$-tuple interpretation motivates a very clever idea/insight
into counting the total number of multisets (and, thus, nonnegative integer $r$-tuple solutions to $n_1+n_2+\cdots + n_r=n$). \\

\noindent {\bf Stars-and-bars counting.}\\
Fix positive integers $n$ and $r$.  {\em How do we count the number of possible multisets?}\\
\noindent The idea behind this counting dates back to the time of Max Planck but popularized by
William Feller in his classic probability book, {\em Probability Theory and its Applications, Volume 1}.
From the fact, we can count the number of multisets by instead counting the number of ordered $r$-tuples of the form
$$(n_1,n_2,\dots,n_r),$$
where each entry is a nonnegative integer and all entries sum to $n$.
The multisets we are counting have $n$ labels.  Abstractly, represent each label by a {\em\bf star} $\star$, so that we have $n$ stars.
In the representation of the $r$-tuple there are $r-1$ commas that separate (partition) the nonnegative integers $n_1,n_2,\dots, n_r$ into $r$ pieces.
Let's represent a comma in the $r$-tuple by a {\bf\em bar} $|$, so that we have $r-1$ bars.
Interpret the ordered $r$-tuple above as having

$n_1$ stars to the left of the bar 1,

$n_2$ stars between bar 1 and bar 2,

$n_3$ stars between bar 2 and bar 3,

\qquad\quad \vdots

$n_r$ stars to the right of bar $r-1$.

\noindent We, therefore, have represented the ordered $r$-tuple as a ``word" involving stars and bars:

$$\underbrace{\star\star\cdots\star}_{n_1\mbox{\small \ stars}} | \underbrace{\star\star\cdots\star}_{n_2\mbox{\small \ stars}} | \underbrace{\star\star\cdots\star}_{n_3\mbox{\small \ stars}} |\cdots \cdots | \underbrace{\star\star\cdots\star}_{n_r\mbox{\small \ stars}}$$

\vskip .2 in

\noindent {\bf Important observation:}\\
\noindent Each anagram of this $(n+r-1)$-letter word corresponds to a unique ordered $r$-tuple (i.e., ordered integer partition of $n$) and, therefore, to a unique multiset.
Thus, the number of anagrams of this word {\em is} the total number of multisets of size $n$ from $r$ distinct objects.  The word has $n$ $\star$'s and
$r-1$ $|$'s, so the answer is given in the following result.\\

\noindent {\bf Counting multisets.}\\
The expression
$$\dfrac {(n+r-1)!}{n!(r-1)!} = {n+r-1\choose n} = {n+r-1\choose r-1}$$
counts

\noindent $\bullet$ the number of unordered $n$-tuples of $r$ distinct labels repetition allowed, \medskip

\noindent which is the same as \medskip

\noindent $\bullet$ the number of ordered $r$-tuples $(n_1,n_2,\dots, n_r)$ of nonnegative integer solutions to
$$n_1+n_2+\cdots+n_r=n.$$

%\noindent $\bullet$ the number of ways to distribute $n$ indistinguishable balls into $r$ distinct boxes.




\newpage




\noindent {\bf Remark.}\label{starsbarsremark}\\
In the illustration on page \pageref{starsbarsillustration} I show the correspondence between the stars-and-bars to the multisets and to the ordered $r$-tuples.
The $r-1=4-1=3$ bars left-to-right create 4 ordered buckets left-to-right. The bucket to the left of the first bar, the bucket between the first and second bar, the bucket
between the second and third bar, and finally, the bucket to the right of the third bar.
So, for instance, in the first one: $\star\star\star|||$ says
$$\underbrace{\star\star\star}_{3}|\underbrace{}_{0}|\underbrace{}_{0}|\underbrace{}_{0},$$
the representation $|\star||\star\star$ says
$$\underbrace{}_{0}|\underbrace{\star}_{1}|\underbrace{}_{0}|\underbrace{\star\star}_{2},$$
and, so on.\\

\vskip .8 in

\noindent We will now do several examples.\\


\noindent {\bf Example.}\\
How many ways can we distribute $8$ {\em identical} balls into $5$ {\em distinct} boxes?\\

\noindent SOLUTION:  \\
Viewing this as an integer partition problem: we can observe how many identical balls went into each box.
I.e., we are counting the number of nonnegative integer solutions $(n_1,n_2,n_3,n_4,n_5)$ to $n_1+n_2+n_3+n_4+n_5=8$.
Of course, $n_i$ is the number of identical balls in box $i$. The answer is
$$\dfrac {(8+5-1)!}{8!(5-1)!} = \dfrac {12!}{8!4!} = 495.$$

\noindent On the other hand, we could have viewed the counting above as a multiset problem as follows:
assign one of the labels 1 thru 5 to each of the 8 balls allowing labels to be repeated. The label on a ball
will tell us which box that ball is in.  But, we cannot distinguish balls -- they are identical.  So, if the 8 balls received
the labels like this:
$$1,4,4,4,5,3,2,2$$
and also like this:
$$4,4,4,2,2,1,3,5$$
then we cannot tell the difference because, again, balls are indistinguishable: we can't say, for instance, that in the first assignment
ball 1 got label 1, but in the second assignment, ball 1 got label 4. How would you know which one ball 1 is?? They aren't distinguishable.
So, we are really counting unordered 8-tuples of the numbers 1 thru 5 repetition allowed, i.e., from this point of view we are counting multisets.\\









\newpage





\noindent {\bf Example and discussion.}\\
\noindent Imagine we are forming bags of $10$ marbles.
The marbles come in $5$ different colors, but are otherwise identical.
We can fill our bag with as many or as few of each color we desire\footnote{ We assume that we have an unlimited supply of marbles of each color.}.
How many different bags of marbles can be formed?\medskip

\noindent SOLUTION: \\
How should we start?  If you're stuck, write down what a ``few" bags might look like.
Say, one bag is all $10$ color 1's.
Another bag is $9$ color 1's and $1$ color 2.
Yet another is $2$ of color $i$, $i=1,2,3,4,5$.  And, there are {\em many}  more!
Each bag will always have 10 marbles, but the numbers of each color
will vary from bag to bag.
How should we represent a sample point?
What model can we choose for a {\em bag} of such marbles?
After listing a few bags we get an idea for a model. Each bag can be identified by how many of each color went into it.
So, we can represent such a bag as an ordered 5-tuple $(n_1,n_2,n_3,n_4,n_5)$, where $n_i$ is the number of color $i$ marbles in the bag.
We need $n_1+n_2+n_3+n_4+n_5 = 10$.  The answer is
$$\dfrac {(10 + 5-1)!}{10!(5-1)!} = \dfrac {14!}{10!4!} = 1001\mbox{ bags.}$$

\bigskip

\noindent {\bf Example and discussion.}\\
Throw 5 {\em identical} 6-sided dice simultaneously.  How many outcomes are possible?\medskip

\noindent SOLUTION:\\
Of course, we know the answer if these dice were distinguishable: $6^5$
because, in this case, we would be counting {\em ordered} 5-tuples where each entry is any of the ranks 1 thru 6 repetition allowed;
maybe the distinguishable dice are different colors that we can plainly see, or we are thinking that the throws
of the identical dice are one-at-a-time and we observe the order of the ranks\dots all to justify the equally likely assumption.
But, the situation now is {\em not} this.
So, what do we observe?  What's a model for the sample points?\\

\noindent One point of view is this: since the dice are indistinguishable, then all we observe is the {\em unordered} 5-tuples of the numbers 1 thru 6 repetition allowed,
i.e., we observe multisets with $n=5$. Moreover, the entries of these multisets are ranks belonging to the set $\{1,2,3,4,5,6\}$, this implies $r=6$.
So, the number of multisets is
$$\dfrac {(5+6-1)!}{5!(6-1)!}=\dfrac {10!}{5!5!} = \displaystyle {10\choose 5} = 252,$$
far fewer than $6^5=7776$.\\

\noindent Alternatively, if we view the experiment as throwing indistinguishable dice all at once, then all we can observe is how many (identical) dice
are showing each rank. I.e., we observe $n_i$ dice showing rank $i$, $i=1,2,3,4,5,6$. And, $n_1+n_2+n_3+n_4+n_5+n_6=5$.
So, the number of possible ordered 6-tuple solutions,
$(n_1,n_2,n_3,n_4,n_5,n_6)$,
is, again,
$$\dfrac {(5+(6-1))!}{5!(6-1)!}.$$






\newpage




\noindent {\bf Thought exercise.}\\
\noindent In the last example, should we view these 252 sample points as equally likely?
To get a feel for an answer, re-do this example with only 2 indistinguishable dice instead of 5.
In this case, interpret what having these outcomes being equally likely means compared to experiment 2.2 on page \pageref{2dicesamplespace}.\\


\vskip .5 in

\noindent {\bf Remark.} \\
The last few examples show the power of star and bars counting when we can model the sample points as multisets of size $n$, i.e., as unordered $n$-tuples of
$r$ distinct labels repetition allowed, or as
ordered $r$-tuples of nonnegative integers that sum to a positive integer $n$. Indeed, the sample space of all such sample points
has cardinality $$\dfrac {(n+r-1)!}{n!(r-1)!}.$$

\vskip .5 in

\noindent Now for some slight generalizations\dots\\

\noindent {\bf Example.} \\
Throw 5 identical 6-sided dice.  How many outcomes will show {\em exactly} one 6?\\

\noindent SOLUTION:\\
Now, we are being told that we want the number of nonnegative integer solutions to
$$n_1+n_2+n_3+n_4+n_5+n_6=5\qquad \mbox{\bf\em and} \qquad n_6=1.$$
This means we are looking for the number of nonnegative integer solutions to
$$n_1+n_2+n_3+n_4+n_5=4.$$
Here, $n=4$ and $r=5$, so there are
$$\dfrac {(4+(5-1)!}{4!(5-1)!} = \dfrac {8!}{4!4!} = 70$$
throws of 5 identical dice that show exactly one 6.\\

\noindent Here's the multiset way of looking at the problem: we are trying to count all the multisets that have exactly one 6.
The structure of these multisets is rather simple; they look like
$$\{6\}\cup \{\mbox{a multiset of size 4 of labels 1,2,3,4,5}\}.$$
These multisets are, clearly, in one-to-one correspondence with multisets of size 4 drawn from 5 labels, and we'll get the same answer as above.
The intuition this point of view gives is this: put exactly one rank 6 into each multiset, and then count the number of multisets of one less size from
the remaining ranks.\\


\newpage


\noindent Let's generalize more\dots \\


\noindent {\bf Example.} \\
Throw 5 identical 6-sided dice.  How many outcomes will show a 6?\\

\noindent SOLUTION:\\
An outcome showing a 6 means that a 6 occurs on the throw of 5 dice, which implies that {\em at least} one 6 appears on the throw.
What does this constraint do to the counting we did in the last example?  Now, it is clear that we want to count the number of
nonnegative integer solutions to
$$n_1+n_2+n_3+n_4+n_5+n_6=5\qquad \mbox{\bf\em and}\qquad n_6\ge 1.$$
Now, $n_6\ge 1$ is the same as $\widetilde{n}_6:= n_6-1\ge 0$. Then the above becomes
$$n_1+n_2+n_3+n_4+n_5+n_6-1=5-1\qquad \mbox{\em and}\qquad \widetilde{n}_6:=n_6-1\ge 0,$$
or, equivalently, we are looking for the number of nonnegative integer solutions to
$$n_1+n_2+n_3+n_4+n_5+\widetilde{n}_6=4.$$
But this now fits the paradigm of the stars-and-bars counting with $n=4$, $r=6$:
$$\dfrac {(4+(6-1))!}{4!(6-1)!} = \dfrac {9!}{4!5!}=126$$
throws where a 6 occurs.\\

\vskip .5 in

\noindent {\bf Thought exercise.}\\
In this last example, what would be the reasoning using the multiset point of view?\\











\newpage




\begin{center}{\bf \Large EXERCISES.}\label{problemset4}\end{center}


%\vskip .5 in

\noindent {\bf 1.} In how many ways can you give 9 children 14 chocolate chip cookies?\\


\noindent {\bf 2.} Repeat Question 1 so that Rick, one of the kids, receives exactly one cookie.\\



\noindent {\bf 3.} Repeat Question 1 so that Rick receives at least two cookies.\\


\noindent {\bf 4.} Repeat Question 1 so that no child goes hungry (i.e., each receives at least one cookie).\\


\noindent {\bf 5.} Blaze Pizza offers 12 choices of toppings for their pizzas.
You can get none of a topping, one of a topping, double, triple, etc, but, for instance, double
sausage counts will count as two toppings. How many 8 topping pizzas are possible?\\


\noindent {\bf 6.} A {\em multiset} is a set that is allowed to have repeats. For example, $\{1,2,5,5,7,7\}$ is a multiset of size 6. How many multisets of size 6 are there that consist of the digits $0$ thru $9$?\\

\noindent {\bf 7.} A PE teacher puts 20 dodgeballs away into 6 bins. In how many ways can the PE teacher do this?\\


\noindent {\bf 8.} In {\em Yahtzee}, 5 $6-$sided dice are rolled. How many different outcomes are possible?\\

\noindent {\bf 9.} How many integers solutions are there to $x_1 + x_2 + x_3 + x_4 + x_5 = 60$, where $x_i \geq i$, $i = 1,2,\dots, 5$?\\



\noindent {\bf 10.} How many $8-$letter strings are able to be made with the 5 vowels in the alphabet if the order is irrelevant?\\

\noindent {\bf 11.} Four (4) letters will be chosen from 26 repetition allowed, and four (4) digits will be chosen from $0$ thru $9$ with repetition allowed.
Assume the order in which the letters and the digits are chosen is irrelevant.  We create a code by putting the 4 letters followed by the 4 digits.
How many such codes are possible?\\


\noindent {\bf 12.} (Bose-Einstein statistics)
Consider a system with $B$ identical bosons (subatomic particles) in a fixed volume and energy level.
Suppose the bosons can be in any of $g$ possible states associated with the fixed energy level.
In how many ways can these bosons be distributed over the states?
In how many ways can these bosons be distributed over the states where at least one state is missing?


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{center}{\bf Lecture }%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center} {\bf \Large II. Probability measures, Axioms and consequences.}\end{center}

\vskip 1 in


\noindent The purpose of this section is to introduce probability measures in
generality through axioms, which will provide us with properties that {\em all} probability
measures enjoy.  More importantly, these properties lead us to develop strategies to compute probabilities.\\

\newpage

\noindent {\bf Motivating the axioms of probability.}\label{axiommotivation}\\
\noindent When an experiment leads to a sample space $\Omega$ with finite and equally likely sample points a
natural probability measure to use is the classical probability:

For any event $A\subset \Omega$,
$$P(A) = \dfrac {|A|}{|\Omega|}.$$

\noindent The entire first section of these notes is devoted to methods for computing cardinalities in this model
and counting the number of sample points in general, exclusive of the equally likely assumption.
We will want to generalize the notion of probability to other models -- not just finite equally likely, but also to\medskip

\noindent $\bullet$ finite, non-equally likely sample spaces

\noindent $\bullet$ countable sample spaces like ${\mathbb N}:=\{0,1,2,3,\dots\}$ or ${\mathbb Z}:=\{\dots,-2,-1,0,1,2,\dots\}$, and

\noindent $\bullet$ uncountable sample spaces like the set of real numbers ${\mathbb R}$, ${\mathbb R}^2$ and ${\mathbb R}^n$.\\

\vskip .3 in

\noindent Suppose $\Omega$ is a finite sample space. The classical probability has the following 3 essential properties:\\

\noindent {\bf 1.} For any event $A\subseteq \Omega$, $0\le P(A)\le 1$.

This is because $0\le |A|\le |\Omega|$, so $\dfrac {0}{|\Omega|}\le \dfrac {|A|}{|\Omega|}\le \dfrac {|\Omega|}{|\Omega|}$.\\

\noindent {\bf 2.} $P(\Omega) = \dfrac {|\Omega|}{|\Omega|}=1$.\\

\noindent {\bf 3.} If $A_1,A_2,\dots,A_n$ are mutually exclusive ($A_i\cap A_j=\varnothing$ for all $i\ne j$), then

$|A_1\cup A_2\cup \cdots \cup A_n|= |A_1|+|A_2|+\cdots |A_n|$, and therefore,

$P(A_1\cup A_2\cup \cdots \cup A_n) = P(A_1)+P(A_2)+\cdots + P(A_n)$.\\

\noindent These 3 properties of the classical probability measure are properties we would want {\em all} probability measures to have.\\

\noindent The first one says that the probability is between 0 and 1 (inclusive). For the classical measure, $P(A)$ returns the proportion of all sample points that lie in $A$.
The closer this is to 1 the more likely the event $A$ will occur, the closer to 0, the less likely $A$ will occur. \\

\noindent As for the second property, since $\Omega$ has every possible sample point that can occur in the experiment, it is certain that $\Omega$ will occur when the experiment is performed.\\

\noindent The last property states that our probability measure should be {\em additive}:
when we union together the sample points from {\em mutually exclusive events}, the
chance the experiment produces a sample point from the union should be the sum of the
chances the sample point came from any of these events.  Strictly speaking, property 3
above says that $P$ is {\em finitely} additive since we are only considering finitely
many mutually exclusive events.  For technical reasons, we will want a similar property to hold for
infinite sequences of mutually exclusive events - to have {\em countable} additivity.\\




\newpage





%\noindent {\bf Quick reminder of basic set theory: terminology, notation.}\\
%
%\noindent Let $A$ and $B$ be any two events in a sample space $\Omega$.\\
%
%\noindent {\bf Intersection of two events.}\\
%\noindent $A\cap B := \{\omega\in \Omega:\omega\in A\ \mbox{and}\ \omega\in B\}$,
%i.e., the set of $\omega$ that belong to both $A$ and to $B$, the $\omega$'s in common with both $A$ and $B$.
%The event $A\cap B$ occurs means both events occur simultaneously.\\
%
%\noindent {\bf Mutually exclusive.}\\
%Two events $A$ and $B$ are {\bf \em mutually exclusive} if $A\cap B=\varnothing$,
%i.e., they have no sample points in common.  It is impossible for both events to occur simultaneously,
%so if one of them occurs, the other does not occur.
%More generally, a sequence of events $A_1,A_2,A_3,\dots$ is {\bf\em mutually exclusive}
%means every pair of them are mutually exclusive: for $i\ne j$, $A_i\cap A_j=\varnothing$.\\
%
%\noindent {\bf Union of two events.}\\
%\noindent $A\cup B:=\{\omega\in \Omega:\omega\in A\ \mbox{or}\ \omega\in\Omega\}$, i.e., the set of $\omega$ that belongs to $A$ or to $B$ (or to both $A$ and $B$).
%The event $A\cup B$ occurs means that {\em at least one of the events $A$ or $B$ occurs}.\\
%
%\noindent {\bf Complement of an event.}\\
%\noindent $A^c :=\{\omega\in\Omega:\omega\not \in A\}$, i.e., the set of $\omega$ that are not members of $A$.
%Clearly, $A\cap A^c = \{\omega\in \Omega: \omega\in A \  \mbox{and} \ \omega\not\in A\} = \varnothing$,
%it's impossible to find a sample point that simultaneously belongs to $A$ and doesn't belong to $A$;
%therefore, trivially, for any event $A$, $A$ and $A^c$ are mutually exclusive.\\


\newpage



\noindent Let $P$ be any probability law, not necessarily the classical one.  The following are properties that $P$ is postulated to have.\\

\noindent {\bf The 3 Axioms of Probability.}\label{axioms1} (A. Kolmogorov, 1933)\smallskip

\noindent {\bf 1.} {\bf \em nonnegativity}:

For any event $A$, $0\le P(A)\le 1$.\smallskip

\noindent {\bf 2.} {\bf \em normalization}:

$P(\Omega)=1$.\smallskip

\noindent {\bf 3.} {\bf \em countable additivity}:

For any sequence of mutually exclusive events $A_1,A_2,A_3,\dots$,
$$P(A_1\cup A_2\cup A_3\cup\cdots) = P(A_1)+P(A_2)+P(A_3) + \cdots.$$


\vskip .5 in

\noindent {\bf Remark.} ($P(\varnothing)=0$)\\
\noindent Let's show that these axioms imply $P(\varnothing)=0$ in general, not just for the classical probability measure. Intuitively,
this says the probability no outcome will happen when the experiment is performed is {\em zero},
and this makes sense because when the experiment is performed we know {\em some} sample point will occur.
Notice $\Omega = \Omega \cup \varnothing \cup \varnothing \cup \varnothing \cdots$ and
$\Omega, \varnothing ,\varnothing,\varnothing,\dots$ is a sequence of mutually exclusive events.
$P$ is countably additive, so
$$\underbrace{P(\Omega)}_{=1} = \underbrace{P(\Omega)}_{=1} + \underbrace{P(\varnothing)}_{=c} + \underbrace{P(\varnothing)}_{=c} + \underbrace{P(\varnothing)}_{=c} + \cdots.$$
By nonnegativity, $P(\varnothing)\ge 0$, and this equation balances exactly when $P(\varnothing)=c=0$. \\

\vskip  .5 in

\noindent {\bf Remark.} (countable additivity implies {\em finite} additivity)\\
Let $A_1,A_2,\dots,A_n$ be a {\em finite} collection of mutually exclusive events.  Then
$$A_1,A_2,\dots,A_n,\varnothing,\varnothing,\varnothing,\dots$$
is a {\em sequence} of mutually exclusive events and,
using a parallel argument, countable additivity implies
\begin{eqnarray*}
P(A_1\cup A_2\cup\cdots\cup A_n) &=& P(A_1\cup A_2 \cup\cdots \cup A_n\cup \varnothing\cup \varnothing\cup\cdots)\\
&=& P(A_1) + P(A_2) + \cdots + P(A_n)  + \underbrace{P(\varnothing)}_{=0} +\underbrace{P(\varnothing)}_{=0} + \cdots\\
&=& P(A_1) + P(A_2) + \cdots + P(A_n).\end{eqnarray*}
So, a countably additive probability measure is also finitely additive.
The converse is {\em not} true!  We will want our probability measures to be countably additive in this course
because it will be necessary to consider {\em countable} unions of mutually exclusive events.
In the scheme of what we will cover in this course, the distinction between these two types of additivity is {\em not} important, but becomes important
in more advanced treatments of probability.
Of course, in finite sample spaces the two concepts are identical. \\



\newpage

\noindent When our sample space $\Omega$ is finite and equally likely, then the Axioms tell us that the
classical probability measure {\em is} the resulting probability measure for this situation. Let's show this now:\\

\vskip .2 in


\noindent {\bf For $\Omega$ finite, equally likely, the axioms imply $P$ is the classical probability law.}\label{classicalprobmeasure2}\\
Let $|\Omega|<\infty$, say, $\Omega = \{\omega_1,\omega_2,\dots,\omega_N\},$
and suppose each $\omega\in \Omega$ is equally likely, i.e., there is a constant $c$ such that
$P(\{\omega\})=c$ for all $\omega\in \Omega$. Then,
$$\Omega = \bigcup_{i=1}^N \{\omega_i\} = \{\omega_1\}\cup \{\omega_2\}\cup \cdots \cup \{\omega_N\},$$
and because the $\omega_i$'s are distinct, the {\bf\em singleton sets} $\{\omega_1\}, \{\omega_2\},\dots, \{\omega_N\}$
are mutually exclusive. So, by normalization and {\em finite additivity},
$$1=P(\Omega) = \sum_{i=1}^N P(\{\omega_i\}) =  \sum_{i=1}^N c = cN \implies c = P(\{\omega\}) = \dfrac 1N\quad \mbox{for each }\omega\in\Omega.$$
Now, in a similar manner, we can write any event $A$ as a mutually exclusive union of its members: say,
$A = \{\omega_{i_1},\omega_{i_2},\dots,\omega_{i_k}\} = \{\omega_{i_1}\}\cup \{\omega_{i_2}\}\cup\cdots\cup \{\omega_{i_k}\}.$
It will then follow
$$P(A) = \underbrace{P(\{\omega_{i_1}\}) +  P(\{\omega_{i_2}\}) + \cdots + P(\{\omega_{i_k}\})}_{k\ \mbox{\small copies of }\frac 1N} = \dfrac kN = \dfrac {|A|}{|\Omega|}.$$
This is the classical probability law!$\hfill \Box$\\

\vskip .75 in

\noindent We now generalize the classical probability measure:\\

\noindent {\bf Discrete probability measures.}\label{discreteprobmeasure1}\\
Let $\Omega$ be finite or countably infinite sample space,
$\Omega = \{\omega_1,\omega_2,\omega_3,\dots\}.$
Suppose we have nonnegative numbers $p_1,p_2,p_3,\dots$ -- called {\bf\em probability masses} -- such that $\sum_i p_i =1$, and we assign
$$P(\{\omega_i\}) = p_i\quad\mbox{for each }i=1,2,3,\dots .$$
Then, given an event $A$, write it as
$A = \{\omega_{i_1}\}\cup \{\omega_{i_2}\}\cup\{\omega_{i_3}\}\cdots$ -- a mutually exclusive union of its members.
By countable additivity,
$$P(A) = P(\{\omega_{i_1}\}) + P(\{\omega_{i_2}\}) + P(\{\omega_{i_3}\}) + \cdots = \sum_{k:\omega_{i_k}\in A} p_{i_k},$$
i.e., to compute the probability of $A$ we just sum the probability masses at all those $\omega$ that belong to the event $A$.
This defines the {\bf\em discrete probability measure}.\\



\newpage


\noindent {\bf Basic example.}\\

\includegraphics*[-280,0][159,140]{discrete1probvenn1.jpg}


\vskip -2.15 in

\noindent Using the probability masses given in the figure to the right,

\noindent Compute

\noindent $P(A), \ P(B),\ P(A\cap B),$

\noindent $P(A\cup B),\ P(A^c\cap B),\ P(A\cap B^c).$\\

\noindent SOLUTION:\\
$P(A) = .05+.10+.20 = .35$.\\
$P(B) = .10+.10+.15+.20+.20=.75$.\\
$P(A\cap B) = .10+.20=.30$.\\
$P(A\cup B) = 1-(.05+.15)=.80$.\\
$P(A^c\cap B) = .10+.15+.20 = .45.$\\
$P(A\cap B^c) = .05$.\\

\vskip .3 in

\noindent {\bf Example.}\\
An experiment has a sample space $\Omega = \{\omega_1,\omega_2,\omega_3,\omega_4\}$.
Experience dictates that $\omega_2$ and $\omega_3$ are equally likely, but are each twice as likely as $\omega_1$; and, $\omega_4$ is 4 times as likely as $\omega_1$.
Construct the discrete probability masses associated with each sample point.\\

\noindent SOLUTION: \\
\noindent If we set $P(\{\omega_1\})=x$, then we are told $P(\{\omega_2\})=P(\{\omega_3\})=2x$ and $P(\{\omega_4\})=4x$. The normalization axiom says
$$P(\Omega) = x + 2x + 2x + 4x = 9x =1 \implies x = \dfrac 19.$$
Therefore,
$$P(\{\omega_1\})=\dfrac 19=:p_1,\ P(\{\omega_2\})=\dfrac 29=:p_2,\ P(\{\omega_3\})=\dfrac 29=:p_3,\ \mbox{and}\ P(\{\omega_4\})=\dfrac 49=:p_4.$$

\vskip .3 in


\noindent {\bf Example.} (the geometric$(\frac 12)$ distribution - modeling probability masses)\label{geomfaircoin}\\
Toss a fair coin until the occurrence of the first head. The sample space $\Omega = \{\omega_1,\omega_2,\omega_3,\omega_4\dots\}$
is countably infinite, and we can think of $\omega_i$ as the outcome that the first head occurs on the $i$th toss:

\qquad $\omega_1 = h$

\qquad $\omega_2 = th$

\qquad $\omega_3 = tth$

\qquad $\omega_4 = ttth$

\qquad \quad \, \vdots

\qquad $\omega_i = \underbrace{tt\cdots t}_{i-1\,\mbox{\small tails}}\!\!h$\\
\noindent If $p_i=P(\{\omega_i\}) = P(\{tt\cdots th\})$, then what would be a reasonable model for the $p_i$ ?\\
Use this model to compute the probability the first head occurs {\em before} the 4th toss. How about an {\em even} toss?


\newpage




\noindent The coin is fair, so I would expect $p_1=P(\{\omega_1\})=\dfrac 12$ -- half the time the first toss is a head, half the time a tail.\\

\noindent How about $P(\{\omega_2\})=P(\{th\})$ ?\\

\noindent If we think of tossing a fair coin repeatedly, then eventually we will surpass the second toss in the experiment. The event $\{th\}$
can be thought of as tossing a coin just twice and getting a tail followed by a head.  This is only one sample point of $2^2=4$ equally likely possibilities,
$hh,ht,th,tt$, that can result
when tossing the coin twice. So, I would expect $p_2=P(\{\omega_2\}) = \dfrac 1{2^2} = \Big(\dfrac 12\Big)^2$.\\

\noindent In fact, $p_i = P(\{tt\cdots th\}) = \Big(\dfrac 12\Big)^i$.\\

\noindent The event $A$ that the first head occurs before the 4th toss is $\{\omega_1,\omega_2,\omega_3\}$. So,
$$P(A) = P(\{\omega_1\})+P(\{\omega_2\})+P(\{\omega_3\}) = \frac 12 + \frac 14 + \frac 18 = \frac 78.$$
For the event $B$ that the first head occurs on an even toss,
i.e., $B = \{\omega_2,\omega_4,\omega_6,\omega_8,\dots\}$, and
\begin{eqnarray*}
P(B)
&=& P(\{\omega_2\}) + P(\{\omega_4\}) + P(\{\omega_6\}) + P(\{\omega_8\}) + \cdots \\
&=& \Big(\dfrac 12\Big)^2 + \Big(\dfrac 12\Big)^4 + \Big(\dfrac 12\Big)^6 + \Big(\dfrac 12\Big)^8 + \cdots \qquad(\mbox{geomtric ratio $=\frac 14$})\\
&=& \dfrac {\dfrac 14}{1 - \dfrac 14} \\
&=& \dfrac 13.\end{eqnarray*}

\bigskip


\noindent In the calculation of $P(B)$ above I used following:\\

\noindent {\bf Calculus fact: Sums of a geometric series.}\label{d:geomseries1}\\
For any constants $A$ and $r$, the sequence
$A, Ar, Ar^2, Ar^3,\dots$
is called a {\bf\em geometric progression}\label{d:geomprog1}. The value $r$ is called the {\bf\em geometric ratio}.\label{d:geomratio1}
%When $r\ne 1$ and $m\le n$ the {\bf\em sum of a geometric progression} is
%$$\sum_{k=m}^{n} Ar^k = Ar^{m} + Ar^{m+1} + Ar^{m+2} + \cdots + Ar^{n} = \frac {Ar^m - Ar^{n+1}}{1-r}.$$
When $|r|< 1$ the {\bf\em sum of the geometric series} is
$$\sum_{k=m}^{\infty}Ar^k = \frac {Ar^m}{1-r}.$$

\bigskip

\noindent {\bf Advice:}\\
{\bf\em Memorize} this formula for the sum of a geometric series.  It will be used several times in this course.
A good mnemonic for remembering the formula for a geometric series is ``it's the first term in the series divided by 1 minus the geometric ratio".\\



\newpage


\noindent {\bf Consequences of the Axioms: Properties of probability measures.}\label{pmproperties}\\

\noindent The axioms will imply some properties that all probability measure will possess.

\noindent I name a few:\medskip

\noindent $\bullet$ The Complementary Rule.\\
\noindent $\bullet$ Monotonicity.\\
\noindent $\bullet$ Subadditivity.\\
\noindent $\bullet$ Inclusion-exclusion rules.\\
and others.\\

\vskip .5 in



\noindent {\bf The Complementary Rule.}\label{complementaryrule}

For any event $A$,
$$P(A) = 1 - P(A^c).$$

\bigskip

\noindent Proof.

\noindent $A$ and $A^c$ are mutually exclusive and $A\cup A^c = \Omega$. So, by finite additivity,
$$1=P(\Omega) = P(A\cup A^c) = P(A) + P(A^c).$$
$\hfill \Box$\\

\vskip .2 in

\noindent {\bf Advice:}\\
Computing the probability of the complement event may be much easier than computing the probability of the event directly. You should keep this in mind!\\

\vskip .4 in


\noindent {\bf Monotonicity.}\label{monotonicity}

If $A$ and $B$ are events, and $A\subseteq B$, then
$$P(A)\le P(B).$$



\includegraphics*[-280,0][180,100]{AsubsetofB.jpg}

\vskip -1.1 in

\noindent Proof.

\noindent Notice $B=A\cup (A^c\cap B)$ and the events $A$ and $A^c\cap B$

\noindent are mutually exclusive. Therefore, by finite additivity

\noindent and nonnegativity,

$P(B) = P(A) + \underbrace{P(A^c\cap B)}_{\stackrel{\ge 0\ \mbox{\tiny by}}{\mbox{\tiny nonnegativity}}}\ge P(A) + 0=P(A).$

$\hfill \Box$


\bigskip

\noindent {\bf Remark.}\\
Basically monotonicity says if you add sample points to an event, the probability cannot become smaller.\\


\newpage

\noindent {\bf Subadditivity.}\label{subadd1} (also called Boole's inequality)\\
For {\em any} events $A_1,A_2,\dots,A_n$,
$$P(A_1\cup A_2\cup \cdots \cup A_n) \le P(A_1) + P(A_2) + \cdots + P(A_n),$$
or, succinctly,
$$P\left(\bigcup_{i=1}^n A_i\right) \le \sum_{i=1}^nP(A_i).$$
Of course, if the events were mutually exclusive, then we have equality, but in general, Boole's inequality provides a very crude upper bound for the probability of the union.\\

\noindent Proof.\\
I will proceed by induction.  When $n=1$ there's nothing to prove, we have a tautology. So, consider the case $n=2$. For any events $A_1$ and $A_2$,
$A_1\cup A_2 = A_1 \cup (A_1^c\cap A_2)$, where $A_1$ and $A_1^c\cap A_2$ are mutually exclusive (see the picture).

\includegraphics*[-150,0][160,80]{subaddpic1.jpg}
\begin{center}{\bf Figure.} $A_1$ (yellow) and $A_1^c\cap A_2$ (blue) are mutually exclusive.\end{center}

\noindent Therefore,
$P(A_1\cup A_2) = P(A_1)+P(A_1^c\cap A_2)$.  But, since $A_1^c\cap A_2\subseteq A_2$, by monotonicity, $P(A_1^c\cap A_2) \le P(A_2)$. Thus,
$$P(A_1\cup A_2) = P(A_1)+P(A_1^c\cap A_2) \le P(A_1) + P(A_2),$$
and the statement is true for $n=2$ events.

Suppose, for some {\em fixed} $k\ge 2$, the statement is true for any events $A_1,A_2,\dots, A_k$; namely
$$P(A_1\cup A_2\cup \cdots \cup A_k) \le \sum_{i=1}^k P(A_i).$$

Let $A_1,A_2,\dots, A_k,A_{k+1}$ be any $k+1$ events.
\begin{eqnarray*}
P\left( \bigcup_{i=1}^{k+1} A_i \right) & = & P\left(\left[A_1\cup A_2\cup \dots\cup A_k\right]\cup A_{k+1}\right)\\
& \le &
P\left(A_1\cup A_2\cup \dots\cup A_k\right) + P\left(A_{k+1} \right) \qquad (\mbox{$n=2$ case})\\
&\le & \sum_{i=1}^k P(A_i) + P(A_{k+1}) \qquad (\mbox{induction hypothesis})\\
& = & \sum_{i=1}^{k+1} P(A_i),\end{eqnarray*}
and this completes the induction.
$\hfill \Box$



\newpage


\noindent Two useful results when working with unions of events are
the inclusion-exclusion rules and DeMorgan's laws. We discuss each in turn.
The results that follow are true for any events $A_1,A_2,\dots,A_n$ unless noted otherwise.\\

\noindent {\bf The inclusion-exclusion rules.}\label{inclusionexclusionrules}\\
\noindent $\displaystyle P(A\cup B) = P(A) + P(B) - P(A\cap B).$

\bigskip

\noindent $\displaystyle P(A\cup B\cup C) = P(A) + P(B) + P(C)$

\hskip 1 in  $- P(A\cap B) - P(A\cap C) - P(B\cap C) + P(A\cap B\cap C).$\label{inclusionexclusion3}


\bigskip

\noindent $\displaystyle P(A\cup B\cup C\cup D) = P(A) + P(B) + P(C) + P(D)$

\hskip 1.3 in $ - P(A\cap B) - P(A\cap C) - P(A\cap D) - P(B\cap C) - P(B\cap D) - P(C\cap D)$

\hskip 1.3 in $+ P(A\cap B\cap C)+ P(A\cap B\cap D) + P(A\cap C\cap D) + P(B\cap C\cap D)$

\hskip 1.3 in $-P(A\cap B\cap C\cap D).$

\bigskip

\noindent In general,\\

\noindent $\displaystyle P\left(\bigcup_{i=1}^n A_i\right) = \underbrace{\sum_{i=1}^n P(A_i)}_{{n\choose 1}\ \mbox{\tiny terms}}\  - \ \underbrace{\!\!\!\!\!\!\!\!\!\!\!\!\!
\sum_{\qquad 1\le i_1<i_2\le n}\!\!\!\!\!\!\!\!\!\!\!\!\sum P(A_{i_1}\cap A_{i_2})}_{{n\choose 2}\ \mbox{\tiny terms}}\ + \
\underbrace{\sum\!\!\!\!\!\!\!\!\!\!\sum_{1\le i_1<i_2<i_3\le n}\!\!\!\!\!\!\!\!\!\!\sum P(A_{i_1}\cap A_{i_2}\cap A_{i_3})}_{{n\choose 3}\ \mbox{\tiny terms}} - +  \cdots $

\vskip .2 in

\noindent {\bf Remark.}\\
The inclusion exclusion rules are very useful to compute the probability of a union of events when probabilities of intersections of the events are easier to compute.
In fact, in many applications of this result it will turn out that the probabilities of the intersections of a fixed number of events will all be the same value, making it even
easier to use than may first appear.\\

\bigskip

\noindent {\bf Advice:}\\
The rules are not very hard to remember if you look at the patterns.  We {\em add} the probabilities of the all $n$ single sets, then {\em subtract} the probabilities of all the 2-way intersections, than {\em add} the probabilities of all the 3-way intersections, and so forth, alternating additions and subtractions until we get to the $n$-way intersection.\\


\vskip .1 in

\noindent {\bf The Boole-Bonferroni inequalities.}\label{boolebonferroni}\\

\noindent $\displaystyle P\left(\bigcup_{i=1}^n A_i\right) \le  \sum_{i=1}^n P(A_i)$\\

\bigskip

\noindent $\displaystyle P\left(\bigcup_{i=1}^n A_i\right) \ge  \sum_{i=1}^n P(A_i)\  - \ \!\!\!\!\!\!\!\!\!\!\!\!\!\sum_{\qquad 1\le i_1<i_2\le n}\!\!\!\!\!\!\!\!\!\!\!\!\sum P(A_{i_1}\cap A_{i_2})$\\

\bigskip

\noindent $\displaystyle P\left(\bigcup_{i=1}^n A_i\right) \le  \sum_{i=1}^n P(A_i)\  - \ \!\!\!\!\!\!\!\!\!\!\!\!\!\sum_{\qquad 1\le i_1<i_2\le n}\!\!\!\!\!\!\!\!\!\!\!\!\sum P(A_{i_1}\cap A_{i_2})\ + \
\sum\!\!\!\!\!\!\!\!\!\!\sum_{1\le i_1<i_2<i_3\le n}\!\!\!\!\!\!\!\!\!\!\sum P(A_{i_1}\cap A_{i_2}\cap A_{i_3})$\\

\qquad \vdots


\newpage



\noindent Let's prove the inclusion-exclusion rule for 2 sets, then show how the rule for more sets can follow inductively without giving a formal proof.\\

\noindent Proof of the inclusion exclusion rule for 2 sets:\\
For $n=2$ events, $A_1, A_2$, we can write $A_1\cup A_2 = A_1 \cup (A_1^c\cap A_2)$ where $A_1$ and $A_1^c\cap A_2$ are mutually exclusive. Therefore,
$P(A_1\cup A_2) = P(A_1) + P(A_1^c\cap A_2)$.


\includegraphics*[-140,0][160,80]{inclexcl2pic1.jpg}
\begin{center}{\bf Figure.} Decomposition of $A_1\cup A_2$ into mutually exclusive pieces.\end{center}

\noindent However, $A_2 = (A_1\cap A_2) \cup (A_1^c\cap A_2)$ and, again, $A_1\cap A_2$ and $A_1^c\cap A_2$ are mutually exclusive, so
$P(A_2) = P(A_1\cap A_2) + P(A_1^c\cap A_2)$ implying $P(A_1^c\cap A_2) = P(A_2) - P(A_1\cap A_2)$.  Substituting this into the equation above the picture
we get
$$P(A_1\cup A_2) = P(A_1) + \underbrace{P(A_1^c\cap A_2)}_{=P(A_2)-P(A_1\cap A_2)} = P(A_1)+P(A_2)-P(A_1\cap A_2),$$
which shows the statement is true in the case of $n=2$ events.$\hfill \Box$\\

\vskip .5 in

\noindent {\bf The inclusion-exclusion rule for 2 sets implies the rule for 3 sets:}\\
\noindent Now consider any 3 events $A_1, A_2$ and $A_3$.
\begin{eqnarray*}
P(A_1\cup A_2\cup A_3)
&=& P([A_1\cup A_2]\cup A_3)\\
&=& P(A_1\cup A_2) + P(A_3) - P([A_1\cup A_2]\cap A_3)\qquad (\mbox{by the rule for 2 sets})\\
&=& P(A_1)+P(A_2)-P(A_1\cap A_2) + P(A_3) - P([A_1\cap A_3]\cup [A_2\cap A_3])\\
&=& P(A_1)+P(A_2)-P(A_1\cap A_2) + P(A_3)  \\
& & \hskip .25in  - \Big(P(A_1\cap A_3) + P(A_2\cap A_3) - P(\underbrace{[A_1\cap A_3]\cap [A_2\cap A_3]}_{=A_1\cap A_2\cap A_3})\Big)\\
&=& P(A_1)+P(A_2)+P(A_3)\\
& & \hskip .25 in -P(A_1\cap A_2) - P(A_1\cap A_3) - P(A_2\cap A_3)\\
& & \hskip .5 in + P(A_1\cap A_2\cap A_3).\\
\end{eqnarray*}

\vskip .5 in

\noindent {\bf Exercise for the student.}\\
Now that we showed the inclusion-exclusion rule for 2 sets and for 3 sets, show how the
rule for 4 sets follows from these lower order rules.\\



\newpage





\newpage

\noindent {\bf DeMorgan's laws.}\label{demorgan}

For any events $A$ and $B$,

\begin{center}$\big(A\cup B\big)^c = A^c\cap B^c$\quad and \quad $\big(A\cap B\big)^c = A^c\cup B^c$ \end{center}

and, more generally, for {\em any} number of events
\begin{center}$\Big(\bigcup_i A_i\Big)^c = \bigcap A_i^c$\quad and \quad $\Big(\bigcap_i A_i\Big)^c = \bigcup_i A_i$ \end{center}



\vskip .2 in

\noindent Proof.\\
$\omega\in (A\cup B)^c$ iff $\omega\not\in A\cup B$ iff $\omega$ is {\em not} in at least one of $A$ or $B$ iff $\omega$ is {\em not} in $A$ and $\omega$ is {\em not} in $B$ iff $\omega\in A^c\cap B^c$. Therefore, $(A\cup B)^c = A^c\cap B^c$.\\

\vskip .5 in


\noindent {\bf Useful fact.}\\
\noindent By the complementary rule,
\begin{eqnarray*}
P\left(\bigcup_{i=1}^n A_i\right) &=& 1 - P\left(\Big( \bigcup_{i=1}^n A_i\Big)^c\right)\\
 &=& 1 - P\left(\bigcap_{i=1}^n A_i^c\right)\qquad (\mbox{by DeMorgan's law}), \\
\end{eqnarray*}
and, replacing $A_i$ with $A_i^c$ everywhere,
\begin{eqnarray*}
P\left(\bigcup_{i=1}^n A_i^c\right) &=& 1 - P\left(\bigcap_{i=1}^n A_i\right). \\
\end{eqnarray*}

\vskip .5 in

\noindent {\bf Remark.}\\
This fact is especially useful to compute the probability of a union of events when
computing the probability of the intersection of the {\em complements} of the events is easier; in particular,
such will
be the case when the events $A_1,A_2,\dots,A_n$ happen to be {\em independent}, which we will discuss shortly (see page \pageref{demorganindependence}).\\



\newpage




\noindent {\bf Example and discussion.}\label{5spinnersproblem}\\
\noindent We have 5 spinners labeled 1 thru 5 each having 3 equally likely regions numbered 1,2,3 as in this picture:

\includegraphics*[-80,0][300,60]{5spinners3regions1.jpg}

\medskip

\noindent Here $\Omega = \{(x_1,x_2,x_3,x_4,x_5): x_i\in \{1,2,3\},\ i=1,2,3,4,5\}$ is finite and equally likely.


\noindent We spin each spinner.  What's the probability that we are {\em void} in at least one number?\\



\noindent SOLUTION:

\noindent Being void in a number means that a value (1, 2, or 3) is missing in the outcome. Here are some sample points (ordered 5-tuples):

$$\underbrace{(1,3,3,1,1)}_{\mbox{\small void in 2's}}\quad,\quad \underbrace{(2,2,3,2,2)}_{\mbox{\small void in 1's}}\quad,\quad \underbrace{(3,3,3,3,3)}_{\mbox{\small void in 1 and 2's}}\quad \mbox{and}\quad \underbrace{(1,2,1,2,2)}_{\mbox{\small void in 3's}}.$$

\noindent If we define $V_i$ to be the event that we are void in the number $i$, then we
are interested in computing the probability of $V_1\cup V_2\cup V_3$ -- this is the event that
{\em at least one} of the events $V_1,V_2$ or $V_3$ happens which is exactly what we want!
I claim
this is a situation where computing the probabilities of the intersections of these events is easier (than computing the intersection of their complements). Therefore, I will
employ the inclusion-exclusion rule for 3 sets here (see the second formula at the top of page \pageref{inclusionexclusion3}):
\begin{eqnarray*}
P(V_1\cup V_2\cup V_3)
& = & P(V_1) + P(V_2) + P(V_3) \\
& &  \quad - P(V_1\cap V_2) - P(V_1\cap V_3) - P(V_2\cap V_3) \\
& & \quad \quad + P(V_1\cap V_2\cap V_3).\end{eqnarray*}



\noindent The event $V_1$ has all the ordered 5-tuples
that are missing the number 1, i.e., can only have entries from the set $\{2,3\}$.
This is just sampling with replacement 5 times from a set of size 2:
$|V_1|=2^5$.  Likewise, $|V_2|=|V_3|=2^5$.  The event $V_1\cap V_2$ are the ordered 5-tuples that are missing {\em both} the numbers 1 and 2, i.e., can only have the one entry $\{3\}$.
So, $V_1\cap V_2 = \{(3,3,3,3,3)\}$ and $|V_1\cap V_2|=1$. Similarly, $|V_1\cap V_3|=|V_2\cap V_3|=1$.  Lastly, $V_1\cap V_2\cap V_3=\varnothing$, so $|V_1\cap V_2\cap V_3|=0$.\\

\noindent Putting these calculations into the inclusion-exclusion rule above and noting that $|\Omega|=3^5$ we obtain
$$P(V_1\cup V_2\cup V_3) = 3\cdot \dfrac {2^5}{3^5} - 3\cdot \dfrac {1}{3^5} + \dfrac 0{3^5} = \dfrac {31}{81}.$$

\vskip .5 in

\noindent {\bf Further discussion on the spinners example.}\\
\noindent What if we had attempted to use the formula $P(V_1\cup V_2\cup V_3) = 1 - P(V_1^c\cap V_2^c \cap V_3^c)$ ?\\
$V_1^c\cap V_2^c\cap V_3^c$ is the event that we are not void in any of the numbers 1,2 or 3; i.e., there is at least one of each number. There are two cases to consider:

case 1: 3 of one number, one each of other two.

case 2: 2 each of two numbers, one of the remaining.

\newpage

\noindent In case 1: there are ${3\choose 1}$ ways to select the number that will be tripled, and once this is done, there are ${5\choose 3}$ of the spinners that this number can appear on, once this is done, there are $2!$ ways the remaining two numbers can appear on the remaining two spinners. The probability is $\dfrac {{3\choose 1}{5\choose 3}\cdot 2!}{3^5}$.\\

\noindent In case 2: there are ${3\choose 2}$ ways to select the 2 numbers for the two numbers since the doubles are indistinguishable, once this is done there are ${5\choose 2,2,1}$ ways to position to numbers in the 5-tuple. The probability is $\dfrac {{3\choose 2}{5\choose 2,2,1}}{3^5}$.\\

\noindent The result is
$$P(V_1^c\cap V_2^c\cap V_3^c) = \dfrac {3\cdot {5\choose 3}\cdot 2! + 3{5\choose 2,2,1}}{3^5}=\dfrac {150}{3^5}=\dfrac {50}{81}.$$
Consequently, $$P(V_1\cup V_2\cup V_3) = 1 - \dfrac {50}{81} = \dfrac {31}{81}.$$

\noindent You might think that, comparatively, these two computations were about the same degree of difficulty. But, what if there had been 20 spinners instead of 5 and even more equally likely regions than 3?  The calculation using the inclusion exclusion rule is no more difficult than the one with 5 spinners; however, the calculation involving the complement event is nightmarish!  Of course, if we are very careful one could use multiset counting to enumerate the cases, but you can see the immediate challenges with this approach.



\newpage


\noindent {\bf Example.} (``Men with hats" problem)\\
Suppose $n$ men wearing hats put their hats in a room. When they leave each man grabs a hat uniformly at random the hats remaining.
What's the probability that no man selects his own hat?\\

\noindent For example, when $n=3$,
$\Omega = \left\{(1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2), (3,2,1) \right\},$
where, for instance,  $(2,1,3)$ means person 1 selects person 2's hat, person 2 selects person 1's hat, person 3 selects person 3's hat.
Out of the $|\Omega|=3!$ possibilities, only 2, namely $(2,3,1)$ and $(3,1,2)$,
have no man selecting their own hat -- so, the probability is $\frac 2{3!}=\frac 13$.\\

\noindent {\em What do we do with the case of general $n$?}\\

\noindent Let $A$ be the event that no man selects their own hat. Then $A^c$ is the event that at least one man selects their own hat; i.e., $A^c = \bigcup_{i=1}^nM_i$, where
$M_i$ is the event that the $i$th man selects his own hat.  The sample space $\Omega$ of this experiment is the set of all $n!$ orderings of the integers 1 thru $n$, and it is important to recall the {\em exchangeability property} this sample space exhibits.\\

\noindent $P(M_i) = \dfrac 1n$ for $1\le i\le n$.\\
\noindent $P(M_{i_1}\cap M_{i_2}) = \dfrac 1{(n)_2}$ for $1\le i_1<i_2\le n$.\\
\noindent $P(M_{i_1}\cap M_{i_2}\cap M_{i_3}) = \dfrac 1{(n)_3}$ for $1\le i_1<i_2<i_3\le n$.\\
\noindent In general, $P(M_{i_1}\cap M_{i_2}\cap \cdots \cap M_{i_k}) = \dfrac 1{(n)_k}$ for $1\le i_1<i_2<\cdots <i_k\le n$.\\

\noindent By the inclusion-exclusion rule for $n$ events,
\begin{eqnarray*}
P\left(\bigcup_{i=1}^nM_i\right)
&=& \sum P(M_i) - \sum\sum P(M_{i_1}\cap M_{i_2}) + \sum\sum\sum P(M_{i_1}\cap M_{i_2}\cap M_{i_3}) - + \cdots\\
&=& \sum \dfrac 1{n} - \sum\sum \dfrac 1{(n)_2} + \sum\sum\sum \dfrac 1{(n)_3} - \sum\sum\sum\sum \dfrac 1{(n)_4 } + - \cdots\\
&=& n\cdot \dfrac 1n - {n\choose 2}\dfrac 1{(n)_2} + {n\choose 3}\dfrac 1{(n)_3} - {n\choose 4}\dfrac 1{(n)_4} + - \cdots + (-1)^{n+1}\dfrac 1{n!} \\
&=& 1 - \dfrac 1{2!} + \dfrac 1{3!} - \dfrac 1{4!} + \dfrac 1{5!} - + \cdots + (-1)^{n+1}\dfrac 1{n!} = P(A^c).\\
\end{eqnarray*}
$$P(A) = 1-P(A^c) = 1 - \dfrac 1{1!} + \dfrac 1{2!} - \dfrac 1{3!} + \dfrac 1{4!} - \dfrac 1{5!} - + \cdots + \dfrac {(-1)^n}{n!} \approx e^{-1} =0.367879\dots,$$
where we recognize the MacLaurin series expansion of the exponential function:\\

\noindent {\bf Calculus fact: MacLaurin series for $e^u$. }\label{d:maclaurinseries1}\\
For any (real) $u$,
$$e^u = \sum_{k=0}^{\infty} \frac {u^k}{k!} = 1 + u + \frac {u^2}{2!} + \frac {u^3}{3!} + \frac {u^4}{4!} + \cdots.$$

\newpage

\noindent {\bf Continuity of probability measures.}\label{s:continuityprobmeas}\\

\noindent Let  $E_1,E_2,E_3,\dots$ be a {\bf \em nested increasing sequence of events}\label{nestedincreasing}: this means, for every $n$, $E_n\subseteq E_{n+1}$,
$$E_1\subseteq E_2\subseteq E_3\subseteq \cdots \subseteq E_n\subseteq E_{n+1} \subseteq \cdots.$$
Each event in this sequence is contained in the next, therefore, for every $n$,
$$\bigcup_{i=1}^nE_i = E_n.$$
So, if we set $E = \bigcup_{i=1}^{\infty}E_i$, then $\bigcup_{i=1}^nE_i\to \bigcup_{i=1}^{\infty}E_i=:E$ as $n\to \infty$, which we will abbreviate as $E_n\uparrow E$ as $n\to \infty$.  The figure below show a nested increasing sequence of events $E_1\subseteq E_2\subseteq \cdots$ and its decomposition into mutually exclusive events
$E_1, E_2-E_1, E_3-E_2,\dots$.\\

\noindent Here's a similar situation with a {\bf \em nested decreasing sequence of events}\label{nesteddecreasing}:
$$F_1\supseteq F_2\supseteq F_3\supseteq \cdots\supseteq F_n\supseteq F_{n+1}\supseteq \cdots.$$
Each event in this sequence contains the next, therefore, for every $n$,
$$\bigcap_{i=1}^nF_i = F_n.$$
So, if we set $F = \bigcap_{i=1}^{\infty}F_i$, then $\bigcap_{i=1}^nF_i\to \bigcap_{i=1}^{\infty}F_i=:F$ as $n\to\infty$, which we will abbreviate as $F_n\downarrow F$ as $n\to\infty$.

\includegraphics*[0,0][420,150]{friedeggdecomp1.jpg}
\begin{center}{\bf Figure.} On left, nested events $E_1\subseteq E_2\subseteq\cdots$; on right, decomposition into mutually exclusive pieces.
\end{center}

\vskip .3 in

\noindent Here's the result I'd like to show:\\

\noindent {\bf Continuity of probability.}\label{continuityofprobability}\\
\noindent If $E_n\uparrow E$ as $n\to \infty$, then $P(E_n)\to P(E)$ as $n\to\infty$.\\
\noindent If $F_n\downarrow F$ as $n\to\infty$, then $P(F_n)\to P(F)$ as $n\to \infty$.\\


\newpage

\noindent Proof.\\
\noindent Suppose $E_n\uparrow E$. Then, we can write as a union of mutually exclusive events:
$$E=E_1\cup \bigcup_{i=1}^{\infty} \Big( E_{i+1}-E_i\Big).$$
Therefore,
\begin{eqnarray*}
P(E)
&=& P(E_1) + \sum_{i=1}^{\infty}P(E_{i+1}-E_i)\quad (\mbox{by countable additivity})\\
&=& P(E_1) + \sum_{i=1}^{\infty}\Big(P(E_{i+1})-P(E_i)\Big)\quad (E_i\subseteq E_{i+1})\\
&=& \lim_{n\to\infty}\left(P(E_1) + \sum_{i=1}^{n}\Big(P(E_{i+1})-P(E_i)\Big)\right)\quad(\mbox{series is a limit of its partial sums})\\
&=& \lim_{n\to\infty}P(E_n).\qquad (\mbox{the partial sums are telescoping})\\
\end{eqnarray*}
Now, suppose $F_n\downarrow F$ as $n\to\infty$.  Then $F_n^c\uparrow F^c$. By what we just proved:
$$\lim_{n\to\infty}P(F_n^c)=P(F^c)=1-P(F).$$
But, $\lim_{n\to\infty}P(F_n^c)=\lim_{n\to\infty}\Big(1-P(F_n)\Big) = 1-\lim_{n\to\infty}P(F_n)$. Putting this together with the above
it follows that
$$\lim_{n\to\infty}P(F_n)=P(F),$$
which completes the proof.$\hfill \Box$







\newpage



\noindent {\bf Conditional probability.}\label{conditionalprob1}\\

\vskip .3 in



\noindent Sometimes partial information about the outcome of an experiment
comes available, and this extra information may change the probability of events.
For example,
two fair 6-sided dice are rolled,
the probability the sum is 10 is $\frac 3{36}$ because out of the 36 equally likely sample points, 3 of them sum to 10.  But, suppose when the dice are thrown someone saw the number 1 was on at least one of the dice.  Given this information, we see it is now impossible for the sum to be 10, so indeed, this partial information changed the probability the sum is 10.
We will say the conditional probability the sum is 10 given a 1 appeared is {\em zero}.\\

\noindent Carrying this one step further, the probability that the sum is 7 is $\frac 16$ without any additional information. Now consider the question:

\begin{center}{\em If a 1 appears, what's the probability the sum is 7 ?}\end{center}

\noindent First of all, this question only cares about the probability that the sum is 7 under the specific condition, and thus, we are trying to compute a conditional probability.
The {\em given information} in the condition is the {\em event} `a 1 appears'.  So, we need to compute the probability that the sum of the dice will be 7 given that the
event `a 1 appears' has occurred.  \\


\noindent Let's look at the sample space ($xy$ means $x$ rolled first, $y$ rolled second):

$$\begin{array}{rrrrrl}\Omega=\{11,&12,&13,&14,&15,&16,\\
21,&22,&23,&24,&25,&26,\\
31,&32,&33,&34,&35,&36,\\
41,&42,&43,&44,&45,&46,\\
51,&52,&53,&54,&55,&56,\\
61,&62,&63,&64,&65,&66\}\end{array}$$

\noindent Under the given condition `a 1 appears', the sample space $\Omega$ as written has too many sample points -- we can ignore, for instance, 66 since a 1 doesn't appear in it.
The given information {\bf\em reduces} the sample space to just those sample points in the given event.  Here's the reduced sample space:

\includegraphics*[-122,0][180,100]{reducedsamplespace2dice.jpg}

\noindent I'll emphasize that we cannot tell if the die we saw with the number 1 on it was rolled first or was rolled second, which hopefully explains why we needed to include all 11 sample points that have at least one 1 in them. The original sample space was equally likely, so these 11 sample points are also equally likely, but only 2 of these sum to 7, namely, 61 and 16; so the conditional probability the sum is 7 given a 1 appears is $\frac 2{11}$.\\




\newpage

\noindent To follow-up a bit\dots, suppose the two dice were different colors -- say, one is red, the other is green -- and, we are interested in
computing the (conditional) probability that the sum is 7 given the red die shows a 1.  Now, the original sample space of 36 equally likely sample points is
reduced to only these 6 sample points (assuming $xy$ means $x$ on the red, $y$ on the green die):


\includegraphics*[-122,0][180,100]{reducedsamplespace2dice-pic2.jpg}

\vskip - 1in

\noindent Among these 6 equally likely sample points only 1 sums to 7, so the conditional probability is $\frac 16$, i.e., the information that the
red die shows a 1 doesn't alter the unconditional probability the sum is 7. \\

\vskip .5 in

\noindent {\bf Notation:}\\
We write the (conditional) probability of $A$ given $B$ as $P(A|B)$. It represents the probability of $A$ under the condition the event $B$ occurs.\\

\vskip .5 in


\noindent {\bf The conditional probability formula.}\label{conditionalprobabilityformula}\\
For any event $A$, when $P(B) >0$, we can compute
$$P(A|B) = \dfrac {P(A\cap B)}{P(B)}.$$


\vskip .5 in

\noindent {\bf Advice:}\\
Memorize this formula.  Especially remember that $P(B)$ needs to be positive to use it.\\

\vskip .5 in

\noindent The motivation behind this formula is that if we are thinking that
$B$ is the new sample space, then the sample points within it should occur in the same relative proportions.
Thus, we should normalize the probabilities of each sample point within $B$ by $P(B)$ (of course, we'd need $P(B)>0$ to divide by it!).
After doing this all the probabilities in $B$
will sum to 1 making it a valid sample space.  If we're given $B$ occurs, then only the portion of $A$ that belongs to $B$ will matter (i.e., $A\cap B$);
the probability of $A$ in this new sample space is really the ratio of the unconditional probabilities given in the formula.  I'll illustrate what's going on in the next example.\\


\newpage



\noindent {\bf Example.}\\

\includegraphics*[-280,0][159,130]{discrete1probvenn1.jpg}


\vskip -2 in

\noindent With the discrete probability space pictured to the right,

\noindent compute $P(A|B)$.\\

\noindent SOLUTION:\\
The event $A = \{\omega_2,\omega_4,\omega_8\}$, and
$B = \{ \omega_3,\omega_4,\omega_6,\omega_7,\omega_8\}$.


\noindent Using the conditional probability formula:

$P(A\cap B) = P(\{\omega_4\})+P(\{\omega_8\})=.10+.20=.30$.

\noindent Similarly, $P(B) =.75$. Therefore,

$P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac {.30}{.75} = \dfrac 6{15}$.\\

\vskip .5 in

\includegraphics*[-280,0][159,130]{discrete1probvenn-pic3.jpg}

\vskip -1.5 in



\noindent Alternatively, knowing $B$ is the ``new" sample space,

\noindent the sample points within $B$ occur in the same relative

\noindent proportions. Therefore, the probabilities of each

\noindent sample point in $B$ should be normalized by $P(B)=.75$.

\noindent (see picture to right: $B$ is now a sample space!)

\noindent only the sample points in $A\cap B$ are now possible, as it is

\noindent impossible for $\omega_2$ to occur given $B$. The remaining

\noindent sample points in $A\cap B$ should occur in the same

\noindent proportions within $B$.  Since $A\cap B = \{\omega_4,\omega_8\}$,
$P(A|B) = \frac {.10}{.75}+\frac {.20}{.75} = \frac {.30}{.75}$ as before.



\vskip 1 in

\noindent {\bf Exercise for students.}\\
Fix an event $B$ in a sample space $\Omega$ with $P(B)>0$.  We {\em define} the set function
$$P_B(A) := \dfrac {P(A\cap B)}{P(B)}\quad\mbox{for each event }A\subseteq \Omega.$$
Show that
$P_B$ satisfies the 3 axioms of a probability and {\em is}, therefore, a probability measure.
Moral of the story: For {\em fixed} $B$ with $P(B)>0$, the conditional probability $P(\cdot|B)$ is a probability measure (just with $B$ as its sample space).\\



\newpage





\noindent {\bf Example.} (continued from the geometric($\frac 12$) example from page \pageref{geomfaircoin})\\
\noindent You toss the fair coin repeatedly. If the first head occurs on an even-numbered toss, what's the probability it was the second toss?
Also answer: If the first head occurs on an odd-numbered toss, what's the probability it was the first toss?\\

\noindent SOLUTION:\\
Define $B$ to be the event the first head occurs on an even-numbered toss, and let
$A_i$ be the event that the first head occurs on the $i$th toss.  We are interested in
$P(A_2|B)$.  On page \pageref{geomfaircoin}, we learned that $P(B) = \frac 13 > 0$.
Now, if we compute $P(A_2\cap B)$ then we can apply the conditional probability formula.
But, in this problem, $A_2\subseteq B$ since the second toss is also an even-numbered toss; therefore, $A_2\cap B=A_2$.
$P(A_2\cap B) = P(A_2) = \big(\frac 12\big)^2 = \frac 14$. Finally,
$$P(A_2|B) = \dfrac {\frac 14}{\frac 13}=\dfrac 34.$$

\noindent Since the first head occurs on either an even-numbered toss or an odd-numbered toss and not both, $B^c$ is
the event the first head occurs on an odd-numbered toss. By the complementary rule, $P(B^c) = 1-P(B)=\frac 23$.
We are now interested in $P(A_1|B^c)$.  Again, $A_1\subseteq B^c$, so $P(A_1\cap B^c)=P(A_1) = \frac 12$.  Therefore,
the probability the first toss is a head {\em given} the first head occurs on an odd-numbered toss is
$$P(A_1|B^c) = \dfrac {\frac 12}{\frac 23}=\dfrac 34.$$

\vskip .5 in

\noindent  {\bf Example.}\\
\noindent I deal you a 5-card hand from a standard deck of 52 cards. What's the probability they are all hearts given they are all red?\\

\noindent SOLUTION:\\
Let $R$ be the event all cards are red, and $H$ the event that all cards are hearts.
We are interested in $P(H|R)$.
Sometimes, when working with sample spaces that were originally equally likely
(equally likely before the given information that all cards are red),
it's often better to work with the reduced sample space, especially when the event
of interest is a subset of the given event (as it is here: all hearts is a subset of all red):
$$P(H|R) = \dfrac {P(H\cap R)}{P(R)}= \dfrac {P(H)}{P(R)}=\dfrac {\frac {|H|}{|\Omega|}}{\frac {|R|}{|\Omega|}}=\dfrac {|H|}{|R|}.$$
In this case, $|R|={26\choose 5}$ and $|H|={13\choose 5}$. Therefore,
$$P(H|R) = \dfrac {{15\choose 5}}{{26\choose 5}}\approx 0.02.$$
By the way, the unconditional probability $P(H) = \dfrac {|H|}{|\Omega|} = \dfrac {{13\choose 5}}{{52\choose 5}}\approx 0.0005$, quite a difference!\\


\newpage

\noindent {\bf Example.} (continued from the 5 spinners problem on page \pageref{5spinnersproblem})\\
\noindent You spin the 5 spinners. Compute the probability that the number 3 is missing given you are void in a number.\\

\noindent SOLUTION:\\
\noindent Here's a problem where intuition can lead you astray if you are not careful.
You {\em might} think the answer should be $\frac 13$ because we are equally likely to
be missing any of the three numbers if we are told we are void, and, this would be true
if the only way we can be void in a number is to be void in exactly one number. However,
we can be void in 3's simultaneously with being void in 1's or 2's.\\

\noindent This problem is asking us to compute $P(V_3|V_1\cup V_2\cup V_3)$.
$$P(V_3|V_1\cup V_2\cup V_3) = \dfrac {P(V_3\cap (V_1\cup V_2\cup V_3))}{P(V_1\cup V_2\cup V_3)}= \dfrac {P(V_3)}{P(V_1\cup V_2\cup V_3)}= \dfrac {\frac {2^5}{3^5}}{\frac {31}{81}} = \dfrac {32}{93}\approx 0.344.$$

\vskip .5 in

\noindent {\bf Example.}\\
We have 30 balls: 16 are red, 8 are green, and 6 are yellow. We line up the balls left to right. Given all the green balls are consecutive, what's the probability there
are no two adjacent yellows?\\

\noindent SOLUTION:\\
Let $G$ be the event that all green balls are consecutive, and let $Y$ be the event that there are no adjacent yellows.
I'll work in the equally likely sample space of all anagrams of the 30-letter word with 16 $R$'s, 8 $G$'s and 6 $Y$'s: $|\Omega|=\frac {30!}{16!8!6!}$.
$G$ is the event that the anagram has all $G$'s in a row. So, treat the 8 $G$'s as a single super-letter $\tilde{G}$. The resulting word will have 23 letters now: 1 super-$G$, 16 $R$'s and 6 $Y$'s.  The number of anagrams is, therefore, $\frac {23!}{1!16!6!}$
$$P(G) = \dfrac {\frac {23!}{1!16!6!}}{\frac {30!}{16!8!6!}}.$$
Now let's look at the event $Y\cap G$: keep the $Y$'s separated {\em and} the $G$'s together.\\
$$\_\,\tilde{G}\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_\,R\_$$
To keep the $Y$'s separated in this word, we need to select 6 of the $17+1=18$ blanks to put the identical $Y$'s into; and, for each such choice, there are
$\frac {17!}{1!16!}=17$ anagrams that keep the greens together. Therefore,
$$P(Y\cap G) = \dfrac {{18\choose 6}17}{\frac {30!}{16!8!6!}}.$$
Therefore, by the conditional probability formula
$$P(Y|G) = \dfrac {P(Y\cap G)}{P(G)} = \dfrac {\frac {{18\choose 6}17}{\frac {30!}{16!8!6!}}}{\frac {\frac {23!}{1!16!6!}}{\frac {30!}{16!8!6!}}}
=\dfrac {{18\choose 6} 17}{\frac {23!}{16!6!}}\approx 0.18.$$



\newpage




\noindent {\bf Remark.}\\
\noindent Sometimes conditional probabilities are given or are easier to
compute/understand in a given situation than some related unconditional probabilities.
By rewriting the conditional probability formula as
$$P(A\cap B) = P(A|B)P(B)\qquad (\mbox{assuming }P(B)>0)$$
or as
$$P(A\cap B) = P(B|A)P(A)\qquad (\mbox{assuming }P(A)>0)$$
can be especially useful. These formulas are sometimes called the {\bf\em multiplicative rule of conditional probability}\label{multrulecondprob}.


\vskip .5 in


\noindent {\bf Basic example.}\\
\noindent Among left handed people, 50\% exhibit a counter-clockwise cowlick (spiral behavior of hair on head).
10\% of people are left handed. What proportion
of people are both lefthanded and exhibit a counter-clockwise cowlick?\\

\noindent SOLUTION:\\
Let $L$ be the set of lefthanded people and $C$ the event of exhibiting a counter-clockwise cowlick.
We are told $P(C|L)=.50$ and $P(L)=.10$.  We want $P(C\cap L)$.  By the multiplicative rule of conditional probability
$P(C\cap L) = P(C|L)P(L) = 0.50\cdot 0.10 = 0.05$. So, 5\% of people are lefthanded and have counter-clockwise cowlicks.\\

\noindent It should be clear that $P(L|C)$ is something entirely different here. This is the proportion of people with counter-clockwise
cowlicks that are lefthanded.\\




\vskip .4 in


\noindent {\bf Example.}\\
\noindent In a typical year 40\% of Probability students are underclassmen (Freshmen and Sophomore).  Among underclassmen, 50\% get an A, while
among non-underclassmen 30\% get an A. Let $U$ be the set of underclassmen, and let $A$ be the set of students who receive an A in Probability.\\
(a) Find the proportion of students who receive an A in Probability.\\
(b) What proportion of students who receive an A in Probability are underclassmen?\\

\noindent SOLUTION: \vskip -.3 in

\includegraphics*[-320,0][170,90]{condprobvenn1.jpg}


\vskip -1 in

\noindent Sometimes a Venn diagram can help facilitate a solution.

\noindent We are told $P(U)=.40$, $P(A|U)=.50$ and $P(A|U^c)=.30$.

\noindent The event $A = (A\cap U)\cup (A\cap U^c)$.

\noindent By the multiplicative rule of conditional probability

$P(A\cap U) = P(A|U)P(U) = .50\cdot .40 = .20$ (green region).

$P(A\cap U^c) = P(A|U^c)P(U^c) = .30\cdot (1-.40)=.18$ (yellow region).

\noindent (a) $P(A) = P(A\cap U) + P(A\cap U^c) = .20 + .18 = .38$.

\noindent (b) $P(U|A) = \dfrac {P(U\cap A)}{P(A)} = \dfrac {.20}{.38}=\dfrac {10}{19}$.


\newpage


\noindent {\bf Remark.}\\
\noindent Sometimes it is easier to compute an unconditional probability by computing it in pieces by assuming an event occurs.
This is the situation where $P(A)$ might be difficult but, for some well-chosen $B_i$'s, $P(A|B_i)$ is fairly easy/straightforward.
This idea works especially well in situations where the experiment is performed in stages (i.e., a ``sequential experiment").\\

\vskip .5 in

\noindent {\bf The law of total probability (LOTP).}\label{lotp}\\
\noindent If $B_1, B_2, \dots, B_n$ are mutually exclusive and exhaustive, then for any event $A\subset \Omega$,
\begin{eqnarray*}
P(A)
&=& \sum_{i=1}^n P(A\cap B_i)\\
&=& \sum_{i=1}^n P(A|B_i)P(B_i)\end{eqnarray*}

\vskip .5 in

\noindent {\bf Advice:}\\
The law of total probability is one of the more widely used results in all of applied probability and
is an indispensable tool that will be used throughout this course.
Read the examples I do, and know this result!\\

\vskip .5 in

\noindent Here's a picture of this result with a partition of $n=6$ events $B_1,B_2,\dots,B_6$:\\

\includegraphics*[-150,0][200,100]{lotp1.jpg}
\begin{center}{\bf Figure.} $B_1,\dots,B_6$ partition $\Omega$\end{center}


\noindent The picture above shows the event $A$ being decomposed into $6$ ``pieces": the piece where $A$ meets $B_1$, the piece where $A$ meets $B_2$, and so on.
Since $B_i$ for $i=1,2,3,4,5,6$ are mutually exclusive, $A\cap B_i$ for $i=1,2,3,4,5,6$ are also mutually exclusive; therefore, since
$$A=\bigcup_{i=1}^n (A\cap B_i),$$
by the countable additivity axiom $\displaystyle P(A) = \sum_{i=1}^n P(A\cap B_i)$. Moreover, since $P(A\cap B_i)=P(A|B_i)P(B_i)$, we also have the alternate form
of the law of total probability.



\newpage

\noindent {\bf Example.}\\
We have two coins in a box. One coin is fair, the other has two heads.  One of the coins is selected uniformly at random and tossed 3 times.
Compute the probability you will get 3 heads.\\

\noindent SOLUTION:\\
Let $F$ be the event we select the fair coin, $F^c$ the biased coin, and let $A$ be the event we get 3 heads.
We are told $P(F)=\frac 12$ and $P(F^c)=\frac 12$, $P(A|F) = \frac 18$ and $P(A|F^c)=1$. So,
$$P(A) = P(A|F)P(F)+P(A|F^c)P(F^c) = \frac 18\cdot \frac 12 + 1\cdot \frac 12 = \frac {9}{16}.$$


\vskip .5 in

\noindent {\bf Example.}\\
You toss a fair coin. If it shows heads, you roll {\em one} fair 6-sided die. If it shows tails, you roll {\em two} fair 6-sided dice.
Compute the probability the sum total of the die(dice) is 6.\\

\noindent SOLUTION:\\
Let $H$ be the event you toss heads, and let $S$ be the event the total on the die(dice) shows 6.
$P(S|H) = \frac 16$, and $P(S|H^c) = \frac {5}{36}$. Therefore,
$$P(S) = P(S|H)P(H)+P(S|H^c)P(H^c) = \frac 16\cdot \frac 12 + \frac 5{36}\cdot\frac 12 = \frac {11}{72}.$$

\vskip .5 in

\noindent {\bf Example.}\\
You roll a fair 6-sided die three times. Find the probability the sum is 9.\\

\noindent SOLUTION:\\
There are other ways to do this problem, but I will illustrate a solution using the law of total probability, where I condition on the value of the first die.
When we roll the first die, any of the numbers $1,2,3,4,5$ or $6$ can occur equally likely. Let $F_i$ be the event the first die shows an $i$.
Let $A$ be the event the sum of the three dice is 9. We want to compute $P(A)$.  \\

\noindent We try to compute $P(A|F_i)$ for each $i=1,2,3,4,5$ and $6$.  Start with $P(A|F_1)$. If the first die shows a 1, then the other two dice would need to show a total of 8
in order for the sum of the three dice to be 9; therefore $P(A|F_1)= P(8\mbox{ on two dice}) = \frac 5{36}$. Similarly,
for $P(A|F_2)$, if a 2 occurs on the first die, then the other two dice would need to total to 7, so $P(A|F_2) = P(7\mbox{ on two dice}) = \frac 6{36}$.
Continuing in this manner, $P(A|F_3) = P(6\mbox{ on two dice})=\frac 5{36}$, $P(A|F_4) = P(5\mbox{ on two dice})=\frac 4{36}$,
$P(A|F_5) = P(4\mbox{ on two dice})=\frac 3{36}$, and $P(A|F_6) = P(3\mbox{ on two dice})=\frac 2{36}$. Since $P(F_i)=\frac 16$ for all $i$,
$$P(A) = \sum_{i=1}^6P(A|F_i)P(F_i) = \frac 16\left(\frac 5{36} + \frac 6{36} + \frac 5{36} + \frac 4{36} + \frac 3{36} + \frac 2{36} \right)=\frac {25}{216}.$$



\newpage



\noindent {\bf Example.} (Simple epidemic model)\label{simpleepidemicmodel}\\
A population of size $N=10$ consists of people who are either diseased or non-diseased.
Within the population a subset of size $D=3$ are diseased ($N-D=7$ are non-diseased).
We randomly select a subset of size 2.
If they are both non-diseased, put them back into the population.
If you get one of each, the non-diseased person becomes diseased and then {\em both are put back into the population}.
If they are both diseased, remove them from the population.
{\em One} person is then selected uniformly at random from the group. Compute the probability they are diseased.\\

\noindent SOLUTION:\\
You recognize this experiment is performed in two stages.
In the first stage we select a subset of size 2, and the configuration
of the population changes depending on the sample that was chosen.\\

\noindent Let $D_i$ denote the event that, in the first stage, we select $i$ diseased people.
If $i=0$, then the population configuration remains the same (two just put the two non-diseased people back into the population).
If $i=1$, then the population remains the same size, namely, $N$, but the number of diseased people increases from $D$ to $D+1$ (non-diseased goes from $N-D$ to $N-D-1$).
If $i=2$, then the population decreases from $N$ to $N-2$, but the number of diseased people also decreases from $D$ to $D-2$.\\

\noindent In the second stage we select one person from the resulting population.  Let $I$ be the event that we select a diseased person.\\

$P(I|D_0) = \frac DN= \frac {3}{10},\qquad\quad\  P(I|D_1) = \frac {D+1}N=\frac {4}{10},\qquad\ \  P(I|D_2) = \frac {D-2}{N-2}= \frac {1}{8}.$\\

$P(D_0) = \frac {{D\choose 0}{N-D\choose 2}}{{N\choose 2}}=\frac 7{15},\quad P(D_1) = \frac {{D\choose 1}{N-D\choose 1}}{{N\choose 2}}=\frac 7{15},\quad P(D_2) = \frac {{D\choose 2}}{{N\choose 2}}=\frac 1{15}.$\\

\noindent By the law of total probability
$$P(I) = \frac 3{10}\cdot \frac 7{15} + \frac 4{10}\cdot \frac 7{15} + \frac 18\cdot \frac 15=\frac {211}{600}\approx 0.35.$$

\vskip .5 in


\noindent {\bf Exercise for the student.}\\
Try to re-do this example with one small change: in the case $i=1$ where we draw 1 diseased and 1 nondiseased, instead of making the non-diseased person diseased,
flip a fair coin so that with probability $p=\frac 1{10}$ make the non-diseased person diseased; otherwise, keep them non-diseased. Then put them both back into the population.





\newpage






\noindent {\bf Example.}\\
In a standard deck, a ``10-card" is card with rank 10, Jack, Queen or King.
I deal you two cards from a standard deck. If you make `21' (i.e., an Ace together with a 10-card) you win, and the games stops.
Else, you get to trade in either 1 or 2 cards to make 21.
You trade one card if you need exactly one of an Ace or a 10-card to get 21.  You trade 2 cards if you have neither an Ace nor a 10-card.
What's the chance you win?\\

\noindent SOLUTION:\\
Let $W$ be the event you win, i.e., make 21.
Let $T_0$ be the event you trade 0, meaning you made 21 on the initial deal,
$T_A$ the event that you trade in one card needing an Ace,
$T_{10}$ the event that you trade in one card needing a 10-card, and
$T_2$ the event that you trade in 2 cards.
In this problem I assume if you haven't made 21 on the initial deal you will {\em not} trade in an Ace nor will you trade in the 10-card.\\

\noindent $P(W|T_0) = 1.$\\

\noindent $P(W|T_A) = \frac {{4\choose 1}}{{50\choose 1}}$ since there are 4 Aces in the remaining deck of 50 cards.\\

\noindent $P(W|T_{10}) = \frac {{16\choose 1}}{{50\choose 1}}$ since there are 16 10-cards in the remaining deck of 50 cards.\\

\noindent $P(W|T_2) = \frac {{4\choose 1}{16\choose 1}}{{50\choose 2}}$.\\

\noindent $P(T_0) = \frac {{4\choose 1}{16\choose 1}}{{52\choose 2}}$ since there are 4 Aces and 16 10-cards.\\

\noindent $P(T_A) = \frac {{16\choose 2}+{16\choose 1}{32\choose 1}}{{52\choose 2}}$: you have either two of 16 10-cards or one of the 10-cards and one of the remaining 32 non-10-cards and non-Aces.\\

\noindent $P(T_{10}) = \frac {{4\choose 2}+{4\choose 1}{32\choose 1}}{{52\choose 2}}$: you have either 2 Aces or one of the 4 Aces and one of the remaining 32 non-10-cards and non-Aces.\\

\noindent $P(T_2) = \frac {{32\choose 2}}{{52\choose 2}}$: you drew 2 cards from the 32 non-Aces and non-10-cards.\\

\bigskip
\noindent By the law of total probability,
\begin{eqnarray*}
P(W) &= & P(W|T_0)P(T_0) +  P(W|T_A)P(T_A) +  P(W|T_{10})P(T_{10}) +  P(W|T_2)P(T_2)\\
& & \\
& = & 1\cdot \frac {{4\choose 1}{16\choose 1}}{{52\choose 2}} +
\frac {{4\choose 1}}{{50\choose 1}}\cdot \frac {{16\choose 2}+{16\choose 1}{32\choose 1}}{{52\choose 2}} +
\frac {{16\choose 1}}{{50\choose 1}}\cdot \frac {{4\choose 2}+{4\choose 1}{32\choose 1}}{{52\choose 2}} +
\frac {{4\choose 1}{16\choose 1}}{{50\choose 2}}\cdot \frac {{32\choose 2}}{{52\choose 2}}\\
& & \\
& & \\
&\approx & 0.138\end{eqnarray*}



\newpage

\noindent {\bf Example.}\\
You roll a fair 6-sided die repeatedly.  Compute the probability that no odd number occurs before the first 6 appears.\\

\noindent SOLUTION:\\
Where do we start?  Think about what sample points must look like in this event. Look at where the first 6 can possibly occur, and
then we need to make sure no odds occur before it (evens other than 6 are okay, but $1,3$ and $5$ are no-no's). \\

\hskip 1 in $6$, \quad
$\underbrace{26, \quad
46}_{\mbox{\small 1st 6 on 2nd roll}}$, \quad
$\underbrace{226, \quad
246, \quad
426, \quad
446}_{\mbox{\small 1st 6 on 3rd roll}}$, \quad
etc.\\

\noindent If we knew where the first 6 occurs, say on the $i$ roll ($i\ge 1$), then on rolls $1$ thru $i-1$ we can only have 2's and 4's repetition allowed.
This suggests that we condition on which roll the first 6 occurs. Let $F_i$ be the event that the first 6 occurs on the $i$th roll.
Let $A$ be the event that only 2's and 4's occur before the first 6 (i.e., no odds!). \\

\noindent By the law of total probability

\begin{eqnarray*}
P(A)
&=& P(A\cap F_1) + P(A\cap F_2) + P(A\cap F_3) + P(A\cap F_4) + \cdots\\
&=& \frac 16 + \frac 2{6^2} + \frac {2^2}{6^3} + \frac {2^3}{6^4}  + \cdots \qquad (\mbox{a geometric series with $r=\frac 13$})\\
& = & \frac {\frac 16}{1 - \frac 13} = \frac 14.\\
\end{eqnarray*}


\vskip .5 in

\noindent {\bf Remark.} (some high-powered intuition) \\
There is an interesting intuitive argument for why the answer to this problem is $\frac 14$. It relies on the fact
that when you roll a die repeatedly {\em every} one of the six numbers $1,2,3,4,5$ and $6$ will eventually occur, i.e., we can ignore extreme instances where a sequence is missing at least one of the numbers. The intuitive idea is then:
when we look at where the first 6 occurs in a sequence relative to the first 1, the first 3 and the first 5; then each of the $4!$ permutations of the numbers $1,3,5,$ and $6$ should be equally likely. That is, when we look at an infinite sequence of rolls, and we identify where the first 1, the first 3, the first 5 and the first 6 occurs within it, then each of the $4!$ possibilities should be equally likely.  Therefore, of the $4!$ equally likely ways of seeing these numbers appear for their first time,
$3!$ of them keep the 6 before the $1,3$ and $5$, so the answer is $\frac {3!}{4!} = \frac 14$.\\




\newpage


\noindent {\bf The Bayes rule.}\label{s:bayesrule}\\

\noindent This section is just a straightforward extension of the law of total probability.
\noindent The situation now is like this: We have an event $A$ and a partition $B_1,B_2,\dots, B_n$ of $\Omega$. We know
$P(A|B_i)$ and $P(B_i)$ for $i=1,2,\dots,n$ and we want $P(B_j|A)$ for some fixed $j$.  But, this is not hard to find. Look:
\begin{eqnarray*}
P(B_j|A)
&=& \dfrac {P(B_j\cap A)}{P(A)} \qquad (\mbox{by the conditional probability formula})\\
&=& \dfrac {P(A|B_j)P(B_j)}{P(A)} \qquad (\mbox{by the multiplicative rule on page \pageref{multrulecondprob}})\\
&=& \dfrac {P(A|B_j)P(B_j)}{P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+\cdots P(A|B_n)P(B_n)}. \qquad (\mbox{by LOTP})\\
\end{eqnarray*}
We just proved the formula called\dots


\bigskip


\noindent {\bf Bayes rule.}\label{bayesrule1}\\
\noindent If $B_1,B_2,\dots,B_n$ are mutually exclusive and exhaustive, and $A\subseteq \Omega$ is any event, then for any $j$,
$$P(B_j|A) = \dfrac {P(A|B_j)P(B_j)}{\sum_{i=1}^n P(A|B_i) P(B_i)}.$$

\vskip .3 in

\noindent {\bf Advice:}\\
Don't memorize the Bayes formula: it's usually better to understand the computation of $P(B_j|A)$ above.\\

\vskip .3 in

\noindent We now reconsider some old examples we did from the law of total probability section.\\

\noindent {\bf Example.}\\
We have two coins in a box. One coin is fair, the other has two heads.  One of the coins is selected uniformly at random and tossed 3 times.
{\em Old question}: Compute the probability you will get 3 heads.\\
{\em New question}: Given you tossed 3 heads, what's the probability you picked the fair coin?\\

\noindent SOLUTION:\\
$P(F|A) = \dfrac {P(F\cap A)}{P(A)}$, so we just need to compute $P(A)$ and $P(F\cap A)$. We can get $P(A)$ via the law of total probability:
$$P(A) = P(A|F)P(F)+P(A|F^c)P(F^c)=\dfrac 18\cdot \dfrac 12 + 1\cdot \dfrac 12 = \dfrac 9{16}.$$
Moreover, by the multiplicative rule, $P(F\cap A) = P(A\cap F) = P(A|F)P(F) = \dfrac 18\cdot \dfrac 12 = \dfrac 1{16}.$
Therefore,
$$P(F|A) = \dfrac {P(A|F)P(F)}{P(A)}=\dfrac {\frac 1{16}}{\frac 9{16}}=\dfrac 1{9}.$$




\newpage



\noindent {\bf Example.}\\
You toss a fair coin. If it shows heads, you roll {\em one} fair 6-sided die. If it shows tails, you roll {\em two} fair 6-sided dice.
{\em Old question}: Compute the probability the sum total of the die(dice) is 6.\\
{\em New question}: If the sum total of the die(dice) is 6, what's the chance you flipped heads?\\

\noindent SOLUTION:\\
$P(H|S)=\dfrac {P(H\cap S)}{P(S)}=\dfrac {P(S|H)P(H)}{P(S)}$, where, by the law of total probability,
$$P(S) = P(S|H)P(H)+P(S|H^c)P(H^c) = \frac 16\cdot\frac 12 + \frac 5{36}\cdot\frac 12 = \frac {11}{72}.$$
Moreover,
$P(S|H)P(H) = \frac 16\cdot\frac 12 = \frac 1{12}$; so, $P(H|S) = \dfrac {\frac 1{12}}{\frac {11}{72}}=\frac 6{11}.$\\

\vskip .5 in

\noindent {\bf Example.}\\
You roll a fair 6-sided die three times. {\em Old question}: Find the probability the sum is 9.\\
{\em New question}: If the sum is 9, what's the probability the first die shows a 1?\\

\noindent SOLUTION:\\
By law of total probability,
$$P(A) = P(A|F_1)P(F_1) + \cdots + P(A|F_6)P(F_6) = \dfrac {25}{216}.$$
Therefore,
$$P(F_1|S)=\dfrac {P(A|F_1)P(F_1)}{P(A)}=\dfrac {\frac {5}{36}\cdot \frac 16}{\frac {25}{216}}=\dfrac 15.$$


\vskip .5 in

\noindent {\bf Example.}\\
You roll a fair 6-sided die repeatedly.  {\em Old question}: Compute the probability that no odd number occurs before the first 6 appears.\\
{\em New question}: If no odds occur before the first 6, what's the probability the first 6 occurs on the first roll?\\

\noindent SOLUTION:\\
We computed earlier that $P(A) = \frac {1}{4}$ and $P(F_1\cap A) = P(F_1) = \frac 16$ since $F_1\subseteq A$. Therefore,
$$P(F_1|A) = \dfrac {\frac 16}{\frac 14} = \dfrac 23.$$

\vskip .4 in

\noindent {\bf Remark.}\\
\noindent In all these examples, the new question asked for a
conditional probability with the roles of the events involved being {\em reversed}.\\




\newpage


\noindent {\bf Example.}\\
At a certain factory 3 machines make widgets. Respectively, 30\%, 50\%, and 20\% of the widgets are made by machines A, B, and C.
Historically, it is known that 4\%, 5\%, and 3\% of the widgets made by A, B, and C, respectively are defective.  A widget is randomly sampled from all widgets made
and it is found to be defective.  Compute the probability it was made by machine A, then by machine B, and finally, by machine C.\\

\noindent SOLUTION:\\
Let $A$ (resp., $B,C$) be the event the widget was made by machine A (resp., B,C), and $D$ the event the widget is defective.
We are told the following information:
$$P(A)=.3,\quad P(B)=.5,\quad P(C)=.2,$$
$$P(D|A)=.04,\quad P(D|B)=.05,\quad\mbox{and}\quad P(D|C)=.03.$$
We want to compute $P(A|D), P(B|D),$ and $P(C|D)$.
I'll point out the obvious and state that a widget cannot be made by more than one machine, so $A, B,$ and $C$ are mutually exclusive; moreover,
every widget is made by one of $A,B$, or $C$ so they are exhaustive. Therefore, the law of total probability gives
\begin{eqnarray*}
P(D) &=& P(D|A)P(A) + P(D|B)P(B)+P(D|C)P(C)\\
&=& .04(.3)+.05(.5)+.03(.2) \\
&=& .012+.025+.006 = .043,\end{eqnarray*}
and, by Bayes rule,
\begin{eqnarray*}
P(A|D) = \dfrac {P(D|A)P(A)}{P(D)} = \dfrac {.012}{.043} \approx 0.279\\
P(B|D) = \dfrac {P(D|B)P(B)}{P(D)} = \dfrac {.025}{.043} \approx 0.581\\
P(A|D) = \dfrac {P(D|C)P(C)}{P(D)} = \dfrac {.006}{.043} \approx 0.140.\\
\end{eqnarray*}
In the Bayes rule setting, the conditional probabilities $P(D|A), P(D|B),$ and $P(D|C)$ are called the {\bf\em likelihoods}\label{likelihoodsbayes}, the values
$P(A),P(B),$ and $P(C)$ are called the {\bf\em prior probabilities}\label{priorprob} and
$P(A|D),P(B|D),$ and $P(C|D)$ are called the {\bf\em posterior probabilities}\label{posteriorprob}.  Bayes rule give us a way to compute
the posterior probabilities from the prior probabilities and the likelihoods.\\





\newpage



\noindent {\bf Independence.}\label{independence1}\\

\noindent The concept of independence is unique to probability theory.
We first discuss what it means for two events to be independent, then we discuss
what it means for several (or infinite sequences of) events to be independent.
Later in the course (after we've discuss random variables) we will revisit the independence idea and discuss
what it means for random variables to be independent.\\

\vskip .5 in

\noindent {\bf Independence of two events.}\label{d:2eventsindependent}\\
Suppose $A$ and $B$ are events.  We say $A$ and $B$ are {\bf\em independent} provided
$$P(A\cap B) = P(A)P(B).$$
\noindent If $A$ and $B$ are not independent, we say they are {\bf\em dependent}\label{dependentevents}.\\

\vskip .5 in

\noindent {\bf Remark.} (intuitive meaning of independence of events)\\
Loosely speaking, $A$ and $B$ are independent means $P(A|B)=P(A)$ and $P(B|A)=P(B)$, i.e., knowledge of one of these events occurring
does not influence the probability of the other event. To see why, assume $P(B)>0$. Then, if $A$ and $B$ are independent we would have
$$P(A|B) = \dfrac {P(A\cap B)}{P(B)} = \dfrac {P(A)P(B)}{P(B)}=P(A).$$
Likewise, if $P(A)>0$ and $A$ and $B$ are independent we would have $P(B|A)=P(B)$.\\

\noindent The reason we say ``loosely speaking" is that we also want the definition of independence to hold for events that have probability {\em zero}, too, and
strictly speaking, we can't use the conditional probability formula if the conditioning event has probability 0. Nevertheless, if the events have positive probability and
we can show one of $P(A|B)=P(A)$ or $P(B|A)=P(B)$ then the events $A$ and $B$ are independent and if, in addition, $P(A)>0$ and $P(B)>0$ then the other will be true as well!
Let's show this:\\

\noindent Suppose $P(A)>0$ and $P(B)>0$ and, without loss of generality, say $P(A|B)=P(A)$. Then
$$P(A|B) = \dfrac {P(A\cap B)}{P(B)} = P(A) \implies P(A\cap B)=P(A)P(B)\implies A,\,B\ \mbox{independent.}$$
Moreover, because $A$ and $B$ are independent and $P(A)>0$ as well, it follows
$$P(B|A) = \dfrac {P(B\cap A)}{P(A)} = \dfrac {P(B)P(A)}{P(A)}=P(B),$$
and the other conditional statement holds, too!\\



\newpage



\noindent {\bf Remark.}\\
\noindent Suppose $A$ and $B$ are independent events.  It seems intuitively clear that if knowledge that $B$ occurs does not influence the probability of $A$,
then knowledge that $B$ doesn't occur shouldn't influence the probability of $A$ either.  In fact, more is true\dots\medskip

\noindent If $A$ and $B$ are independent events, then

$A$ and $B^c$ are independent events,

$A^c$ and $B$ are independent events, and

$A^c$ and $B^c$ are independent events.\\


\noindent In fact, let's prove: $A$ and $B$ independent implies $A$ and $B^c$ are independent.  We need to show that $P(A\cap B^c) = P(A)P(B^c)$. To this end
\begin{eqnarray*}
P(A\cap B^c) & = & P(A) - P(A\cap B)\\
& = & P(A) - P(A)P(B) \quad (\mbox{used $A$ and $B$ being independent})\\
& = & P(A)[1-P(B)]=P(A)P(B^c)\quad (\mbox{complementary rule}).\end{eqnarray*}

\vskip .3 in

\noindent {\bf Exercise for the student.}\\
Please provide similar proofs to show that $A$ and $B$ independent implies the other pairs of events are independent, too.\\


\vskip .3 in



\noindent {\bf Example.}\\
Consider the following three Venn diagrams.  Each shows probabilities within the designed regions.
Which diagrams show independent events, which show dependent events?  Explain.\vskip -.2 in

\includegraphics*[-60,0][400,100]{independencevenns1.jpg}


\noindent SOLUTION:\\
\noindent Figure (a) shows independent events: $P(A)=.8$, $P(B)=.5$, and $P(A\cap B) = .4 = P(A)P(B).$  Notice that from the conditional probability
point of view, if, for instance, given $B$ occurred, then $A\cap B$ and $A^c\cap B$ occur in the same proportions as $A$ and $A^c$ in the original sample space:
$\frac {.4}{.5}=.8,\ \frac {.1}{.5}=.2$. The information that $B$ occurred didn't influence the probability of $A$ (nor $A^c$).\\

\noindent Figure (b) shows dependent events: $P(A)=.5$, $P(B)=.1$, and $P(A\cap B)=P(\varnothing)=0\ne P(A)P(B)$.  This is a situation where mutually exclusive events are dependent.\\

\noindent Figure (c) shows independent events: $P(A)=.6$, $P(B)=0$, and $P(A\cap B) = P(\varnothing)=0=P(A)P(B)$.  This is a special case where mutually exclusive events can be
independent (when one or both have probability 0).



\newpage



\noindent {\bf Example and discussion.}\\
\noindent We have a box filled with 3 red and 4 green balls. We choose two balls.
Let $A$ be the event that the first ball is red. Let $B$
be the event the second ball is green.  Discuss whether or not these events are independent in each of the following situations:\\
sampling with replacement, and sampling without replacement.\\

\noindent SOLUTION:\\
\noindent First, try to guess the answer without any computations.  In each situation we ask: Will information of one of these events change the probability of the other?
Let's look at each situation separately\dots\\

\noindent If we sample with replacement, then the configuration of the box is the same at the second draw as it was for the first, so I'd expect these to be independent.
However, if we were sampling without replacement, then knowledge of the first draw will now {\em alter} the configuration of box at the second draw, so we cannot
expect these events to be independent.\\

\noindent In the situation of sampling {\em with replacement}:\\
\noindent $P(A)=\frac {3\cdot 7}{7\cdot 7}=\frac 37$ as there are $7^2$ possible outcomes but only $3\cdot 7$ of these ordered 2-tuples have a red in the first position.
$P(B)=\frac {7\cdot 4}{7\cdot 7}=\frac 47$ as there are $7\cdot 4$ ordered 2-tuples that have a green ball in the second position.
and
$P(A\cdot B) = \frac {3\cdot 4}{7\cdot 7} = \frac 37\cdot \frac 47 = P(A)P(B)$ shows $A$ and $B$ are independent.\\

\noindent In the situation of sampling {\em without replacement}:\\
$P(B|A) = \frac 46$ since a red drawn on the first leaves us with 6 equally likely balls of which 4 are green, but then $P(B|A)\ne \frac 47=P(B)$ and, therefore, the events $A$ and $B$ must be dependent.\\


\vskip .5 in

\noindent {\bf Exercise for the student.}\\
Roll two fair 6-sided dice. Let $A$ be the event that the sum total of the dice is even. Let $B$ be the event the sum total is $3,6,9$ or $12$.
Are $A$ and $B$ independent?\\


\vskip .5 in

\noindent {\bf Exercise for the student.}\\
Flip a coin twice: $\Omega=\{hh,ht,th,tt\}$. \\
Let $H_1$ be the event of a head on the first toss.\\
Let $H_2$ be the event of a head on the second toss.\\
Verify whether or not $H_1$ and $H_2$ are independent in each of the following situations:

(a) $\Omega$ is equally likely, i.e., $P(\{hh\})=P(\{ht\})=P(\{th\})=P(\{tt\})=\frac 14$.

(b) $P(\{hh\})=.06, P(\{ht\})=.24, P(\{th\})=.14, P(\{tt\})=.56$.

(c) $P(\{hh\})=.4, P(\{ht\})=.3, P(\{th\})=.2, P(\{tt\})=.1$.



\newpage

\noindent Before we state the definition of independence of {\em many} events, we first define what it means for just
3 events to be independent.\\

\noindent {\bf Independence of 3 events.}\label{d:3eventsindependent}\\
Let $A$, $B$, and $C$ be events. We say they are {\bf \em (mutually) independent} provided {\bf \em all} 4
of the following conditions hold true:
$$\left.\begin{array}{ccc}P(A\cap B) & = & P(A)P(B)\\
P(A\cap C) & = & P(A)P(C)\\
P(B\cap C) & = & P(B)P(C)\end{array}\right\}\quad \mbox{\bf (*)}$$
{\bf\em and}
$$P(A\cap B\cap C) = P(A)P(B)P(C).$$
If $A, B$ and $C$ satisfy the conditions $\mbox{\bf (*)}$, they are called {\bf\em pairwise independent}.  If, additionally, they
also satisfy the last condition, we call them {\em mutually} independent or, just, independent.\\

\vskip .5 in
\noindent {\bf Remark.}\\
From the definition above, 3 independent events are always pairwise independent, but the converse is not necessarily true.
There are examples of events that are pairwise independent but not (mutually) independent.  Here's a classic example:\\

\vskip .5 in

\noindent {\bf Example.} (pairwise independent events that are not independent)\\
Roll two fair 6-sided dice (one red, one green).  Let\\
$A$ be the event the red die shows a 1,\\
$B$ be the event the green die shows a 6,\\
$C$ be the event the sum is 7.\\
Show that $A, B$ and $C$ are pairwise independent but not mutually independent.\\

\noindent SOLUTION:\\
In this experiment we clearly have $P(A)=P(B)=P(C)=\frac 16$. Now,
$$A\cap B = \{16\},\quad A\cap C = \{16\},\quad \mbox{and}\quad B\cap C = \{16\}.$$
Therefore, $P(A\cap B) = \frac 1{36} = P(A)P(B)$, and, similarly, $P(A\cap C) = P(A)P(C)$, $P(B\cap C)=P(B)P(C)$ and these events are pairwise independent.
{\em However}, $A\cap B\cap C = \{16\}$ and
$$P(A\cap B\cap C) = \frac 1{36}\ne P(A)P(B)P(C) = \frac 1{216},$$
so these events are {\em not} mutually independent!\\



\newpage

\noindent {\bf Independence of events.}\label{d:independentevents}\\
We say a collection of events (finite or infinite) $\{A_1,A_2,A_3,\dots\}$ is {\bf\em independent}
provided {\em every} finite subcollection (of two or more) of these events,
$$\{A_{i_1},A_{i_2},\dots, A_{i_k}\},$$
has the property
$$P(A_{i_1}\cap A_{i_2}\cap \cdots \cap A_{i_k}) = P(A_{i_1})P(A_{i_2})\cdots P(A_{i_k}).$$

\vskip .5 in

\noindent {\bf Remark.}\\
Generalizing an earlier result, if $\{A_1,A_2,A_3,\dots\}$ is a collection of independent events, then
replacing any number of the $A_i$ in this collection by its complement, $A_i^c$, will give another independent
collection.\\


\vskip .3 in

\noindent {\bf Remark.}\\
In practice, independence can usually be assumed by virtue of the experiment.  Here are some illustrations where assuming independence is plausible:

$\bullet$ Alan and Betty are each tossing coins. Then the result of Alan's tosses shouldn't influence Betty's tosses (and vice-versa).

$\bullet$ A person rolls a die repeatedly (or tosses a coin repeatedly). The the result on a roll (toss) shouldn't influence other rolls (tosses).

$\bullet$ A person buys a lottery ticket each day. The events she has a winning ticket on day $i$ ($i=1,2,3,\dots$) should be independent.\\

\vskip .3 in

\noindent {\bf Remark.}\label{demorganindependence}\\
The thing that is nice about independence (when we have it or when it can be assumed) is that, if $A_1,A_2,A_3,\dots$ are indepedent, then,
for instance,
\begin{eqnarray*}
P\left(\bigcup_{i=1}^n A_i\right)
&=& 1 - P\left( \Big( \bigcup_{i=1}^n A_i \Big)^c\right)\quad(\mbox{by complementary rule})\\
&=& 1 - P\left(\bigcap_{i=1}^n A_i^c\right)\quad \qquad \quad \, (\mbox{by DeMorgan's law})\\
&=& 1 - \prod_{i=1}^nP\left(A_i^c\right) \quad \qquad \qquad \ \ \ \, (\mbox{by independence})\\
&=&  1 - \prod_{i=1}^n\left(1 - P(A_i) \right)\quad (\mbox{by complementary rule}),
\end{eqnarray*}
which may be easier to employ than, say, the inclusion-exclusion rule.

\newpage


\noindent {\bf Example.} (The State of Maryland Powerball lottery)\\
\noindent A Powerball lottery ticket is formed in two steps.

step 1: select of 5 numbers from 1 thru 69 inclusive;

step 2: choose a Powerball number from 1 thru 26 inclusive. \\
The pair
$$\left( \stackrel{\mbox{subset of size 5}}{\mbox{from }\{1,2,\dots,69\}}, \stackrel{\mbox{choose one from}}{\{1,2,\dots,26\}}\right)$$
is a Powerball lottery ticket.  You can play this game each Wednesday and Saturday every week until you die.  As of now, the cost is \$2 per lottery ticket.\\
(a) (Old question) How many Powerball lottery tickets are possible?\\
(b) What's the probability you hold a winning ticket$^*$?\\
(c) If you play twice per week for the next 50 years, what the probability you win at least once?\\
(d) Suppose that you and 4999 of your most trusted friends all {\em independently} follow this same strategy and agree to share the winnings if anyone wins. What's
the probability you win at least once now?\\
$^*$ By {\em win} and {\em winning ticket} I mean winning the whole shebang -- the jackpot!\\

\noindent SOLUTION:\\
(a) The basic counting principle implies there are ${69\choose 5}26 = 292,201,338$ possible Powerball tickets.\\

\noindent (b) The reciprocal of the answer in part (a) is the probability of selecting the winning ticket: $\frac 1{292201338} \doteqdot 3.422\times 10^{-9}$.\\

\noindent (c) Playing twice per week for 50 years means you are buying
$$2\,\frac {\mbox{tickets}}{\mbox{week}}\times 52\,\frac{\mbox{weeks}}{\mbox{year}}\times 50\,\mbox{years} = 5200\ \mbox{ lottery tickets}.$$
\noindent The event that we win at least once is
$$\bigcup_{i=1}^{5200} W_i,$$
where $W_i$ is the event we win on the $i$th lottery ticket. By the way, we know that, for every $i$, $P(W_i)=P(W_1) \doteqdot 3.422\times 10^{-9}$. \\
\noindent Therefore,
$$P\left(\bigcup_{i=1}^{5200} W_i \right) = 1 - \prod_{i=1}^{5200}\left(1-P(W_i)\right) = 1-\left(1-3.422\times 10^{-9}\right)^{5200}\doteqdot 1.779\times 10^{-5}\approx .0000179.$$

\bigskip

\noindent (d) Let $B_1$ be the event you win at least once with this strategy. Let $B_i$ be the event that friend $i$ wins at least once with this strategy for $i=2,3,4,\dots, 5000$.  Then, assuming all $B_1,B_2,\dots,B_{5000}$ are independent, the probability we win is
$$P\left(\bigcup_{i=1}^{5000} B_i \right) = 1 - \prod_{i=1}^{5000}\left(1-P(B_i)\right) = 1-\left(1-1.779\times 10^{-5}\right)^{5000}\approx .085.$$








\newpage




\newpage

\begin{center}{\bf \Large III. Random variables, discrete random variables.}\end{center}

\vskip 1 in



\newpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{center}{\bf Lecture }%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}{\bf Random variables}\label{rvs1}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{center}{\bf Lecture 9.}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\noindent Up to know we've been dealing with probability at the experiment level, i.e., we've concerned ourselves with the sample space - either counting sample
points or observing some property of the sample space and/or events.\\

\noindent In many cases we do not have to concern ourselves with the sample space, i.e., the sample points themselves. For example, many times an experiment
is performed the experimenter might make a measurement on the outcome and observe, instead, some real number. In this way the experimenter may not care necessarily about
the specific sample point observed but rather the set of sample points having a specific measured value (or values).\\


\vskip .5 in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{center}{\bf Lecture }%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent A {\bf\em random variable}\label{d:rv} (or {\bf\em rv}) $X$ is a real-valued function defined on all of $\Omega$, $X:\Omega\to {\mathbb R}$.\\

\vskip .25 in

\noindent {\bf Remark.}\\
Chance picks the $\omega$, and we observe $X(\omega)$ which is a real number.  Think that $X$ is making a ``measurement" on the $\omega$ that chance produced.
$X$ is really a deterministic function - we call it
a ``random" variable because we don't typically observe the random input $\omega$ into the function and, therefore, it would appear the $X$ randomly outputs $X(\omega)$.\\

\vskip .25 in

\noindent {\bf Notation.}\\
\noindent If $x\in {\mathbb R}$, then $(X=x)$ is shorthand for the subset of $\Omega$ that $X$ maps to $x$:
$$(X=x)=\{\omega\in\Omega:X(\omega)=x\}.$$
This is called the {\bf \em preimage of}\label{d:preimage} $\{x\}$ under $X$. Mathematicians sometimes call this the {\bf\em inverse image of $\{x\}$ under $X$}\label{d:inverseimage} and
denote it by
$X^{-1}[\{x\}]$.\\


\newpage


\noindent The following example demonstrates that rvs generalize experiment-level probability.\\

\noindent {\bf Example.} (A Bernoulli rv)\label{e:bernoulli}\\
Let $A\subseteq \Omega$ be any event. Then, by defining the rv
$$X(\omega) = \left\{ \begin{array}{cl}1 & \mbox{for }\omega\in A\\ 0 & \mbox{for }\omega\in A^c  \end{array}\right.,$$
we would have $P(X=1)=P(A)$ and $P(X=0)=P(A^c)$.\vskip -.4 in

\includegraphics[-80,0][300,170]{rvmapping2.jpg}\vskip -.4 in

\begin{center}{\bf Figure.}\label{f:bernoullimapping}  The random variable $X$ is mapping $\Omega$ into the set $\{0,1\}$.\end{center}

\vskip .5 in

\noindent The figure above shows that $X$ is mapping all the $\omega\in A$ to the point 1 and mapping all the $\omega\in A^c$ to the point 0.
In particular, if $X$ returns the value 1, then we know that the $\omega\in A$, but
we may not necessarily know which specific $\omega\in A$ occurred (nor do we care).\\


\vskip .5 in

\noindent {\bf Advice:}\\
\noindent When considering random variables (however they are defined) it's {\em always}
a good idea to keep in mind the possible values the random variable can take on (and which they cannot).\\
%This will (hopefully) be emphasized throughout the course.\\




\newpage

\noindent {\bf Example.}\\
Toss a fair coin 3 times:  $\Omega = \{hhh, hht, hth, htt, thh, tht, tth, ttt\}$.\\
Consider the rvs:

$X =$ the number of heads.

$Y =$ 1 if the first two tosses are the same parity, and $Y=0$ otherwise.\\


\noindent $X$ takes possible values 0, 1, 2 and 3. \quad $Y$ takes possible values 0 and 1.

\noindent The set of possible values $\{0,1,2,3\}$ is the {\bf\em image of} $X$\label{d:imageofrv}, $\{0,1\}$ is the {\bf\em image of} $Y$.

\noindent To the right of the figure below are the preimages.


\includegraphics[-90,0][300,220]{rvmapping4.jpg}

\vskip -2.4 in
\hskip 4.25 in $(X=0) = \{ttt\}$

\hskip 4.25 in $(X=1) = \{htt, tht, tth\}$

\hskip 4.25 in $(X=2) = \{hht, hth, thh\}$

\hskip 4.25 in $(X=3) = \{hhh\}$

\vskip 0.95 in

\hskip 4.25 in $(Y=0) = \{hth, htt, thh, tht\}$

\hskip 4.25 in $(Y=1) = \{hhh, hht, tth,ttt\}$

\vskip .5 in

\begin{center}{\bf Figure.}  Visualization of the $X$ and $Y$ mappings into the real numbers.\end{center}

\vskip .5 in

\noindent {\bf Remark.}\\
Assume the coin is fair.  We get the {\bf\em probability mass function (pmf)}\label{e:pmf} of rv $X$:

\begin{center}$P(X=0) = \frac 18$, $P(X=1) = \frac 38$, $P(X=2)=\frac 38$ and $P(X=3)=\frac 18$.\end{center}

\noindent We can also write this in {\bf\em tabular form}\label{e:tabularform}:
$$\displaystyle \begin{array}{c||cccc|}x & 0 & 1 & 2 & 3\\ \hline P(X=x) & .125 & .375 & .375 & .125\end{array}.$$

\bigskip

\noindent There happens to be a {\bf\em functional form}\label{e:functionalformpmf} as well:
$$P(X=x)=\dfrac {{3\choose x}}8\quad \mbox{for }x=0,1,2,3.$$

\bigskip

\noindent The pmf for $Y$ is $P(Y=0)=\frac 12 = P(Y=1)$.






\newpage




\noindent {\bf Basic types of random variables (rvs).}\label{typesofrvs1}\\
\noindent We'll see that rvs are extremely useful in helping us model common experiments and quantities that come up in practice. We now talk about types of rvs we will encounter.\\


\noindent There are essentially two different types of rvs: discrete rvs and continuous rvs, and
we classify the type of rv we are dealing with by investigating the set of possible values the rv can take on. A rv is classified
as {\bf\em discrete}\label{e:discreterv} if the set of possible values it can take on forms a discrete set, i.e., a finite or countably infinite
set of (real) values.  The criterion for classifying a rv as {\bf\em continuous} is a little tougher; loosely speaking, the set of possible values is a ``continuum", i.e., either
an interval $(a,b)$ (or $[a,b]$) of the real line with $a<b$ (allowing the possibility that $a=-\infty$ and/or $b=+\infty$ here) or a union of such intervals.  But please read the next paragraph.\\

\noindent A rv can be neither discrete nor continuous; for example, a rv whose set of possible values is
$\{x\in {\mathbb R}: 0\le x\le 1\}\cup \{2,3,4\}$ - this is not a discrete set, it is also not a union of intervals described above.
Another example of a rv that is neither discrete nor continuous is when the rv is a {\bf\em mixture} of the two types of rvs.
For example, say we roll a fair die. If we roll an even number,
toss a fair coin one time, and set $X=1$ if you get a head, $X=0$ if you get a tail (a discrete rv);
else, if we roll an odd number, pick a number uniformly at random from the interval $[0,1]$ so that $0\le X\le 1$ (a continuous rv).
The rv $X$ just described takes only values in the interval $[0,1]$ but is not continuous.  Neither is it discrete.  The rv just described is a mixture of
discrete and continuous rvs.
Random variables of this type are best understood using their so-called {\bf \em cumulative distribution functions}.
We will discuss this later in the course.\\

\noindent Lastly, I'll mention that continuous random variables can be further subcategorized into {\bf\em absolutely continuous} and {\bf\em singular continuous}.
The distinction between these types is of no importance right now, but
when we get into a discussion later in the course on continuous random variables we'll be sure to mention the differences then.\\

\vskip .25 in

\noindent {\bf Exercise.}\\
Classify each of the following random variables, and determine their set of possible values.\\
(a) Roll two 6-sided dice. $S$ is the sum of the upfaces.\\
(b) A dart lands on a dartboard$^*$. $R$ is the distance from the center where a dart lands.\\
(c) Throw two darts at a dartboard. If both darts land on the dartboard, then $X$ is

the distance between $x$-coordinate values where these darts land; else $X$ returns the

number of darts that miss the dartboard.\\
$^*$ Assume all dartboards are represented as the unit disk: $\{(x,y)\in {\mathbb R}^2: x^2+y^2\le 1\}$.\\



\newpage



\noindent {\bf Discrete random variables.}\label{d:discreterv}\\
%\noindent We will first focus our attention on the analysis of discrete random variables\dots.\\

A rv $X$ is called {\bf \em discrete} if the {\bf \em image of} $X$:
$$\{x\in {\mathbb R}: X(\omega)=x\mbox{ for some }\omega\in\Omega\}$$

is a discrete set, i.e., finite or countably infinite.  The image of $X$ is the set of possible

real values the random variable can return.

\includegraphics*[-82,0][300,130]{rvmapping6.jpg}\label{f:imagepreimage}

\begin{center}{\bf Figure.} Images and preimages of the discrete rv $X$.\end{center}


\vskip .25 in

\noindent {\bf Some facts regarding random variables.}\\
\noindent {\bf Fact 1}: The events
$(X=x)$ for $x\in \mbox{Image}(X)$ are mutually exclusive subsets of $\Omega$:
$(X=x)\cap(X=y)=\varnothing$ when $x\ne y$ (it is impossible for $X$ to map the same $\omega$ to two different values).\\
\noindent {\bf Fact 2}: The events $(X=x)$ for $x\in \mbox{Image}(X)$ are exhaustive: $\displaystyle \bigcup_{x\in \mbox{\tiny Image}(X)} (X=x) = \Omega$.\\

\noindent These facts imply the events $(X=x)$ for $x\in \mbox{Image}(X)$ are {\bf\em mutually exclusive and exhaustive}.  The axioms imply
$$\sum_x P(X=x) = 1,$$
where the sum is over all possible values of the random variable $X$, i.e., its image. The function of $x$, $p_X(x):=P(X=x)$, is called the {\bf\em probability mass function (pmf) of} $X$.\label{d:pmf}
The image of $X$ is the {\bf\em support of this pmf}.\label{d:pmfsupport}\\


\noindent {\bf Computing probabilities using pmfs.}$\label{pmfcalc}$\\
Once we know the pmf of a discrete rv, computing the probability of any event involving this one rv is straightforward:
If $I\subseteq {\mathbb R}$, then
$$P(X\in I) = \sum_{x\in I}P(X=x),$$
i.e., sum the probability masses for each value in the support that belongs to $I$.
In this way, we no longer need to compute such probabilities at the experiment level;
{\bf\em the pmf allows us to essentially forget about the sample space when computing probabilities.}




\newpage





\noindent {\bf Example.}\\
Roll two fair 6-sided dice. Let $X$ be the total (sum) showing on the upfaces. $X$ is discrete since the image of $X$, namely,
$$\{2,3,4,5,6,7,8,9,10,11,12\}$$
is a discrete set. For the record, the {\bf\em preimages}\label{e:preimage} for each value in the image are given in the following table:\\

\hskip -.5 in
$\begin{array}{rclrcl}
(X=2)\!\!\!\! & = & \!\!\!\!\{(1,1)\} & (X=3)\!\!\!\! & = &\!\!\!\! \{(1,2),(2,1)\} \\
(X=4)\!\!\!\! & = & \!\!\!\!\{(1,3),(2,2),(3,1)\}  & (X=5)\!\!\!\! & = &\!\!\!\!\{(1,4),(2,3),(3,2),(4,1)\} \\
(X=6)\!\!\!\! & = & \!\!\!\!\{(1,5),(2,4),(3,3),(4,2),(5,1)\} & (X=7)\!\!\!\! & = & \!\!\!\!\{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\} \\
(X=8)\!\!\!\! & = &\!\!\!\! \{(2,6),(3,5),(4,4),(5,3),(6,2)\} & (X=9)\!\!\!\! & = & \!\!\!\!\{(3,6),(4,5),(5,4),(6,3)\} \\
(X=10)\!\!\!\! & = &\!\!\!\! \{(4,6),(5,5),(6,4)\} & (X=11) \!\!\!\!& = &\!\!\!\! \{(5,6),(6,5)\} \\
(X=12)\!\!\!\! & = & \!\!\!\!\{(6,6)\} &  &  & \\\end{array}$
and, if $x\not\in\{2,3,4,5,6,7,8,9,10,11,12\}$, then $(X=x)=\varnothing$.\\

\bigskip

\noindent Here's the pmf of $X$ in different forms:

\bigskip

In {\bf\em tabular form}\label{pmftabularform}:
$$\begin{array}{c||ccccccccccc}x & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\ \hline
p_X(x) & \frac 1{36} & \frac 2{36} & \frac 3{36} & \frac 4{36} & \frac 5{36} & \frac 6{36}
& \frac 5{36} & \frac 4{36} & \frac 3{36} & \frac 2{36} & \frac 1{36} \end{array}$$

\bigskip

In {\bf\em functional form}\label{pmffunctionalform}:
$$p_X(x) = P(X=x) = \frac {6 - |x-7|}{36}\quad\mbox{for }x=2,3,4,\dots,11,12.$$


%\vskip .5 in



\vskip .5 in

\noindent Compute each of the following:\\

\noindent  $P(\underbrace{3\le X\le 5}_{\tiny 3,4,5}) = P(X=3)+P(X=4)+P(X=5)=\frac 9{36}=\frac 14.$\\ \bigskip

\noindent  $P(\underbrace{2.5\le X\le 5.5}_{\tiny 3,4,5}) = \frac 14.$\\ \bigskip

\noindent  $P(\underbrace{3<X<5}_{\tiny 4})=\frac 4{36}=\frac 19.$\\ \bigskip

\noindent  $P(\underbrace{5X-X^2 \ge 4}_{\tiny 2,3,4})=P(X=2)+P(X=3)+P(X=4)=\frac 6{36}=\frac 16.$

\newpage



\noindent Random variables are extremely helpful in modeling common experiments that come up all the time in practice.
In fact, some of these experiments are so common that the pmfs and their associated rvs are given special names.\\

\noindent {\bf Some named pmfs (discrete probability distributions) we will study:}\\
The Bernoulli\\
The hypergeometric\\
The binomial\\
The Poisson\\
The geometric\\
The negative binomial\\
The discrete uniform\\
The multivariate hypergeometric\\
The multinomial\\

\bigskip

\noindent The term {\bf\em distribution} is used here to mean the
way the probability masses are distributed among its possible values.

\vskip .5 in

\noindent {\bf 1. The Bernoulli$(p)$ distribution}.\label{d:bernoullip}\\
This is the probability (mass) distribution of a discrete random variable $X$ that only takes two values 0 and 1, and
$$P(X=1) = p\quad\mbox{and}\quad P(X=0)=1-p.$$

\noindent {\bf Notation:} We use the notation
$$X\sim \mbox{Bernoulli}(p)$$
to mean the random variable $X$ has the Bernoulli$(p)$ distribution shown above.\\

\noindent We can think about the Bernoulli$(p)$ random variable this way:

We have a collection of objects of which a proportion $p$ are
labeled {\bf\em successes} (the remaining proportion $1-p$ are labeled {\bf\em failures}).
From this collection we randomly draw one object and set $X=1$
if we draw a success, and set $X=0$ if we draw a failure. This is
the simplest random variable, and
as such it is often used as a building block in modeling more
complicated experiments\dots most commonly, experiments that are
conducted in (possibly many) trials where on each trial we have a
Bernoulli$(p)$ random variable.\\





\newpage


\noindent Here's an experiment we can model using Bernoulli$(p)$ building blocks:\\

\noindent {\bf 2. The hypergeometric distribution.}\label{d:hypergeom}\\
Suppose we have a {\bf\em finite} collection of distinct objects, say $N$ total, of which $M$ are labeled successes (and, $N-M$ are failures).
You can imagine a bag of $N=100$ m\&m candies of which $M=20$ are colored red (and $N-M=80$ are not red).  We draw $n$ objects from this
collection one-at-a-time {\bf\em without replacing} them after they are drawn. Obviously, $n\le N$ else there is nothing left to draw.\\


\noindent In this experiment, we are interested in the random variable:

$X=$ the number of successes we draw in the $n$ trials.\\

\noindent It should be clear that the set of possible values $x$ of $X$ must be between 0 and $n$ inclusive, but also must satisfy
$$x\le M\quad\mbox{\bf\em and}\quad n-x\le N-M,$$
since the number of successes cannot exceed the total number of successes in the collection
and the number of failures cannot exceed the total number of failures in the collection.\\

\noindent Therefore, for $x=0,1,2,\dots,n$, $x\le M$ and $n-x\le N-M$, the event $(X=x)$ doesn't depend on the order in which that $n$ objects were selected, and
since each selection of $n$ objects is equally-likely, we have the {\bf\em pmf of the hypergeometric}:
$$P(X=x) = \dfrac {{M\choose x}{N-M\choose n-x}}{{N\choose n}}.$$

\vskip 1 in

\noindent Once we know the pmf, recall from page \pageref{pmfcalc} computing probability is straightforward.\\

\noindent {\bf Example.}\\
In a bag of 100 m\&m's of which 20 are red, what's the probability that, in a randomly chosen handful of 5 candies, you have 1 or 2 red m\&m's?  At least one red m\&m?\\

Let $X$ count the number of red m\&m's. Then
\begin{eqnarray*}
P(1\le X\le 2) &=& P(X=1)+P(X=2) = \frac{{20\choose 1}{80\choose 4}+{20\choose 2}{80\choose 3}}{{100\choose 5}}\approx 0.6275.\end{eqnarray*}

\begin{eqnarray*}
P(X\ge 1) &=& 1-P(X=0) = 1 - \frac {{20\choose 0}{80\choose 5}}{{100\choose 5}} \approx 1 - 0.3193 = 0.6807.\end{eqnarray*}

%\noindent (separate question) What's the probability the first m\&m you draw is red?\\

%\vskip .75 in

%\noindent (separate question) What's the probability the second m\&m you draw is red?\\


\newpage

\noindent In the hypergeometric experiment, i.e., sampling without replacement from a finite population of successes and failures, the underlying Bernoulli rvs were
{\bf\em dependent} because knowledge of what is drawn on a trial (success or failure) influences the probability on other trials.  We now consider a similar experiment but where it happens that the Bernoulli sequence will be {\bf\em independent}.\\


\noindent {\bf 3. The binomial$(n,p)$ distribution.}\label{d:binomialnp}\\

\noindent Consider a sequence of $n$ independent Bernoulli$(p)$ trials, i.e., the result of each trial is either a 1 (success) or 0 (failure), and knowledge of which occurred will not influence the probabilities on other trials.  We are interested in the rv

$X =$ number of successes in $n$ trials.\\

\noindent The {\bf\em pmf of binomial$(n,p)$} is
$$P(X=x) = {n\choose x}p^x(1-p)^{n-x}\quad\mbox{for }x=0,1,2,\dots,n.$$

\noindent (Why?)\\

\noindent Represent the sample space of this experiment as the set of all $n$-tuples of 0's and 1's:
$$\Omega = \big\{(x_1,x_2,\dots,x_n):\,\mbox{each }x_i\in\{0,1\}\big\},$$
where a 1 in the $i$th entry means a success on trial $i$, a 0 in the $i$th entry means a failure in trial $i$.
The event $(X=x) = \big\{\omega=(x_1,x_2,\dots,x_n):\,\mbox{each }x_i\in\{0,1\},\ \sum_{i=1}^nx_i=x\big\}$,
i.e., it is the subset of $n$-tuples having $x$ successes (1's) and $n-x$ failures (0's). Notice that
$$|(X=x)| = {n\choose x} = \frac {n!}{x!(n-x)!},$$
since $(X=x)$ is all the anagrams of the $n$-letter ``word" comprised of $x$ 1's and $n-x$ 0's. Moreover,
each $\omega=(X=x)$ has the {\em same} probability $p^x(1-p)^{n-x}$ since the results on each trial are independent. It then follows that
$$P(X=x) = \sum_{\omega\in(X=x)}p^x(1-p)^{n-x} = {n\choose x}p^x(1-p)^{n-x}\quad\mbox{for }x=0,1,\dots,n.$$

\bigskip

\noindent {\bf Remark.}\label{binomial=largeNhypergeometric}\\
Suppose we have an infinite population of successes and failures with a proportion $p$ of successes.
A binomial$(n,p)$ experiment can be thought of as sampling $n$ objects from this population without replacement.  Sampling without replacement in this population will lead to {\bf\em independent} Bernoulli$(p)$ trials. This is because knowledge of what was drawn will now {\bf\em not} change the proportion of successes on later trials.\\

\noindent Another way to think about the binomial$(n,p)$ experiment is having a {\bf\em finite} collection of successes and failures but sample {\bf\em with replacement} instead.  Then the Bernoulli$(p)$
trials will be independent here because knowledge of what type was drawn will not change the proportion of successes when we randomly draw on other trials (because we replace what was drawn).\\


\newpage


\noindent {\bf Some interesting binomial experiments/rvs.}\\

\noindent $\bullet$ Flip a fair coin $n$ times. $X$ counts number of heads.
$X\sim \mbox{binom}(n,\frac 12)$.

This is the prototypical binomial experiment.\medskip

\noindent $\bullet$ Roll a fair 6-sided die 5 times. $X$ is the number of times you roll a 1 or 2.

$X\sim \mbox{binom}(5,\frac 13)$.\medskip

\noindent $\bullet$ 90\% of the eggs a hen lays are grade-A.  The hen lays a batch of 20 eggs.

$X$ is the number of grade-A eggs in the batch.  $X\sim \mbox{binom}(20,.9)$.\medskip



\noindent $\bullet$ You walk into a classroom with 50 other people, let $X$ be the number of these people

having your birthday. $X\sim \mbox{binom}(50,\frac 1{365})$.\medskip


\noindent $\bullet$ Every Wednesday for a year you buy a scratch-off lottery ticket that claims there's a

5\% chance you'll win a prize of \$50 or more on each ticket.  Let $X$ be the number of

times you win \$50 or more in the year.  $X\sim \mbox{binom}(52,.05)$.\\




\noindent {\bf Thought exercise.}\\
In the above examples you should really try to understand either why the conditions of a binomial are plausible or what the assumptions of the binomial mean in the context of the example, what the identical ``trials" are, what a ``success" is, what the
probability of success is on each trial.\medskip

\noindent For instance, in the hen example, the ``trials" are each of the 20 eggs -- they can be grade-A (success) or {\em not} grade-A (failure), the chance the hen lays a grade-A egg (i.e., probability of success on a trial) is 0.9, we're assuming that the quality of an egg in the batch does not influence the quality of any other egg in the batch, i.e., the quality from egg to egg are independent.\\

\bigskip

\noindent {\bf Examples.}\\
\noindent Flip a fair coin $100$ times.  Compute the probability of exactly 50 heads. Of between 35 and 65 (inclusive) heads.

$P(X=50) = {100\choose 50}\left(\frac 12\right)^{100} \approx 0.08.$

$P(35\le X\le 65) = \sum_{x=35}^{65}{100\choose x}\left(\frac 12\right)^{100} \approx 0.99821$ using Microsoft Excel$^{\copyright}$.\medskip


\noindent Roll a fair 6-sided die 5 times. Compute the probability a 1 or 2 occurs on exactly 3 dice.

$P(X=3) = {5\choose 3}\left(\frac 13\right)^3\left(\frac 23\right)^2 = 10\cdot \frac 1{27}\cdot \frac 4{9} = \frac {40}{243}\approx 0.1646$.\medskip

\noindent 90\% of the eggs a hen lays are grade-A.  The hen lays a batch of 20 eggs. Compute the probability there is at least 19 grade-A eggs.

$P(X\ge 19) = {20\choose 19}(.9)^{19}(.1)^1 + {20\choose 20}(.9)^{20}(.1)^0 \approx 0.3917$.\medskip


\noindent You walk into a classroom with 50 other people. Compute the probability at least 1 person has your birthday.

$P(X\ge 1) =1-P(X=0) = 1 - {50\choose 0}\left(\frac 1{365}\right)^0\left(\frac {364}{365}\right)^{50} = 1 - \left(\frac {364}{365}\right)^{50}\approx  0.128$.\medskip


\noindent You buy a scratch-off lottery ticket with a 5\% win probability every Wednesday for a year. Compute the probability you lose on every ticket.

$P(X=0) = {52\choose 0}(.05)^0(.95)^{52} = .95^{52} \approx 0.069$.










\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{center}{\bf Lecture 10.}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\noindent {\bf\em The binom$(n,p)$ distribution.}\\
%If $X\sim \mbox{binom}(n,p)$, then
%$$P(X=x) = {n\choose x}p^x(1-p)^{n-x}\quad \mbox{for }x=0,1,\dots,n.$$

\includegraphics*[0,0][600,320]{binomial-pmfs.jpg}
\begin{center}{\bf Figure.} Some plots of binomial pmfs.\end{center}

%\bigskip
%\bigskip

%\noindent {\bf Example 1.}\\
%Flip a coin 100 times. Compute the probability\\
%(a) you flip exactly 50 heads.\\
%(b) you flip between 35 and 65 (inclusive) heads. {\em Use a computer.}\\

%\noindent If $X$ counts the number of heads, then $X\sim \mbox{binom}(100,\frac 12)$ so pmf is $P(X=x) = \dfrac {{100\choose x}}{2^{100}}$,

%\noindent (a) $P(X=50) = \dfrac {{100\choose 50}}{2^{100}} \approx .08$.\\

%\noindent (b) $\displaystyle P(35\le X\le 65) = \sum_{x=35}^{65}\frac {{100\choose x}}{2^{100}} \approx .99821\dots.$\\


%\noindent {\bf Example 1.}\\
%A box has 10 chip number 0 thru 9. Each morning you randomly select a chip, note the number and put it back.
%You do this for 4 mornings. Compute the probability that over these 4 mornings you select a number less than or equal to 2 exactly three times.\medskip
%
%\noindent If we call drawing a number 2 or less a ``success", the probability of a success is 0.3.
%Let $X$ be the number of mornings you draw a 2 or less. Then $X\sim \mbox{binom}(4,0.3)$
%and
%
%$$P(X=3) = {4\choose 3}(0.3)^3(0.7)^1 = 4\times 0.0027 \times 0.7 = 0.0756.$$
%
%\bigskip
%
%\noindent {\bf Example 2.}\\
%Suppose you play the lottery twice per week for 50 years (i.e., $2\times 52\times 50 = 5200$ times) even though you know
%the probability you will win the jackpot on any ticket is $10^{-6}$ independent from ticket to ticket.
%Compute the probability you win the jackpot at least once.  At least twice.\medskip
%
%
%\noindent If $X$ counts the number
%of times you win the jackpot, then $X\sim \mbox{binom}(5200,10^{-6})$.
%\begin{eqnarray*}P(X\ge 1) &= & 1 - P(X=0) \\
%&=& 1-{5200\choose 0}(10^{-6})^0(1-10^{-6})^{5200} \\
%&=& 1-(1-10^{-6})^{5200}\approx 0.005187.\end{eqnarray*}
%
%Also,
%$$P(X\ge 2) = 1 - P(X=0) - P(X=1)\approx 0.0000135.$$

\vskip .5 in


\noindent {\bf Calculus fact: the binomial theorem.}\label{d:binomialtheorem}\\
For integer $n\ge 1$ and real constants $a$ and $b$,
$$\sum_{k=0}^{n} {n\choose k}a^kb^{n-k} = (a+b)^n.$$
Since $(a+b)^n=(b+a)^n$, we also have
$$\sum_{k=0}^{n} {n\choose k}a^kb^{n-k} = \sum_{k=0}^{n} {n\choose k}a^{n-k}b^{k} = (a+b)^n.$$

\vskip .25 in

\noindent {\bf Advice:}\\
\noindent The binomial theorem is absolutely indispensable - and will not only be used several times in this course but also the student will
need to be able to recognize that they can use this result as the situation arises. {\bf \em Know this theorem and memorize it!}\\

\noindent Here's one use of the binomial theorem:\\




\newpage




\noindent {\bf The binomial$(n,p)$ pmf {\em is} a pmf:}\\
\noindent Let $0<p<1$.  By taking $a=p$ and $b=1-p$ in the binomial theorem we obtain
$$\sum_{x=0}^n {n\choose x} p^x(1-p)^{n-x} = (p + 1-p)^n = 1,$$
which shows the binom$(n,p)$ probabilities sum to 1 as they should.\\

\vskip .5 in

\noindent {\bf Calculus fact: general little ``oh" notation.}\label{d:littleoh}\\
We say a given function $f(x)$ of $x$ {\bf\em is o$(g(x))$ as $x\to c$} (pronounced ``is little oh of $g(x)$ as $x$ goes to $c$") if
$$\lim_{x\to c}\dfrac {f(x)}{g(x)}=0.$$

\vskip .5 in

\noindent The value $c$ is allowed to be $+\infty$ or $-\infty$ as well.  The little oh condition says that as $x$ approaches $c$, the magnitude of the
function $g(x)$ increases more rapidly than the magnitude of the function $f(x)$ in such a way that the ratio $f(x)/g(x)$ tends to 0.
From the definition it should be clear that if a function $h(x)$ is $o(1)$ as $x\to c$, then $\displaystyle \lim_{x\to c}h(x) = 0$.\\

\vskip .5 in

\noindent {\bf Exercise.} \\
Please verify for yourself that

$\frac 1{x^{3/2}}=o(1)\ \mbox{as}\ x\rightarrow \infty$,

$\frac 1{x^{3/2}}=o(\frac 1x)\ \mbox{as}\ x\rightarrow \infty$,

$\frac 1{x\ln(x)}=o(\frac 1x)\ \mbox{as}\ x\rightarrow \infty$,

$x^5=o(x^{6})\ \mbox{as}\ x\rightarrow \infty$, and

$x^2-1=o(x+1)$ as $x\to 1$.\\

\bigskip

\noindent I'll do the first two:

Certainly $\dfrac {\frac 1{x^{3/2}}}{1} = \dfrac 1{x^{3/2}}\to 0$ as $x\to \infty$. Therefore, $\frac 1{x^{3/2}}=o(1)$ as $x\to \infty$.

Since $\dfrac {\frac 1{x^{3/2}}}{\frac 1x}=\dfrac 1{x^{1/2}}\rightarrow 0$ as $x\rightarrow \infty$, $x^{-3/2}=o(1/x)$ as $x\to \infty$.

However, notice it is not $o(\frac 1{x^2})$ as $x\to \infty$, nor is it $o(\frac 1x)$ as $x\to {\bf 0}$.





\newpage


\noindent {\bf\em Stirling's approximation}\label{stirling}:\\
$$m! = \sqrt{2\pi}\,m^{m+\frac 12}e^{-m}(1+o(1))\quad \mbox{as }m\to \infty,$$
or, equivalently,
$$\lim_{m\to\infty} \frac {m!}{\sqrt{2\pi}\,m^{m+\frac 12}e^{-m}}=\lim_{m\to\infty} (1+o(1)) = 1.$$
This result says the relative error in approximating $m!$ by $\sqrt{2\pi}m^{m+\frac 12}e^{m}$ goes to 0 as $m$ tends to $\infty$:
$$\frac {m! - \sqrt{2\pi}\,m^{m+\frac 12}e^{-m}}{\sqrt{2\pi}\,m^{m+\frac 12}e^{-m}}= o(1)\quad \mbox{as }m\to\infty.$$

\vskip .5 in

\noindent {\bf Example.}\\
\noindent Find the relative error in approximating $10!$ by Stirling's approximation. \\

Since $10! = 3628800$ and $\sqrt{2\pi}10^{10.5}e^{-10} = 3598695.619\dots,$ we find that

$$\dfrac {10!-\sqrt{2\pi}10^{10.5}e^{-10}}{\sqrt{2\pi}10^{10.5}e^{-10}} = 0.00836535\dots .$$


\vskip .5 in

\noindent {\bf Exercise.}\\
Show that the Stirling's approximation to ${N\choose n}$ is
$$\frac {(N-n)^n \left( 1-\frac nN \right)^{-1/2}}{n!}(1+o(1)).$$

\vskip .5 in

\noindent {\bf Challenging exercise.}\\
The remark on page \pageref{binomial=largeNhypergeometric}
should lead us to believe
that if a finite population is
{\bf\em very} large, then the hypergeometric and binomial distributions should be close.
Fix integers $x$ and $n$ such that $0\le x\le n$.
Suppose the population size $N$ and the number of successes $M$ has the property that
$$\lim_{N\to\infty}\frac MN = p.$$
Show that
$$\lim_{N\to\infty} \frac {{M\choose x}{N-M\choose n-x}}{{N\choose n}} = {n\choose x}p^x(1-p)^{n-x}.$$

\medskip

\noindent Hint:
Use the exercise above on each binomial coefficient, then pass to a limit as $N\to\infty$
keeping in mind that $M/N\to p$, $(N-M)/N\to 1-p$ as $N\to \infty$, and some terms approach 1 as $N\to \infty$.
This is an exercise in good book-keeping.



\newpage



\noindent The next discrete distribution we discuss is the Poisson$(\lambda)$\label{poissoncalcfacts} distribution, but we first need to recall a couple of calculus facts.\\

\noindent {\bf Calculus fact: MacLaurin series for $e^u$. }\label{d:maclaurinseries}\\
For any (real) $u$,
$$e^u = \sum_{k=0}^{\infty} \frac {u^k}{k!} = 1 + u + \frac {u^2}{2!} + \frac {u^3}{3!} + \frac {u^4}{4!} + \cdots.$$

\vskip .5 in

\noindent {\bf Calculus fact: limit representations for $e^u$.}\label{d:limitrep}\\
For any (real) $u$,
$$e^u = \lim_{n\to\infty}\left( 1 + \frac un\right)^n = \lim_{n\to\infty}\left( 1 + \frac un + o(\frac 1n)\right)^n.$$
%where $o(g(n))$ as $n\rightarrow\infty$ is any function of $n$ with the property that $\displaystyle \lim_{n\to\infty}\dfrac {o(g(n))}{g(n)}=0$.\\

\vskip 1 in




\noindent {\bf 4. The Poisson$(\lambda)$ distribution.}\label{d:poissonlambda}\\

\noindent This is the distribution of a discrete rv $X$ that counts the number of ``events" that happen in a fixed amount of exposure (e.g., amount of time, or amount of space) that
roughly satisfy the following criteria:\label{poissonassumptions}\medskip

\noindent {\bf\em randomness}:

the ``events" occur randomly/independently throughout the exposure.

\noindent {\bf\em constancy}:

the ``events" happen at a constant rate $\lambda>0$ throughout the exposure.

\noindent {\bf\em no-clumping}:

the chance of two or more ``events" happening at the same point is negligible.\medskip

\noindent In this case we write $X\sim \mbox{Poisson}(\lambda)$. We'll show that the {\bf\em pmf of the Poisson$(\lambda)$} is
\begin{equation*}P(X=x) = \dfrac {e^{-\lambda} \lambda^x}{x!}\quad\mbox{for }x=0,1,2,3,\dots .\label{poissonpmf}\end{equation*}



\vskip .5 in



\noindent To understand the ``no-clumping" criterion\label{noclumping},
represent the unit of exposure as the unit interval $[0,1]$. For $t\in [0,1]$, let $X(t)$ be the number of events in $(0,t]$, $X(0)=0$.
Then $X(t)-X(t-\frac 1n)$ represents the number of events in $(t-\frac 1n,t]$.  Now, rigorously, we understand
``no-clumping" means: for each $t$, $P(X(t)-X(t-\frac 1n)\ge 2) = o(1)$ as $n\to\infty$.\\






\newpage





\noindent {\bf Remark and discussion.}\\
The following random variables can be modeled well by a Poisson$(\lambda)$ distribution.\\
$\bullet$ number of babies born in a day at a given hospital (say, $\lambda=10$ babies/day).\\
$\bullet$ number of category-5 hurricanes making landfall in a year (say, $\lambda=0.4$ hurricanes/year).\\
$\bullet$ number of defects in a square yard of cloth (say, $\lambda=2$ defects/yard$^2$).\\

\noindent Consider the example of babies being born in a hospital.  The Poisson ``events" that we are counting
are births of babies.  In any given day, say starting from the stroke of midnight,
as time continually moves we will see a first event: the first baby is born.  Then a second baby is born at some time {\em after} the first, and so on.\\

\noindent The randomness assumption says the birth of a baby will not influence the probability of another birth - and this assumption seems plausible in this example. \\

\noindent The constancy assumption says the rate that babies are born in the day is constant throughout the day, i.e., that the Poisson events should not occur
at, say, higher rates in certain subintervals of time than others within the day.  This also seems plausible.  An example where we might expect that constancy is not satisfied is
the case of cars passing through a particular intersection in a day. In this case, one might suspect that the volume of cars is higher say in the mid-morning and late afternoon traffic and have rather light volume in the early morning hours.\\

\noindent The no-clumping assumption for baby births might be challenged, but even twins are not born simultaneously and this appears somewhat plausible.\\

\vskip .5 in

\noindent {\bf Example.}\\
Show that $P(X=x) = \dfrac {e^{-\lambda} \lambda^x}{x!}\quad\mbox{for }x=0,1,2,3,\dots$ is, indeed, a pmf.\\



Since $\lambda >0$, $\dfrac {e^{-\lambda}\lambda^x}{x!}>0$ for all $x\in {\mathbb N}$, and

$$\sum_{x=0}^{\infty} \dfrac {e^{-\lambda}\lambda^x}{x!} = e^{-\lambda}\sum_{x=0}^{\infty}\dfrac {\lambda^x}{x!} = e^{-\lambda}e^{\lambda}=1,$$
\vskip .05 in
shows the masses sum to 1, so {\em is} a pmf.  Notice in the second equality we recognize

the MacLaurin expansion of $e^{\lambda}$.



\newpage





\noindent {\bf How does the Poisson$(\lambda)$ pmf come about?}\label{poisson=limitofbinomial}\\

%\noindent The Poisson$(\lambda)$ distribution comes about as the large $n$ limit of a binom$(n,\frac {\lambda}n)$ distribution.
\noindent Let's consider the example of babies being born
in a hospital on a given day. We represent the exposure
(1 day) as the unit interval $0\le x\le 1$.  This a continuous interval starting at time 0 (the beginning of the day) and ending at time 1 (the end of the day).
We should envision babies being born in this interval at a constant rate but entirely at random -- meaning that a baby being born at one instant will not influence
the probability that a baby is born at another instant -- and that two or more babies cannot be born at the same instant.  The random variable $X$ will count
the number of babies born in the interval $[0,1]$.\\

\noindent The idea is to ``chop up" the unit interval into a large number of subintervals of equal length,
say $n$ pieces of length $\frac 1n$ each, and count the number of babies born
in each subinterval, say $X_j$ counts the number of babies born in the interval $(\frac {j-1}n,\frac jn]$. See figure.

\includegraphics*[-120,0][200,60]{poisson-interval.jpg}


\noindent According to our randomness assumption, $X_1,X_2,\dots,X_n$ should be independent rvs. Since each subinterval is the same length, the constancy assumption implies
that each $X_i$ should have the same probability distribution. Now, if $n$ is {\em really} large, then because of the no-clumping assumption, for any $j$, $P(X_j\ge 2)\approx 0$.
This means for large $n$, each $X_j$ is (approximately) Bernoulli
(since each $X_j$ can only take the values 0 and 1, neglecting the case of 2 or more). Lastly, since $\lambda$
represents the expected number of babies born in the entire interval $[0,1]$, the constancy assumption says we should expect $\frac {\lambda}n$ babies in each subinterval.
Putting this all together we have, for each $n$, the sequence $X_1,X_2,\dots,X_n$ are independent Bernoulli$(\frac {\lambda}n)$ rvs and their sum
$$S = S_n = X_1+X_2+\cdots + X_n \sim \mbox{binom}(n,\frac {\lambda}n).$$

\noindent Letting $n$ tend to infinity, we should get our Poisson$(\lambda)$.  Let's see:

\noindent Fix $x\in {\mathbb N}$, take $n>x$ with the goal that $n\to \infty$ eventually.
\begin{eqnarray*}
P(S_n = x)
&=& \frac {n!}{x!(n-x)!}\left(\frac {\lambda}n\right)^x\left(1 - \frac {\lambda}n\right)^{n-x}\\
&=& \frac {n(n-1)(n-2)\cdots (n-(x-1))}{x!}\left(\frac {\lambda^x}{n^x}\right)\left(1 - \frac {\lambda}n\right)^{n}\left(1 - \frac {\lambda}n\right)^{-x}\\
&=& \frac {\lambda^x}{x!}\cdot \frac {n}{n}\cdot \frac {(n-1)}{n}\cdot \frac {(n-2)}{n}\cdots \frac {(n-(x-1))}{n}\left(1 - \frac {\lambda}n\right)^{n}\left(1 - \frac {\lambda}n\right)^{-x}\\
&=& \frac {\lambda^x}{x!}\cdot \underbrace{\left(1-\frac 1n\right)\left(1-\frac 2n\right)\cdots \left(1-\frac {x-1}n\right)}_{\rightarrow 1\mbox{ \tiny as }n\to\infty\mbox{ \tiny since $x$ is fixed}}
\underbrace{\left(1 - \frac {\lambda}n\right)^{n}}_{\rightarrow e^{-\lambda}\mbox{ \tiny as }n\to\infty}\underbrace{\left(1 - \frac {\lambda}n\right)^{-x}}_{\rightarrow 1\mbox{ \tiny as }n\to\infty}\\
&\longrightarrow & \frac {e^{-\lambda}\lambda^x}{x!}\quad \mbox{ as }n\to \infty.
\end{eqnarray*}



\newpage

\noindent {\bf Example.}\\
Suppose the number $X$ of category-5 hurricanes that make landfall in a year has a Poisson$(0.4)$ distribution.  What is the probability that at least one category-5 hurricane makes landfall next year?  How about the next 5 years?\\

In the next year, we expect $\lambda = 0.4$, so
$$P(X\ge 1) = 1 - P(X=0) = 1 - e^{-0.4}\frac {(0.4)^0}{0!} = 1-e^{-0.4} \approx 0.3297.$$

In the next 5 years, we expect  $\lambda = 5(0.4)=2$ hurricanes (notice the change in

exposure affected the value of $\lambda$ chosen).

$P(X\ge 1) = 1 - P(X=0) = 1 - e^{-2} \approx 0.8647.$\\

\vskip .5 in

\noindent {\bf Remark.}\\
Page \pageref{poisson=limitofbinomial} gives a nice connection between the Poisson and binomial distributions: the
Poisson$(\lambda)$ distribution can be viewed as a limit as $n\to\infty$ of the binomial$(n,\frac {\lambda}n)$ and
we should expect the Poisson$(\lambda)$ pmf to be close to the binomial$(n,\frac {\lambda}n)$ pmf for large $n$; i.e.,
the rv counts the number of successes in a large number of independent Bernoulli trials where
successes are ``{\em rare}" is {\em approximately} Poisson.\\





\newpage




\noindent {\bf 5. The geometric$(p)$ distribution.}\label{d:geometricp}\\

\noindent Suppose $X$ is the discrete rv that returns the {\bf \em trial of the first success} in a sequence of independent
Bernoulli$(p)$ trials.  Then, we write $X\sim\mbox{geometric}(p)$ and
$$p(x) = P(X=x) = p(1-p)^{x-1}\quad \mbox{for }x=1,2,3,\dots.$$

\noindent (Why?)\\

\noindent Certainly, $P(X=1)=p$ since the event $(X=1)$ is the same as the event that we obtain a success in one trial; and,
the event $(X=2)$ is the same as the event that our first success follows a single failure, and the independence of the trials implies
$P(X=2) = (1-p)p$. More generally, if $x>1$, then the event $(X=x)$ is the event that the first success follows $x-1$ failures, and
independence of the trials implies
$$P(X=x) = \underbrace{(1-p)(1-p)\cdots (1-p)}_{x-1\ \mbox{\tiny factors}}p = p(1-p)^{x-1}.$$

\vskip .5 in



\noindent {\bf Remark.}\label{r:altdefgeom}\\
An alternative way of defining a geometric$(p)$ distribution is to have the rv count the numbers of failures before the first success. In this way,
this interpretation of the geometric rv, say $Y$, would take values $0,1,2,\dots$ and
$$P(Y=y) = p(1-p)^y\quad\mbox{for }y=0,1,2,\dots$$
would be the corresponding pmf.  Either of these distributions are geometric$(p)$ distributions, we can choose which to use from the context of the problem.
Of course, they are related by $X=1+Y$.\\

\vskip .5 in

\noindent {\bf Interesting examples of the geometric distribution.}\\

\noindent $\bullet$ Roll a pair of fair dice and stop when you see a {\em double six} for the first time. The trial $X$ where you stop rolling
is a geometric$(\frac 1{36})$.\\

\noindent $\bullet$ You are interviewing people one at a time and we label the people 1, 2, 3, and so on.  The first person $X$ to have your birthday has a geometric$(\frac 1{365})$ distribution.\\

\noindent $\bullet$ Fred has a 60\% chance of hitting the bulls-eye on any given dart throw (when he's aiming for it). The number of dart throws Fred needs to hit the bulls-eye
for the first time has a geometric$(0.6)$ distribution.



\newpage



\noindent {\bf Calculus fact: Sums of a geometric series.}\label{d:geomseries}\\
For any constants $A$ and $r$, the sequence
$A, Ar, Ar^2, Ar^3,\dots$
is called a {\bf\em geometric progression}\label{d:geomprog}. The value $r$ is called the {\bf\em geometric ratio}.\label{d:geomratio}
%When $r\ne 1$ and $m\le n$ the {\bf\em sum of a geometric progression} is
%$$\sum_{k=m}^{n} Ar^k = Ar^{m} + Ar^{m+1} + Ar^{m+2} + \cdots + Ar^{n} = \frac {Ar^m - Ar^{n+1}}{1-r}.$$
When $|r|< 1$ the {\bf\em sum of the geometric series} is
$$\sum_{k=m}^{\infty}Ar^k = \frac {Ar^m}{1-r}.$$


\vskip .5 in



\noindent {\bf Advice:}\\
Become familiar with recognizing when the terms of a sum/series form a geometric progression, and {\bf\em memorize} the formulas for the sums.
A good mnemonic for remembering the formula for a geometric series is ``it's the first term in the series divided by 1 minus the geometric ratio".

\vskip .5 in


\noindent {\bf Example.}\label{geomsumsto1}\\
For $0<p<1$, show that $P(X=x)=p(1-p)^{x-1}$ for $x=1,2,3,\dots$ is a pmf.\\

Clearly, $p(1-p)^{x-1}>0$ for all $x\ge 1$. Moreover, by the calculus fact above
$$\sum_{x=1}^{\infty}p(1-p)^{x-1} = \frac {p(1-p)^{1-1}}{1-(1-p)}=1.$$

\vskip .5 in

\noindent {\bf Examples.}\\
You plan to roll a pair of 6-sided dice until you get {\em double sixes}. What's the probability you succeed by the 3rd roll?

$P(X\le 3) = \frac 1{36} + \frac 1{36}\left(\frac {35}{36}\right) + \frac 1{36}\cdot \left(\frac {35}{36}\right)^2 \approx 0.08104.$\\

\noindent You are interviewing people one at a time. What's the probability that in the first 400 interviews no one had your birthday?
This means that the first person to have your birthday is {\em after} 400.

$P(X>400) = \sum_{x=401}^{\infty} \frac 1{365}\left(\frac {364}{365}\right)^{x-1} = \frac {\frac 1{365}\left(\frac {364}{365}\right)^{400}}{1-\left(1 - \frac {1}{365}\right)} = \left(\frac {364}{365}\right)^{400}\approx 0.33374.$\medskip


\noindent What's the probability that the first person to have your birthday was interviewed between person 100 and person 200 (inclusive)?

$P(100\le X\le 200) = P(X>99) - P(X > 200) = \left(\frac {364}{365}\right)^{99}-\left(\frac {364}{365}\right)^{200}\approx 0.18445.$\\

\noindent Fred has a 60\% chance of hitting the bulls-eye on any given dart throw (when he's aiming for it).  Compute the probability that Fred's first bulls-eye happens on the 3rd throw.

$P(X=3) = 0.6(0.4)^2 = 0.096$.




\newpage

\noindent This next distribution generalizes the geometric$(p)$ distribution.\\


\noindent {\bf 6. The negative binomial($r,p$) distribution.}\label{d:negbinom}\\

\noindent If $X$ represents the {\bf\em trial of the $r$th success} in a sequence of
independent Bernoulli$(p)$ random variables, then $X$ is said to have a
negative binomial$(r,p)$ distribution, which we write as $X\sim \mbox{neg.binom}(r,p)$, and the pmf of $X$ is
$$p(x):= P(X=x) = {x-1\choose r-1}p^r(1-p)^{x-r}\quad \mbox{for }x=r,r+1,r+2,\dots .$$

\bigskip
\bigskip

\noindent (Why?)\\

\noindent Fix an $r\ge 1$.  Clearly, we need at least $x=r$ trials to see the $r$th success, so $x$ needs to be at least $r$.  Now if we fix $x\ge r$, then the event
$(X=x)$ means the $r$th success happens on the trial $x$, which tacitly implies there must be $r-1$ successes in the $x-1$ previous trials. Thus, we have
$$(X=x) = \left\{\mbox{$r-1$ successes in first $x-1$ trials}\right\}\cap \left\{\mbox{success on trial $x$} \right\},$$
where the two events in the intersection are independent. Consequently,
$$P(X=x) = \underbrace{P(\mbox{$r-1$ successes in first $x-1$ trials})}_{={x-1\choose r-1}p^{r-1}(1-p)^{x-1-(r-1)}}\times \underbrace{P(\mbox{success on trial $x$})}_{=p}.$$

\vskip .5 in

\noindent {\bf Remark.}\\
Notice that when $r=1$, this is just the geometric$(p)$ distribution.\\

\noindent Just as in the remark on page \pageref{r:altdefgeom} there is an alternate way of defining a negative binomial rv. Define $Y=X-r$ to be the number of failures
before the $r$th success. In this way, this rv $Y$ would take values $0,1,2,\dots$ and the resulting pmf would be
$$P(Y=y) = {y+r-1\choose y}p^r(1-p)^{y}\quad \mbox{for }y=0,1,2,\dots .$$
Which form of the negative binomial we want to choose should be clear from context, but either $X$ (the trial of the $r$th success) or $Y$ (the number of failures before the
$r$th success in independent Bernoulli$(p)$ trials) are neg.binom$(r,p)$ rvs.



\newpage





\hskip -.25in
\includegraphics*[0,0][800,200]{negbinom-pmfs.jpg}

\begin{center}{\bf Figure. } Some plots of negative binomial pmfs.\end{center}

\vskip .5 in


\noindent {\bf Example.} We roll a fair (balanced) 6-sided die repeatedly.  Find the probability that the 4th `6' occurs at the 10th roll.\\

Here, if $X$ is the roll on which the 4th `6' appears, then $X\sim \mbox{neg.binom}(4,\frac 16)$.

$P(X=10) = {9\choose 3}\left(\frac 16\right)^4\left(\frac 56\right)^6 \approx 0.0217 .$\\

\vskip .5 in

\noindent {\bf Example.}\\
What's the probability that see your third 6 by the 5th roll?\\

If $X$ is the roll of the third 6, then $X\sim\mbox{neg.binom}(3,\frac 16)$ and
\begin{eqnarray*}
P(\underbrace{X\le 5}_{3,4,5})
&=& P(X=3)+P(X=4)+P(X=5) \\
&=& {2\choose 2}\left(\frac 16\right)^3\left(\frac 56\right)^0 +
{3\choose 2}\left(\frac 16\right)^3\left(\frac 56\right)^1 +
{4\choose 2}\left(\frac 16\right)^3\left(\frac 56\right)^2 \\
& &\\
&\approx & 0.0355.\end{eqnarray*}


\newpage


\noindent \mbox{\bf Challenging exercise.}\\
Show that the $p(x)$ above is a pmf; In particular, use mathematical induction to prove
$$\displaystyle \sum_{x=r}^{\infty}{x-1\choose r-1}p^r(1-p)^{x-r} = 1.$$


\vskip .5 in

\noindent When $r=1$ this is just the geometric$(p)$ which we already showed sums to 1 (see page \pageref{geomsumsto1}).
I'll now do the case $r=2$.
I'll need the following {\bf \em combinatorial identity}:

\noindent For integer $n>1$ and $0<r\le n$,
$${n\choose r} = {r-1\choose r-1} + {r\choose r-1} + {r+1\choose r-1} + \cdots + {n-2\choose r-1} + {n-1\choose r-1} = \sum_{j=r-1}^{n-1}{j\choose r-1}.$$
Try to construct a proof by mathematical induction for this identity by appealing to the {\bf\em Pascal identity}\label{pascalid2} repeatedly on ${n\choose r}$.\\

\noindent This identity allows us to represent, for instance,
\begin{eqnarray*}
{6\choose 3} &=& {2\choose 2}+{3\choose 2}+{4\choose 2} + {5\choose 2} \\
& & \\
&=& 1 + 3 + 6 + 10 = 20,\end{eqnarray*}
and
\begin{eqnarray*}
{n+1\choose 2} &=& {1\choose 1}+{2\choose 1} + {3\choose 1} + \cdots + {n\choose 1}\\
& & \\
&=& 1 + 2 +3 + \cdots + n = \frac {n(n+1)}{2}.\end{eqnarray*}

\bigskip

\noindent To show that
$$\sum_{x=2}^{\infty}{x-1\choose 1}p^2(1-p)^{x-2} = 1,$$
we use the above combinatorial identity to represent $\displaystyle {x-1\choose 1}=\sum_{j=0}^{x-2}{j\choose 0} = \sum_{j=0}^{x-2}1$.






\newpage




\noindent Therefore,

\begin{eqnarray*}
\sum_{x=2}^{\infty}{x-1\choose 1}p^2(1-p)^{x-2}
&=& \sum_{x=2}^{\infty}\sum_{j=0}^{x-2}p^2(1-p)^{x-2}\\
&=& \sum_{j=0}^{\infty}\sum_{x=j+2}^{\infty}p^2(1-p)^{x-2}\\
&=& \sum_{j=0}^{\infty}\frac {p^2(1-p)^{j}}{1-(1-p)}\\
&=& \sum_{j=0}^{\infty}p(1-p)^{j} = 1,\\
\end{eqnarray*}
where, in the second equality, we changed the order of summation.
Now use mathematical induction to complete the proof for general $r\ge 1$.\\

\vskip .5 in

\noindent {\bf Analyzing the special case of a neg.binom$(2,p)$.}

\noindent In this case $X$ is the trial of the second success, which means there is a first success that happened before it. So, {\em fix} an $x\ge 2$ and
consider the event, $(X=x)$, that the second success happens at trial $x$.  Since a first success must happen before $x$, we can decompose this event as the mutually
exclusive union
$$(X=x) = \bigcup_{k=1}^{x-1}(X_1=k,X=x),$$
where $X_1$ is the trial of the first success (a geometric$(p)$ rv).
Once this first success happens at $X_1=k$, the trials performed thereafter
are independent of the trials before. So, it's just like starting anew after the first success and getting a second geometric$(p)$ rv, say $X_2$. See the figure below.  $X_2$
represents the number of trials that {\em elapsed} after the first success until the next success.



\includegraphics*[-105,0][300,60]{negbinom2p-pic2.jpg}
\vskip -.3 in
\begin{center}{\bf Figure.} A particular realization of the neg.binom$(2,p)$.\end{center}
Therefore,
$$(X=x) = \bigcup_{k=1}^{x-1}(X_1=k,X-X_1=x-k)=\bigcup_{k=1}^{x-1}(X_1=k,X_2=x-k),$$
and
\begin{eqnarray*}
P(X=x)
&=& \sum_{k=1}^{x-1} P(X_1=k,X_2=x-k) \\
&=& \sum_{k=1}^{x-1} P(X_1=k)P(X_2=x-k)\\
&=& \sum_{k=1}^{x-1} p(1-p)^{k-1}\cdot p(1-p)^{x-k-1} \\
&=& \sum_{k=1}^{x-1} p^2(1-p)^{x-2}= {x-1\choose 1}p^2(1-p)^{x-2}.\end{eqnarray*}


\vskip .5 in


\noindent {\bf Remark.}\label{sumof2geometrics}\\
The analysis done above suggests that the neg.binom$(2,p)$ is the sum of two independent geometric$(p)$ random variables.
In fact, later in this course, $X\sim \mbox{neg.binom}(r,p)$ can be written as $X=X_1+X_2+\cdots+X_r$ where
$X_1,X_2,\dots,X_r$ are {\em independent} geometric$(p)$ random variables.\\





\newpage



\noindent {\bf Expected values of discrete random variables.}\label{d:expectedvaluediscrete}\\
\noindent If $X$ is a discrete rv with pmf $p_X(x)=P(X=x)$, then we define
$$E(X) = \sum_x x\,p_X(x) = \sum_x x\,P(X=x).$$
The sum is taken over {\bf\em all} possible values of the rv $X$.  $E(X)$ is called any of the following: {\bf\em expected value of} $X$, {\bf\em expectation (value) of}\label{expectationvalue} $X$,
{\bf\em the mean (value)\label{mean} of} $X$.\\

\vskip .25 in


\noindent {\bf Advice:}\\
Try to remember $E(X)$ as a real number representing the weighted average of the values of $X$ where the weights are just the respective probability masses.\\

\vskip .25 in


\noindent {\bf Example and Discussion.}\\
Suppose $X$ has the pmf $P(X=-3)=.3,\ P(X=2)=.6,\ P(X=7)=.1$ Then
$$E(X) = -3(.3) + 2(.6) + 7(.1) = 1.$$
Statisticians usually think of a random variable as representing a (typically infinite) population of real values, and, from this viewpoint,
$E(X)$ -- often denoted $\mu$ or $\mu_X$ -- is called the {\bf\em (population) mean} or {\bf\em population average}.
Here's the intuition.
Let's first imagine that $X$ represents a {\em finite} ``population", say, of 10 objects: 3 of these objects are $-3$,\ 6 of these objects are $2$, and 1 of them is $7$.  Then, since this population is finite, our population average $\mu$ is just:
\begin{eqnarray*}
\frac {-3+-3+-3 + 2+2+2+2+2+2+7}{10} &=& -3\left(\frac 3{10}\right) + 2\left( \frac 6{10}\right) + 7\left(\frac 1{10} \right)\\
& & \\
&=& E(X) = 1.\end{eqnarray*}
We see the ``straight average" of the population is just the expected value of the random variable representing it.
This intuition extends naturally if the population had consisted of an infinite number of objects 30\% of which are $-3$, 60\% of which are 2, and 10\% of which are 7 by defining $\mu$ to be $E(X)$.  I'll mention that the expected value
does {\bf\em not} need to be a possible value of the rv as this example clearly demonstrates.\\


\noindent If we think of the pmf as how the (probability) mass is distributed to the values of the discrete rv, then the expected value can be interpreted
as the center of (probability) mass - in physics, this is called the {\bf \em first moment of inertia}.

\includegraphics*[-55,0][400,110]{see-saw.jpg}
\begin{center}{\bf Figure.} We balance the see-saw at $\mu$.\end{center}


\newpage


\noindent {\bf Example.}\\
Let's compute the {\bf\em mean of a Bernoulli}$(p)$.\label{e:bernoullipmean}\\
Recall if $X\sim \mbox{Bernoulli}(p)$, then $P(X=1)=p$ and $P(X=0)=1-p)$.  Therefore,
$$E(X) = \sum_{x\in \{0,1\}}x\,P(X=x) = 0\cdot P(X=0) + 1\cdot P(X=1) = p.$$

\vskip .5 in

\noindent {\bf Example.}\\
Let's show the {\bf\em mean of a binomial}$(n,p)$ is $np$.\label{e:binomialnpmean}\\
We will first compute $E(X)$ using the definition.\\
\begin{eqnarray*}
E(X)
&=& \sum_{x=0}^n x\,P(X=x) \\
&=& \sum_{x={\bf 1}}^n x\cdot \frac {n!}{x!(n-x)!} p^x(1-p)^{n-x}\quad(x=0\mbox{ will not contribute to }E(X))\\
&=& \sum_{x=1}^n \frac {n!}{(x-1)!(n-x)!} p^x(1-p)^{n-x} \\
&=& np\sum_{x=1}^n \underbrace {\frac {(n-1)!}{(x-1)!(n-x)!}}_{=\ {n-1\choose x-1}} p^{x-1}(1-p)^{n-1-(x-1)} \quad(\mbox{change of variable }u=x-1)\\
&=& np\underbrace{\sum_{u=0}^{n-1} {n-1\choose u}p^u(1-p)^{n-1-u}}_{=\ 1\mbox{ \tiny summing binom}(n-1,p)\mbox{ \tiny over its support}}\\
&=& np.
\end{eqnarray*}


\vskip .5 in
\noindent If you understood the calculation for the mean of a binomial$(n,p)$ above, then do this\\

\noindent {\bf Exercise.}\\
Compute the mean of a Poisson$(\lambda)$. {\bf\em Now!}\\




\newpage


\noindent {\bf Example.}\\
Let's show the {\bf\em mean of a geometric$(p)$ is} $\dfrac 1p$.\\

\noindent I'll actually do this calculation 2 ways. \\

\noindent {\bf solution \#1}:\\
\begin{equation}
E(X) =
\sum_{x=1}^{\infty} x\, p(1-p)^{x-1} = p + 2p(1-p) + 3p(1-p)^2 + 4p(1-p)^3 + \cdots.\label{e:exgeometricp1}\end{equation}
Assume this series is convergent for now;
multiply equation (\ref{e:exgeometricp1}) through by $1-p$:
\begin{equation}
(1-p)E(X) =
\sum_{x=1}^{\infty} x\, p(1-p)^{x} = p(1-p) + 2p(1-p)^2 + 3p(1-p)^3 + 4p(1-p)^5 + \cdots.\label{e:exgeometricp2}\end{equation}
Now write equations (\ref{e:exgeometricp1}) and (\ref{e:exgeometricp2}) aligned by like terms:
$$
\begin{array}{rcrrrrrrrrr}
E(X) & = &  p & + & 2p(1-p) & + & 3p(1-p)^2 & + &  4p(1-p)^3  & + & \cdots\\
(1-p)E(X) & = &   &  & p(1-p) & + & 2p(1-p)^2 & + & 3p(1-p)^3 & + & \cdots \\
\end{array}
$$
I want to subtract equation (\ref{e:exgeometricp2}) from (\ref{e:exgeometricp1}) which is allowed
because (by assumption) both these series converge: the difference of the series is the series of the differences.
So, we can subtract term by term.  On the left we get
$$E(X) - (1-p)E(X) = pE(X).$$
On the right we get
$$p + p(1-p) + p(1-p)^2 + p(1-p)^3 + \cdots = 1$$
since this is just a geometric series. Thus,
$$pE(X)=1\implies E(X)=\frac 1p.$$

\bigskip

\noindent {\bf solution \#2}:\\
If we set $q:=1-p$, then
$$\displaystyle \begin{array}{rcll}
E(X) &=& (1-q)\sum_{x=1}^{\infty} x\,q^{x-1} & \\
& & & \\
&=& (1-q)\sum_{x=1}^{\infty}\frac d{dq} \left( q^{x} \right) & \quad (\mbox{power rule})\\
& & & \\
&=& (1-q)\cdot \frac d{dq} \sum_{x=1}^{\infty} q^{x} & \quad (\mbox{$\frac d{dq}$ is a linear operator})\\
& & & \\
&=& (1-q)\cdot \frac d{dq}\!\!\left( \frac q{1-q} \right) &  \quad(\mbox{sum of a geometric series})\\
& & & \\
&=& (1-q)\cdot \frac 1{(1-q)^2} = \frac 1{1-q} = \frac 1p.& \quad (\mbox{using the derivative quotient rule})\\
\end{array}$$



\newpage





\noindent {\bf Example.}\\
If $X\sim \mbox{neg.binom.}(2,p)$, compute $E(X)$.\\

\noindent {\bf solution \#1}:\\
By brute force use of definition
\begin{eqnarray*} E(X) & = & \sum_{x=2}^{\infty} x\, (x-1)p^2(1-p)^{x-2}\\
&=& 2(1) p^2 + 3(2)p^2(1-p) + 4(3)p^2(1-p)^2 + 5(4)p^2(1-p)^3 + \cdots\\
(1-p)E(X) & = & \qquad \ \ \ + 2(1)p^2(1-p) + 3(2)p^2(1-p)^2 + 4(3)p^2(1-p)^3 + \cdots\\
\end{eqnarray*}
Subtracting
$$pE(X) = 2(1)p^2 + 2(2)p^2(1-p) + 2(3)p^2(1-p)^2 + 2(4)p^2(1-p)^3 + \cdots.$$
Multiplying through by $1-p$ in this last equation we obtain
$$(1-p)pE(X) = \qquad \ \ \ + 2(1)p^2(1-p) + 2(2)p^2(1-p)^2 + 2(3)p^2(1-p)^3 + \cdots,$$
and, again, subtracting
$$p^2E(X) = 2p^2 + 2p^2(1-p) + 2p^2(1-p)^2 + 2p^2(1-p)^3 + \cdots = \frac {2p^2}{1-(1-p)} = 2p$$
and $E(X) = \frac 2p$.\\

\vskip .5 in

\noindent {\bf solution \#2}:\\
From the remark on page \pageref{sumof2geometrics} we learned if $X\sim \mbox{neg.binom}(2,p)$, then $X=X_1+X_2$, where $X_1$ and $X_2$ are geometric$(p)$ random variables, in fact, {\em independent} geometric$(p)$'s. But then, by linearity of expectation, $E(X) = E(X_1+X_2)=E(X_1)+E(X_2) = \frac 1p + \frac 1p = \frac 2p$.\\

\newpage

\noindent {\bf Properties of expected value.}\label{expectedvalueproperties}\\

\noindent Following properties are true for rvs in general not just discrete rvs assuming the expected values exist.\\

\noindent {\bf Property \#1}: \\
For any constants $a$ and $b$ and random variable $X$:
$$E(aX+b) = aE(X) + b.$$

\noindent Proof. Assuming $X$ is discrete, and using LOTUS,
\begin{eqnarray*} E(aX+b) &=& \sum_x (ax+b)P(X=x)\\
&=& \sum_x \left(ax\,P(X=x) + b\,P(X=x)\right)\\
&=& \sum_x ax\,P(X=x) + \sum_x  b\,P(X=x)\\
&=& a\underbrace{\sum_x x\,P(X=x)}_{=E(X)} + b\underbrace{\sum_x  P(X=x)}_{= 1}.\end{eqnarray*}$\hfill \Box$

\bigskip

\noindent {\bf Two special cases}:\\

If $a=0$ and $b$ is constant, then $E(b) = b$. This says the expected value of a constant is the constant.\\

If $a$ is constant and $b=0$, then $E(aX) = aE(X)$. This says constant factors can be taken out of the expectation.\\


\vskip .5 in

\noindent {\bf Property \#2}: ({\bf Linearity of expectation})\\
\noindent If the expected values of the rvs $X_1,X_2,\dots,X_n$ exist, then
$$E\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n E(X_i).$$

\bigskip

\noindent We will give a proof of this later in the course.



\newpage

\noindent {\bf Expected values: existence and nonexistence}\label{expectedvaluesexistence}\\

\noindent This discussion pertains to {\bf \em all} \,random variables - not just discrete ones.
For a discrete random variable $X$ we defined
$$E(X) = \sum_x x\,P(X=x).$$
When $X$ is finitely supported there is no ambiguity in this definition - namely, it is just a finite sum of finite numbers and thus will be finite.
Now suppose the random variable is not finitely supported, i.e., can take on arbitrarily large positive and/or negative values, then our definition of expected value
is no longer a finite sum and, in fact, becomes an {\em infinite series}. So care must be taken in interpreting when the expected value exists!\\

\noindent First suppose $X$ is nonnegative but not finitely supported.
Then, since $X\ge 0$ it is clear that $E(X)\ge 0$. We will say the expected value of $X$ exists if $0\le E(X) < \infty$, i.e., the infinite series converges, and in this case the expected value is bounded above and below by finite numbers; otherwise, $E(X)=+\infty$ and we say the expected value is infinite.  Similarly, if $X\le 0$ but not finitely supported, then $E(X)\le 0$.  In this case, if $E(X)>-\infty$, then
$-\infty < E(X) \le 0$ and we will say the expected value of $X$ exists and is finite since it is, again, bounded above and below by finite numbers; otherwise, $E(X)=-\infty$ and we say the expected value is infinite.  FYI: If $X\le 0$, then $-X\ge 0$ and everything written in these last two sentences falls into first case described in this paragraph - so handling the negative case was redundant.\\

\noindent What if $X$ is not finitely supported but can take on both positive and negative values?  In this case, probabilists have agreed to
write $X$ as the difference between its positive and negative parts.
Let's define
{\bf \em the positive part of $X$} as
$$X^+ = \max\{X,0\}$$
and {\bf\em the negative part of $X$} as
$$X^- = \max\{-X,0\}.$$
$X^+$ and $X^-$ are {\em nonnegative} random variables and
$$X = X^+ - X^-.$$
We then say that the the expected value {\bf \em exists and is finite} and label if $\mu$ if
both $E(X^+)$ and $E(X^-)$ exist and are finite.  By the way, we point out that $|X|=X^+ + X^-$ and therefore, $E(X)$ exists and is finite if and only if $E(|X|)$ exists and is finite! \\

\noindent Thus, $E(X)$ can only exist and be finite when both $E(X^+)$ and $E(X^-)$ are both finite.
$E(X)$ will not exist if $E(X^+)$ and $E(X^-)$ are {\em both} infinite.

\newpage


\noindent {\bf\em Calculus fact: $p$-series, and the harmonic series diverges.}\\
The series $$\displaystyle \sum_{n=1}^{\infty} \frac 1{n^p}$$
is called a {\bf\em $p$-series}.

When $0<p\le 1$, this series diverges, i.e., it is infinite.

When $p>1$, the series converges.

In the special case $p=1$, we call the series
$$\displaystyle \sum_{n=1}^{\infty}\frac 1{n}$$
the {\bf \em harmonic series} which diverges.  (Can you give a proof of this?)\\

\vskip .5 in

\noindent {\bf Example.} (A random variable with infinite expected value)\\
The {\bf\em Riemann Zeta function}\label{riemannzeta} is defined as
$\zeta(p) = \sum_{n=1}^{\infty}\frac 1{n^p}$.  We consider this function only on the domain $\{p\in {\mathbb R}:p>1\}$ so that
for $p$ in this domain, $0<\zeta(p)<\infty$. It follows that
$$\frac {\frac 1{n^p}}{\zeta(p)}\quad \mbox{for }n=1,2,3,\dots$$
is a probability mass function and is often called the Zeta$(p)$\label{zetadist} distribution.\\

\noindent Now, consider a (discrete) random variable $X$ having the Zeta(2) distribution.  It can be shown that $\zeta(2)=\frac {\pi^2}6$ but this is not important.
Compute $E(X)$.\medskip

$$E(X) = \sum_{n=1}^{\infty} n\, P(X=n) = \sum_{n=1}^{\infty} n\cdot \frac {\frac 1{n^2}}{\zeta(2)} = \frac 6{\pi^2}\sum_{n=1}^{\infty}\frac 1n = +\infty.$$

\noindent In a sense the reason this random variable has infinite expected value is because the tail probabilities are too large, they don't go to zero fast enough to make the series converge.


\includegraphics*[-100,0][300,150]{zeta2-pmf.jpg}
\begin{center}{\bf Figure.} A plot of the Zeta(2) pmf.\end{center}





\newpage






\noindent {\bf Example.} (An example of a distribution whose expected value does not exist)\\
Consider a discrete random variable $X$ with the pmf
$$P(X=n) = \frac 3{\pi^2}\cdot \frac 1{n^2}\quad \mbox{for }n=\pm 1,\pm 2,\pm 3,\dots .$$
Notice, in this example, that $X$ takes on both arbitrarily large positive and negative values. Moreover,
$$E(X^+) = E(X^-) = \frac 3{\pi^2}\sum_{n=1}^{\infty} n\cdot \frac 1{n^2} = +\infty.$$
Consequently, since both positive and negative parts of $X$ have infinite expected values, $E(X)$ {\bf\em does not exist}.

\includegraphics*[-90,0][300,155]{doublezeta2-pmf.jpg}
\begin{center}{\bf Figure.} plot of the double Zeta(2) distribution.\end{center}


\vskip .25 in


\noindent {\bf Remark.}\\
It might seem strange at first to say that the expected value in the last example does not exist since the distribution is symmetric about $X=0$
and one might want to believe that $E(X)$ is zero.  But, {\em by definition}, we are saying this expected value does not exist (because both the positive and negative parts have infinite expectations).  The calculus reason is that we are demanding our infinite series to be {\em absolutely} convergent and not just conditionally convergent.\\

\noindent The infinite series $E(X)$ is a sum of positive and negative terms.  An infinite series $\sum_n a_n$ is called {\bf \em absolutely convergent} if the series $\sum_n |a_n|$ of the absolute values of each of these terms also converges. If the series $\sum_n a_n$ converges but is not absolutely convergent, then it is {\bf\em conditionally convergent}.
In the above example, $E(X)$ is only conditionally convergent and not absolutely convergent.\\

\noindent The following comparison of facts regarding conditionally convergent and absolutely convergent series is helpful.
No matter how the terms of an absolutely convergent series are rearranged, the sum will always be the {\em same}. However, if we have a conditionally convergent series that is not absolutely convergent (i.e., the sum of the absolute values is infinite), then the terms of the series can be rearranged to sum at any value we desire. This non-uniqueness is the issue we want to avoid in our definition of expected value.


\newpage


\noindent Sometimes we have or know the distribution of a random variable $X$ but we want to compute the expected value of some function of $X$ instead. For example, suppose we have a very good model for the discrete random variable $X$ representing the quantity produced, i.e.,
we know the pmf, say, $P(X=x)$.  We have a {\em cost function} associated with producing the quantity $x$, namely,
$$C(x) = 0.1x^3 -2x^2 + 60x + 200.$$
Then $C(X)$ is a random variable, and we may like to compute the expected cost $E[C(X)]$.\\

\bigskip

\noindent The following
theorem can be very helpful in situations like this.  \\


\bigskip

\noindent {\bf Law of the Unconscious Statistician (LOTUS).}\label{lotus}\\
Suppose $X$ is a discrete random variable and $g=g(x)$ is a function. Then
$$E[g(X)] = \sum_x g(x)\,P(X=x),$$
assuming this expected value exists.\\


\bigskip

\noindent {\bf Proof.}  Set $Y=g(X)$ and let $y$ be a value of the random variable $Y=g(X)$. Then, by definition,
\begin{eqnarray*}
E[g(X)]
& = & \sum_y y\,P[g(X)=y]\\
& = & \sum_y y\,\sum_{x:g(x)=y}P(X=x)\\
& = & \sum_y \sum_{x:g(x)=y}y\,P(X=x)\\
& = & \sum_y \sum_{x:g(x)=y}g(x)\,P(X=x)\\
& = & \sum_x g(x)\,P(X=x).\\
\end{eqnarray*}$\hfill \Box$



\bigskip

\noindent {\bf Example.}  Suppose $X$ has the pmf:
$$\begin{array}{c||cccccc|} x & -2 & -1 & 0 & 1 & 2 & 5 \\ \hline P(X=x) & 0.4 & 0.2 & 0.1 & 0.1 & 0.1 & 0.1\end{array}$$

\noindent Compute $E(X^2)$.\\

Using LOTUS,
\begin{eqnarray*}E(X^2) &=& \sum_{x\in \{-2,-1,0,1,2,5\}} x^2\,P(X=x)\\ &=& (-2)^2\,(0.4)+(-1)^2\,(0.2)+0^2\,(0.1)+1^2\,(0.1)+2^2\,(0.1)+5^2\,(0.1)\\
&=&1.6 + 0.2 + 0 + 0.1 + 0.3 + 2.5 = {\bf 4.8}.\end{eqnarray*}


\newpage

\noindent {\bf Remark.}\\
\noindent Without LOTUS how would we compute $E(X^2)$?  We'd have to use the definition which says to take the weighted average of the values of the rv $Y=X^2$ weighted against the pmf of $Y$.  Therefore, we'd need to derive the pmf of $Y$ since it isn't directly given to us.
From the last example, the possible values of $Y=X^2$ are $y=0,1,4,$ and 25. Moreover,

$P(Y=0) = P(X=0)=0.1$

$P(Y=1) = P(X=1\cup X=-1) = P(X=1)+P(X=-1)=0.1+0.2=0.3$

$P(Y=4) = P(X=2\cup X=-2) = P(X=2)+P(X=-2) = 0.1+0.4 = 0.5$

$P(Y=25) = P(X=5) = 0.1$

\noindent is the pmf for $Y$. Therefore, we'd compute
$$E(Y) = 0\times 0.1 + 1\times 0.3 + 4\times 0.5 + 25\times 0.1 = 4.8.$$
Of course, we get the {\em same} answer, but, LOTUS allowed us to bypass having to compute the pmf of $X^2$ and this sure saved us a lot of time!\\


\vskip 1 in


\noindent {\bf Moments of a random variable/distribution.}\label{moments}\\
Let $X$ be a random variable.  We define
$$\mu_k := E(X^k)$$ to be the {\bf\em $k$th moment of $X$} or the {\bf\em $k$th moment of the distribution of $X$}.\\

\vskip .5 in

\noindent {\bf Remark.}\\
Notice that $\mu_0=1$ for any random variable and, therefore, when we talk about the moments of a random variable we typically mean for $k=1,2,3,\dots$.
Also, when $k=1$, the {\bf \em first moment} is just $\mu$, i.e., the {\bf \em mean} of $X$.  We'll see that the moments of a random variable reveal information about the underlying distribution of the random variable.  In a sense the more moments we know the more information we have about the probability distribution of the rv.\\

\vskip .5 in

\noindent {\bf Example.} \label{bernoullipmoments}\\
Let $X\sim \mbox{Bernoulli}(p)$.  \\
Use LOTUS to compute the $k$th moment of the Bernoulli$(p)$, i.e.,
$E(X^k)$ for integer $k\ge 1$.\\

$\displaystyle E(X^k) = 0^kP(X=0) + 1^kP(X=1) = p$ for every $k\ge 1$.\\




\newpage





\noindent {\bf Variance.}\label{variance}

\noindent Let $X$ be a random variable having a finite mean $\mu$.
Then $X-\mu$ is the deviation $X$ makes from its mean, and $(X-\mu)^2$ is the squared deviation $X$ makes from its mean.
We define
$$\sigma^2:= Var(X) := E[(X-\mu)^2].$$


\vskip .5 in

\noindent {\bf Remark.}\\
The variance of $X$ measures how spread out the values of the random variables are from its mean.  Clearly, $Var(X)\ge 0$ always.
The larger the variance is, the more spread out the values are from its mean -- the values of $X$ can fluctuate wildly from the mean.
The closer the variance is to 0, the less spread out the values are from the mean -- the values do not fluctuate much from its mean.\\

\noindent If we have two rvs $X$ and $Y$ and we know $Var(X)<Var(Y)$, then the values in the distribution for $Y$ are more spread out than those for the distribution $X$. See the figure below.

\vskip .25 in


\includegraphics*[-40,0][400,270]{negbinom-pmfs-2.jpg}
\begin{center}{\bf Figure.} Top picture: $\mu=13.3$, $\sigma^2=31.1$.   Bottom picture: $\mu=5.7, \sigma^2=2.45$.\end{center}




\newpage




\noindent {\bf Remark.}\\
We usually think of a random variable $X$ as making a measurement on the sample point produced by the experiment, and as such, there are typically units associated with
this measurement (e.g., $\mu g/\ell$, inches, $^{\circ}$F, etc.).
By definition, when $X$ is discrete say,\vskip -.1 in
$$\mu = E(X) = \sum_x x\,P(X=x)$$\vskip -.1 in
\noindent and, clearly, we see $\mu$ is in the same units as $X$; BUT!
$$\sigma^2 = Var(X) = \sum_x (x-\mu)^2\,P(X=x),$$
\noindent and variance $\sigma^2$ is in units-squared of the random variable.  For this reason, we may also want a measure of spread/fluctuation that is on the same scale as the mean; and we define
$$\sigma = \sqrt{Var(X)}$$
as the {\bf\em standard deviation of $X$}.\\


\vskip .5 in


\noindent {\bf Example.} Compute the mean, variance, and standard deviation of $X$ having pmf:
$$\begin{array}{c||cccc|}x & -2 & -1 & 0 & 5 \\ \hline P(X=x) & 0.4 & 0.2 & 0.1 & 0.3 \end{array}$$

\bigskip

\begin{eqnarray*}
\mu = E(X) &=& -2(0.4) -1(0.2) + 0(0.1) + 5(0.3) = 0.5.\\
& & \\
& & \\
\sigma^2 = Var(X) &=& E[(X-\mu)^2] \\
& = & (-2-.5)^2(0.4) + (-1-.5)^2(0.2) + (0-.5)^2(0.1) + (5-.5)^2(0.3)\\
& = & 2.5 + 0.45 + 0.025 + 6.075 = 9.05.\end{eqnarray*}

\vskip .5 in

\noindent {\bf Advice:}\\
\noindent When asked to compute a variance it is usually better to use the following formula:

\noindent {\bf \em A computational form for $Var(X)$:}\label{variancecomputation}\\
$$Var(X) = E(X^2) - \left[ E(X)\right]^2$$

\vskip .5 in

\noindent {\bf Remark.}\\
These variance formulas we have given are true for any type of rv; however, the proof below assumes the rv is discrete.\\




\newpage


\noindent {\bf Proof of the computational form of the variance.}
\begin{eqnarray*}
E[(X-\mu)^2]
&=& \sum_x (x-\mu)^2\,P(X=x)\\
&=& \sum_x (x^2-2\mu x + \mu^2)\,P(X=x)\\
&=& \sum_x (x^2\,P(X=x) -2\mu x\, P(X=x) + \mu^2\, P(X=x))\\
&=& \sum_x x^2\,P(X=x) -\sum_x 2\mu x\, P(X=x) + \sum_x \mu^2\, P(X=x)\\
&=& \sum_x x^2\,P(X=x) -2\mu \underbrace{\sum_x x \,P(X=x)}_{=\,\mu} + \mu^2 \underbrace{\sum_x P(X=x)}_{=\,1}\\
&=& E(X^2) - 2\mu^2 + \mu^2\\
&=& E(X^2) - \mu^2
\end{eqnarray*}$\hfill \Box$



\vskip .5 in

\noindent {\bf Example.}\\
From page \pageref{bernoullipmoments} we learned if $X\sim \mbox{Bernoulli}(p)$, then $E(X^k)=p$ for all $k\ge 1$.
Therefore, in particular, $E(X^2)=p$ and $E(X)=p$, and it follows
$$Var(X) = p -p^2 = p(1-p).$$


\vskip .5 in


\noindent {\bf Property of Variance.}\label{propertyofvariance}\\
\noindent For any constants $a$ and $b$,
$$Var(aX+b) = a^2Var(X).$$

\bigskip

\noindent Proof.
\begin{eqnarray*} Var(aX+b) &=& E\left[\Big(aX+b - E(aX+b)\Big)^2\right] = E\left[ \Big(aX+b - (a\mu + b)\Big)^2 \right] \\
&=& E\left[a^2(X-\mu)^2 \right] = a^2Var(X).\end{eqnarray*}

\noindent {\bf Two special cases:}\\

When $a=1$ and $b$ is any constant, then $Var(X+b)=Var(X)$.  This says the variance of $X$ doesn't change if you shift the distribution by a constant $b$, This seems plausible.

\bigskip

When $a$ is constant and $b=0$, then $Var(aX)=a^2Var(X)$. In particular, if $a=-1$, $Var(-X)=Var(X)$.\\



\newpage



\noindent {\bf Example.} Let $X\sim \mbox{binom}(n,p)$.  Compute $Var(X)$.\\

We already know $E(X) = np$.  Next,
\begin{eqnarray*}
E(X^2)
&=&
\sum_{x=0}^n x^2\,\frac {n!}{x!(n-x)!}p^x(1-p)^{n-x}\\
&=&
\sum_{x=1}^n x^2\,\frac {n!}{x!(n-x)!}p^x(1-p)^{n-x}\\
&=&
\sum_{x=1}^n x\,\frac {n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x}\\
&=&
\sum_{x=1}^n (x-1+1)\,\frac {n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x}\\
&=&
\sum_{x=1}^n (x-1)\,\frac {n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x} + \sum_{x=1}^n \frac {n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x}\\
&=&
\sum_{x=2}^n (x-1)\,\frac {n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x} + np\sum_{x=1}^n \underbrace{\frac {(n-1)!}{(x-1)!(n-x)!}}_{=\,{n-1\choose x-1}}p^{x-1}(1-p)^{n-1-(x-1)}\\
&=&
\sum_{x=2}^n \frac {n!}{(x-2)!(n-x)!}p^x(1-p)^{n-x} + np\underbrace{\sum_{x=1}^n {n-1\choose x-1}p^{x-1}(1-p)^{n-1-(x-1)})}_{=\,1\mbox{ \tiny by binomial theorem}}\\
&=&
n(n-1)p^2\sum_{x=2}^n \underbrace{\frac {(n-2)!}{(x-2)!(n-x)!}}_{=\,{n-2\choose x-2}}p^{x-2}(1-p)^{n-2-(x-2)} \quad + \quad np\\
&=&
n(n-1)p^2\underbrace{\sum_{x=2}^n {n-2\choose x-2}p^{x-2}(1-p)^{n-2-(x-2)}}_{=\,1\mbox{ \tiny by binomial theorem}}\quad +\quad np\\
&=& n(n-1)p^2 + np.
\end{eqnarray*}
Finally,
$$Var(X) = E(X^2) - \left[E(X)\right]^2 = n(n-1)p^2 + np - \left[np\right]^2 = np(1-p).$$

\noindent Summary: If $X\sim \mbox{binom}(n,p)$, then $\mu=np$, $\sigma^2 = np(1-p)$ and $\sigma = \sqrt{np(1-p)}$.$\hfill \Box$


\bigskip
\bigskip
\bigskip

\noindent {\bf Exercise for students.}\\
Use same approach to find $Var(X)$ when $X\sim \mbox{Poisson}(\lambda)$.\\



\newpage





\noindent {\bf Moment-generating functions.}\label{mgf-part1}\\

\noindent Suppose $X$ is a random variable with a specified distribution.
A very useful function (when it exists) in working with some of the more well-known probability distributions is the so-called
moment-generating function\label{d:mgf}:

If the following expected value thought of as a function of $\theta$:
$$M(\theta) = M_{X}(\theta) = E(e^{\theta X})$$
exists and is finite for $\theta$ in an open neighborhood of $\theta=0$, i.e., $E(e^{\theta X})<\infty$ for $\theta\in (-\varepsilon,\varepsilon)$ for
some $\varepsilon >0$, then we call $M(\theta)$ the {\bf \em moment-generating function (MGF) of $X$} or {\bf\em of its distribution}.\\


\vskip .5 in

\noindent {\bf Some properties of the moment-generating function.}\label{mgfproperties1}\\

Fact 1.  The MGF uniquely identifies the distribution of the random variable.\\

Fact 2. $|E(X^k)|<\infty$ for all integer $k\ge 1$.\\

Fact 3. Taking derivatives of the MGF with respect to $\theta$ and evaluating at $\theta=0$ will give the moments of $X$.
For example,
\begin{center}$M'(0) = E(X)$,\qquad $M''(0) = E(X^2),\qquad M'''(0)=E(X^3)$,\quad etc.\end{center}

To roughly see why, since expectation and differentiation are linear operations we can exchange their order:
$$\frac{d^k}{d\theta^k}M(\theta) = \frac{d^k}{d\theta^k} E(e^{\theta X}) =  E(\frac{d^k}{d\theta^k}e^{\theta X}) = E(X^ke^{\theta X}) \implies \frac{d^k}{d\theta^k}M(0)=E(X^k).$$

\vskip .5 in


\noindent {\bf Example.} (MGF of a binom$(n,p)$)\label{binomialnpmgf}\\
Let's compute the MGF (if it exists) of $X\sim \mbox{binom}(n,p)$.  Using LOTUS
\begin{eqnarray*}
M(\theta) = E(e^{\theta X})
& = &
\sum_{x=0}^n e^{\theta x}\,{n\choose x}p^x(1-p)^{n-x}\\
& = &
\sum_{x=0}^n {n\choose x}(pe^{\theta})^x(1-p)^{n-x}\\
&=& (1-p + pe^{\theta})^n \qquad (\mbox{from the binomial theorem.})
\end{eqnarray*}






\newpage




\noindent {\bf Example.}\\
Let $X\sim \mbox{Poisson}(\lambda)$. Compute the MGF of $X$.\\


\begin{eqnarray*}
M(\theta) &=& E(e^{\theta X})\\
&=& \sum_{x=0}^{\infty} e^{\theta x} P(X=x)\\
&=& \sum_{x=0}^{\infty} e^{\theta x} \frac {e^{-\lambda}\lambda^x}{x!}\\
&=& e^{-\lambda}\sum_{x=0}^{\infty} e^{\theta x} \frac {\lambda^x}{x!}\\
&=& e^{-\lambda}\sum_{x=0}^{\infty} \frac {\big(e^{\theta} \lambda\big)^x}{x!}\\
&=& e^{-\lambda}e^{\lambda e^{\theta}} = e^{\lambda (e^{\theta}-1)}
\end{eqnarray*}








\vskip .5 in



\noindent {\bf Exercise.} {\bf\em Please do this!}\\
Use the above MGF to compute $\mu$, $\sigma^2$ and $\sigma$ for the Poisson$(\lambda)$.\\
%ANSWERS: For Poisson$(\lambda)$: $\mu=\lambda$, $\sigma^2=\lambda$, $\sigma= \sqrt{\lambda}$, for binom$(n,p)$: $\mu=np$, $\sigma^2=np(1-p)$, $\sigma = \sqrt{np(1-p)}$.\\

\vskip 2 in


\noindent {\bf Exercise.} {\bf\em Please do this!}\\
Use the binomial MGF we computed on page \pageref{binomialnpmgf} to re-derive the mean, variance and standard deviation of the binom$(n,p)$.

\newpage

\noindent {\bf Cumulative distribution functions.}\label{cdfs}\\

Let $X$ be {\bf \em any} random variable.  The {\bf \em cumulative distribution function (CDF)} or, simply, the {\bf\em distribution function} of $X$ is the function
$$F:{\mathbb R}\to [0,1]\quad\mbox{defined by}\quad F(x) = P(X\le x).$$


\vskip .5 in

\noindent {\bf\em Properties of the CDF.} (proofs are on next page\dots)\\

Property 1.  $F=F(x)$ is a monotone nondecreasing function of $x$; namely,
$$x < y \implies F(x)\le F(y).$$

Property 2.  $F=F(x)$ is always a right-continuous function of $x$; namely,
$$\mbox{For all real }x,\quad F(x) = \lim_{h\to 0^+} F(x+h) =: F(x+).$$

Property 3.  $F=F(x)$ always possesses left limits; namely,
$$\mbox{For all real }x,\quad \lim_{h\to 0^+}F(x-h) =: F(x-)\ \mbox{  exists}.$$

Property 4.  $\displaystyle \lim_{x\to -\infty}F(x) = 0$.\\

Property 5.  $\displaystyle \lim_{x\to +\infty}F(x) = 1$.\\

\vskip 1 in

\noindent {\bf Remark.}\\
In principle, knowing the CDF of a random variable $X$ is enough to compute probabilities like
$$P(a<X\le b),\quad P(a\le X\le b),\quad P(a<X<b),\quad\mbox{and}\quad P(a\le X < b).$$


$$P(a<X\le b) = F(b) - F(a).$$

$$P(a\le X\le b)=F(b) - F(a-).$$

$$P(a<X<b)=F(b-)-F(a).$$

$$P(a\le X < b)=F(b-)-F(a-).$$


\newpage

\noindent {\bf Proof of Property 1.}\\
Suppose $x<y$. Then the event $(X\le x)\subseteq (X\le y)$, and, by monotonicity of probability, $F(x)=P(X\le x)\le P(X\le y)=F(y)$.\\

\vskip .5 in


\noindent {\bf Proof of Property 2.}\\
Since $E_n := \bigcap_{k=1}^n (X\le x+\frac 1k)=(X\le x+\frac 1n) \downarrow \bigcap_{k=1}^{\infty}(X\le x+\frac 1k) =^* (X\le x)$ as $n\to \infty$,
by continuity of probability measure from above, \vskip -.1 in
$$P(E_n)=F(x+\frac 1n) \longrightarrow F(x)=P(X\le x)\quad\mbox{as }n\to \infty.$$

\noindent $^*$ to see why $\displaystyle \bigcap_{k=1}^{\infty}(X\le x+\frac 1k) = (X\le x)$ we will show
$$(X\le x)\subseteq \bigcap_{k=1}^{\infty}(X\le x+\frac 1k)\quad \mbox{and}\quad \bigcap_{k=1}^{\infty}(X\le x+\frac 1k) \subseteq (X\le x).$$

\noindent Let $\omega\in (X\le x)$. Then $X(\omega)\le x$. This implies $X(\omega)\le x\le x+\frac 1k$ for every $k\ge 1$. So, $\omega\in (X\le x+\frac 1k)$ for every integer $k\ge 1$.
Therefore, $\omega\in  \bigcap_{k=1}^{\infty}(X\le x+\frac 1k)$.  This proves the first of the two containments above.  We'll show the second containment using complements, namely, we'll show if
$\omega\in (X>x)$, then $\omega \in \bigcup_{k=1}^{\infty}(X>x+\frac 1k)$.  To this end, suppose $\omega\in (X>x)$. Then $X(\omega)>x$. Let $\varepsilon = X(\omega)-x>0$.  Since $\frac 1k\to 0$ as $k\to \infty$, there exists $K$ such that $\frac 1k < \varepsilon$ for all $k\ge K$; i.e., eventually, $\frac 1k$ will be smaller than any fixed positive $\varepsilon$.
So, in particular, $X(\omega)-x=\varepsilon > \frac 1K$, which in turn implies $X(\omega)>x+\frac 1K$, i.e., $\omega\in (X>x+\frac 1K)$ and, therefore, $\omega\in \bigcup_{k=1}^{\infty}(X>x+\frac 1k)$.\\


\vskip .5 in


\noindent {\bf Proof of Property 3.}\\
Since $A_n:=\bigcup_{k=1}^n (X\le x-\frac 1k)=(X-\frac 1n) \uparrow \bigcup_{k=1}^{\infty}(X\le x-\frac 1k) =^{**} (X<x)$ as $n\to \infty$,
by continuity of probability measure from below,\vskip -.1 in
$$P(A_n) = F(x-\frac 1n) \longrightarrow F(x-)=P(X<x)\quad\mbox{as }n\to \infty.$$

\noindent $^{**}$ to see why $\displaystyle \bigcup_{k=1}^{\infty}(X\le x-\frac 1k) = (X < x)$ we'll show
$$\bigcup_{k=1}^{\infty}(X\le x-\frac 1k) \subseteq (X<x)\quad \mbox{and}\quad (X<x)\subseteq \bigcup_{k=1}^{\infty}(X\le x-\frac 1k).$$

\noindent The first containment follows because if $\omega\in \bigcup_{k=1}^{\infty}(X\le x-\frac 1k)$, then $\omega\in (X\le x-\frac 1n)$ for some integer $n\ge 1$, i.e., you can only belong to a union if you belong to at least one of the events in the union.  But then $X(\omega)\le x-\frac 1n < x$ implies $\omega\in (X<x)$.  The second containment take an aribitrary
$\omega\in (X< x)$.  Then $X(\omega)=x -\varepsilon$ for some positive $\varepsilon$. But then $X(\omega) =x-\varepsilon \le x-\frac 1k$ for all integers $k>\frac 1{\varepsilon}$, which shows
$\omega\in \bigcup_{k=1}^{\infty}(X\le x-\frac 1k)$.\\



\newpage


\noindent {\bf Proof of Property 4.}\\
Since $X$ is a random variable, it's real-valued on $\Omega$, and $\bigcap_{n=1}^{\infty}(X\le -n) = \varnothing$. So, by continuity of probability measure
it follows $P(X\le -n)\to P(\varnothing)=0$ as $n\to \infty$.\\

\vskip .5 in



\noindent {\bf Proof of Property 5.}\\
Since $X$ is a random variable, $\bigcup_{n=1}^{\infty}P(X\le n) = \Omega$, and, again, by continuity of probability measure, it follows
$P(X\le n)\to P(\Omega)=1$ as $n\to\infty.$\\

\vskip .5 in




\noindent {\bf Example.}\\
Derive the CDF of the discrete random variable having pmf
$$\begin{array}{c||ccc}x & -1 & 1 & 2 \\ \hline  p(x) & 0.6 & 0.2 & 0.2 \end{array}.$$
\includegraphics*[-120,0][400,200]{discrete-cdf-1.jpg}

\begin{center}{\bf Figure.}  (Top) pmf\quad (Bottom) CDF\end{center}

\vskip .5 in

\noindent {\bf Remark.}\\
Discrete random variables have CDFs that are pure step functions.  For any random variable $X$, $P(X=x) = P(x\le X\le x) = F(x)-F(x-)$.
So, if $X$ is discrete the pmf of $X$ can be recovered from the CDF of $X$.  The pmf will vanish at all continuity points of the CDF, and, at points of jump discontinuity of $F$
the probability mass $P(X=x) = F(x)-F(x-)$ is just the size of the jump at the point of discontinuity.  From the example, the CDF is
$$F(x) = \left\{\begin{array}{cl}0 & \mbox{for }x<-1 \\ 0.6 & \mbox{for }-1\le x < 1\\ 0.8 & \mbox{for }1\le x < 2 \\ 1 & \mbox{for }x\ge 2\end{array}.\right.$$





\newpage
\noindent {\bf Example.}\\
The following is a CDF for a random variable, say, $X$:
$$F(x) = \left\{\begin{array}{cl}
0 & \mbox{for }x<-2\\
.1 & \mbox{for }-2\le x < -1.5 \\
.1\left(1+\sqrt{2.25-x^2}\right) & \mbox{for }-1.5\le x < 0\\
.5 & \mbox{for }0\le x < 1\\
.6+.1(x-1) & \mbox{for }1\le x < 5\\
1 & \mbox{for }x\ge 5. \end{array}\right.$$
\noindent Graph this CDF and then use it to compute each of the following:\\
(a) $P(X=1)$\\
(b) $P(X=-1)$\\
(c) $P(0<X<3)$\\
(d) $P(0\le X\le 2)$\\

\noindent SOLUTION:\\
\includegraphics*[-50,0][350,150]{cdfgeneralrv1.jpg}

\vskip .4 in

\noindent (a) $P(X=1) = F(1) - F(1-) = .6 + .1(1-1) - .5 = {\bf .1}$.\\

\noindent (b) $P(X=-1) = F(-1)-F(-1-) = {\bf 0}$ since $F$ is continuous at $x=-1$.\\

\noindent (c) $P(0<X<3) = F(3-)-F(0) = F(3)-F(0) = .6 + .1(3-1) - .5 = {\bf .3}$.\\

\noindent (d) $P(0\le X\le 2) = F(2)-F(0-) = .6+.1(2-1) - .1\left(1+\sqrt{2.25-(0)^2}\right)=.7-.25={\bf .45}.$\\

\vskip .4 in

\noindent {\bf Remark.}\\
The random variable $X$ having the CDF in this last example cannot be discrete since the CDF is not a step function.
We'll see shortly that $X$ cannot be continuous either; therefore, this rv $X$ is neither discrete nor continuous. The important point here is that,
with this rv, we computed probabilities through its CDF.


\newpage


\begin{center}{\bf \Large IV. Continuous random variables.}\end{center}



\newpage

\noindent {\bf Continuous random variables.}\label{continuousrvs}\\
Discrete random variables have CDFs that are pure steps functions.  Continuous random variables have CDFs that are {\bf\em continuous functions}. \\

\noindent A random variable $X$ will be called a {\bf \em continuous random variable} if its CDF is a continuous function on the real line.\\

\bigskip

\noindent Let's assume we have a continuous random variable $X$ and let
$$F_X(x) = P(X\le x)$$
be its continuous CDF.  Fix an arbitrary $x\in {\mathbb R}$. Then
$$P(X=x)=P(x\le X\le x) = F_X(x)-F_X(x-) = 0$$
since $F(x)$ being continuous means the left limit $F(x-)$ at $x$ equals the value of the function $F(x)$ ar $x$.  And we arrive at a strange feature of continuous random variables. They assign {\bf\em zero} probability mass to individual points.\\

Now, although
\begin{equation}P(x-h<X\le x) = F_X(x) - F_X(x-h)\longrightarrow 0\quad \mbox{as }h\downarrow 0,\label{densitynumerator}\end{equation}
\noindent equation
(\ref{densitynumerator}) gives us some hope; namely, if the CDF is continuously differentiable (more than just continuous), then
$$\frac {P(x-h<X\le x)}{h} = \frac {F_X(x) - F_X(x-h)}{h}\longrightarrow f(x)\quad\mbox{as }h\downarrow 0.$$
Of course, the function $f(x)=:F'(x)$ and is measuring the probability mass per unit value at $x$, namely, $f(x)$ is the {\bf\em probability density function (pdf)}\label{pdf} at $x$.\\

\vskip .3 in

\noindent {\bf\em Assumption about continuous rvs we will make:}\label{pdfexistassumption}\\
Throughout this course {\bf \em we will assume} the CDF admits the existence of a pdf $f(x)$.\\
%probability mass may be tending to 0 as $h\downarrow 0$ but it may happen that the probability mass per unit value of the random variable is not.  That is, maybe

\vskip .5 in

\noindent {\em What can we do with the pdf when we have it ?}\\
\vskip .5 in

\noindent Let $-\infty < a < b < \infty$ and take $n>1$ to be large. Set $h = \frac {b-a}{n}$ so that
$$(a,b] = \left(a,a+h\right]\cup \left(a+h,a+2h\right] \cup \cdots \cup \left(a+(n-1)h,b\right].$$
In what follows I am assuming the pdf $f(x)$ is continuous but this is not needed.


\newpage



\includegraphics*[-150,0][200,160]{pdfintegral.jpg}
\begin{center}{\bf Figure.} On $\Big(a+(j-1)h,a+jh\Big]$, minimizer = $x_*$, maximizer = $x^*$\end{center}

\vskip .5 in

\begin{eqnarray*}
P(a<X\le b) &=& \sum_{j=1}^n P\left(a+(j-1)h  <X\le  a+jh\right)\\
&=& \sum_{j=1}^n F\left(a+jh\right)-F\left(a+(j-1)h\right)\\
&=& \sum_{j=1}^n \frac {F\left(a+jh\right)-F\left(a+(j-1)h\right)}{h}\cdot h\\
&=& \sum_{j=1}^n \underbrace{\frac {F\left(a+jh\right)-F\left(a+jh - h\right)}{h}}_{\approx f(x)}\cdot h
\end{eqnarray*}
\noindent Therefore,
$$
\sum_{j=1}^n f(x_{*})\cdot h \le  P(a<X\le b) \le \sum_{j=1}^n f(x^*)\cdot h,$$
and, as $n\to\infty$, we get the Riemann integral of $f(x)$ over the interval $(a,b]$:
$$P(a<X\le b) = \int_a^b f(x)\,dx.$$



\newpage

\noindent {\bf Properties of a pdf.}\\
\noindent 1. $F(x) = P(X\le x) = \int_{-\infty}^x f(u)\,du$ - this is the first fundamental theorem of calculus.\medskip

\noindent 2. $f(x) = F'(x)$ - this is the second fundamental theorem of calculus.\medskip

\noindent 3. Since the CDF is defined on the entire real line, so is the pdf.\medskip

\noindent 4. Since the CDF is monotone nondecreasing, $f(x)\ge 0$ for all $x\in {\mathbb R}$.\medskip

\noindent 5. Since $\lim_{x\to\infty}F(x)=1$, $\int_{-\infty}^{\infty} f(x)\,dx = 1$.\medskip

\vskip .5 in

\noindent {\bf Remark.}\\
Properties $3,4$ and $5$ characterize a pdf; i.e., if a function
$f:{\mathbb R}\to [0,\infty)$ is such that $\int_{-\infty}^{\infty}f(x)\,dx=1$,
then there is a random variable having $f(x)$
as its pdf.\\

\vskip .5 in

\noindent {\bf Remark.} (computing probabilities of a continuous rv with given pdf)\\
\noindent For any $a\le b$, $P(a<X\le b) = F(b) - F(a) = \int_{-\infty}^b f(x)\,dx - \int_{-\infty}^a f(x)\,dx = \int_a^b f(x)\,dx$, and this allows us
to compute probabilities when the pdf $f(x)$ is known. Moreover, because the pdf puts {\em no} probability mass on individual values of the rv,
$P(a\le X\le b) = P(a< X\le b) = P(a\le X < b) = P(a< X< b).$\\

\vskip .5 in

\noindent {\bf Advice:}\\
It's helpful to remember, like in a course in Physics, that when computing probabilities involving discrete random variables
we add probability masses to get probability mass; when
computing probabilities involving continuous random we integrate probability density to get probability mass.\\

\newpage


\noindent {\bf Example.}\\
A researcher believes the pdf of a continuous rv $X$ has the shape of the following function:
$$f(x) = \left\{ \begin{array}{cl} cx(1-x) & \mbox{for }0\le x\le 1\\ 0 & \mbox{elsewhere}  \end{array} \right..$$
(a) Find the constant $c$ that makes this a pdf.\\
(b) Compute $P(\frac 13\le X\le \frac 23).$\\

\noindent SOLUTION:\\
(a) $f(x)=0$ for all $x\not\in [0,1]$, and, for $x\in [0,1]$ and any fixed constant $c$, $cx(1-x)$ doesn't change sign. So $f(x)$ will be $\ge 0$ for all real $x$ when $c\ge 0$.
We now look for $c$ that makes the total integral 1:
\begin{eqnarray*}\int_{-\infty}^{\infty}f(x)\,dx &=& \int_{-\infty}^0 f(x)\,dx + \int_0^1 f(x)\,dx + \int_1^{\infty}f(x)\,dx\\
&=& \int_{-\infty}^0 0\,dx + \int_0^1 cx(1-x)\,dx + \int_1^{\infty}0\,dx\\
&=& c\int_0^1 x-x^2\,dx = c\Big( \left.\frac {x^2}2 -\frac {x^3}3\right|_{x=0}^{x=1}\Big) = \frac c6 = 1 \implies c=6.\end{eqnarray*}

\includegraphics*[-40,0][400,260]{6x1-xpdf1.jpg}\vskip -.2 in
\begin{center}{\bf Figure.} Graph of the pdf in this example. Green shaded area is $P(\frac 13\le X\le \frac 23)$.\end{center}



\noindent (b) $P(\frac 13\le X\le \frac 23) = \int_{\frac 13}^{\frac 23}6x-6x^2\,dx = \left. 3x^2 -2x^3\right|_{x=\frac 13}^{x=\frac 23} = \frac {4}{3}-\frac {16}{27}=\frac {20}{27}$.


\newpage



\noindent {\bf Remark.}\\
Many of the pmfs for the named discrete distributions were modeled from discrete experiments: Bernoulli trials,
sampling with or without replacement, or repeated trials of such.   As for continuous
random variables, we will work with several {\em very useful} pdfs that have been modeled
to mimic many of these discrete pmfs and/or their properties to the continuous realm.
Here's a list of some notable ones we will discuss:

uniform$(a,b)$ - continuous analog to the discrete uniform to the interval $[a,b]$

exponential$(\lambda)$ - continuous analog to the geometric distribution

Gamma$(\alpha,\beta)$ - continuous analog to the negative binomial distribution

$\chi_n^2$ (chi-square distribution with $n$ degrees of freedom) - just a Gamma$(\frac n2,2)$

Normal$(\mu,\sigma^2)$ distribution

Pareto distribution - continuous analog to the Zeta distribution

Laplace/double exponential

Beta$(\alpha,\beta)$

$t$- and $F$-distributions

bivariate normal distribution

Dirichlet distribution

many others as time permits.\\

\vskip .2 in

\noindent {\bf Example.} (the {\bf\em exponential$(\lambda)$ distribution})\label{expdist}\\
Suppose $X$ is a continuous random variable having pdf
$$f(x) = \left\{ \begin{array}{cl} \lambda e^{-\lambda x} & \mbox{for }x\ge 0 \\ 0 & \mbox{elsewhere} \end{array} \right..$$
\noindent We will write this as $X\sim \mbox{exp}(\lambda)$. This distribution is a widely used model of component lifetimes, $X$,
i.e., where $X$ measures the how long a component lives.
If $X>x$ then this means the component is still ``alive" at time $x$.
The parameter $\lambda > 0$ is the reciprocal of the {\em mean lifetime} of the component.
As a concrete example, suppose the component is an incandescent lightbulb having a quoted mean
lifetime of $10$ (thousand hours) so that $\lambda = \frac 1{10}$. In this case, $f(x)=\frac 1{10}e^{-\frac x{10}}$ for $x\ge 0$.\\

\noindent If $X\sim \mbox{exp}(\frac 1{10})$, compute (a) $P(X>10)$,\ (b) $P(5<X<15)$, \ and \ (c) $P(-5<X<10)$.\\


\noindent SOLUTION:\\
(a) This problem is asking us to compute the probability the component lasts longer that its mean lifetime.
$$P(X>10) = \int_{10}^{\infty} \frac 1{10}e^{-\frac x{10}}\,dx = \left. -e^{-\frac x{10}}\right|_{x=10}^{\infty} = 0-(-e^{-\frac {10}{10}}) = e^{-1}\approx .3679\dots.$$
This says about 63.21\% of the components (the majority) will not last 10,000 hours.\\

\noindent (b) $P(5<X<15) = \int_5^{15}\frac 1{10}e^{-\frac x{10}}\,dx = \left. -e^{-\frac x{10}}\right|_{x=5}^{x=15} = e^{-\frac 12}-e^{-\frac 32} \approx 0.3834.$\\

\noindent (c) $P(-5<X<10) = \int_{-5}^{10}f(x)\,dx = \int_{-5}^0 0\,dx + \int_0^{10}\frac 1{10}e^{-\frac x{10}}\,dx = P(X\le 10)=.6321\dots$ from part (a).
Just notice that $(-5,10)$ crosses over into the part of the domain where the density vanishes.




\newpage


\includegraphics*[-95,0][350,180]{exp-point11.jpg}
\begin{center}{\bf Figure.} Graph of the exp($\frac 1{10}$) pdf of the example.\label{exppdfpic}\end{center}

\vskip .5 in



\noindent {\bf Indicator notation for piecewise functions.}\\
Probability density functions more often than not are piecewise defined, i.e., the functions change their rules over different portions of the real line.
The exp$(\lambda)$ pdf is one example: the pdf equals $\lambda e^{-\lambda x}$ for $x\ge 0$, but $=0$ for $x<0$.  On the previous page I decided to write
this pdf like this:
$$f(x) = \left\{ \begin{array}{cl} \lambda e^{-\lambda x} & \mbox{for }x\ge 0 \\ 0 & \mbox{for }x<0 \end{array} \right..$$
But, there is a rather clever alternate way to write this pdf using {\bf \em indicator function notation}, which I now describe.\\

\noindent Let $A$ be a set.  Define the {\bf\em indicator function}\label{indicatorfunction} as
$${\bf 1}_A(x) =\left\{ \begin{array}{cl} 1 & \mbox{for }x\in A \\ 0 & \mbox{for }x\not\in A\end{array} \right..$$
With this notation, we can write our exp$(\lambda)$ pdf {\em succinctly} like this:
$$f(x) = \lambda e^{-\lambda x}{\bf 1}_{[0,\infty)}(x).$$
The presence of the indicator function just tells us that if $x\in [0,\infty)$,
it multiplies by 1 and we get $\lambda e^{-\lambda x}$; and, if $x\not\in [0,\infty)$, it multiplies by 0 and we just get the {\em zero} function.\\

\noindent We have a teaching assistant who fancies using this notation, so you may see it used often in certain circles, but I tend to use the notation on the previous page usually
spelling out the piecewise definitions and just keeping track of the domains of definition especially in the univariate case (i.e., just one rv case).  Nevertheless, there is a benefit to using this notation, especially when we start working
with joint pdfs (more than one rv), and we may revisit the use of this notation later in the course.\\


\newpage



\noindent {\bf Exercise for the student.}\\
Let $X\sim \mbox{exp}(\lambda)$.\\
(a) Show that for {\em any $t>0$}, $P(X>t) = e^{-\lambda t}$.\\
(b) Show the exponential distribution has this peculiar property:

the {\bf\em memoryless property}\label{memorylessexp}:

$$\mbox{For any }s,t>0, P(X>s+t|X>s) = P(X>t).$$
\noindent In words this property says: the probability a component survives an additional $t$ units of time given it has survived $s$ units of time
{\em is the same as} the probability a ``new" component survives $t$ unit of time, i.e., {\em the component has no memory of its age!}\\



\newpage





\noindent {\bf Expected values of continuous random variables.}\label{expectedvaluecontinuous}\\
\noindent If $X$ is a continuous random variable with pdf $f(x)$, we define the {\bf\em expected value} to be
$$E(X) = \int_{-\infty}^{\infty}xf(x)\,dx$$
when it exists. Recall the discussion on the existence of expected values on page \pageref{expectedvaluesexistence}:
$E(X)$ exists when $E(|X|) = \int_{-\infty}^{\infty}|x|f(x)\,dx$ is finite.\\

\vskip .5 in

\noindent {\bf Advice:}\\
The integration in this expected value need only be done over the support of the pdf because the pdf (and, hence, the integrand) will vanish for values not in the support.\\

\vskip .5 in

\noindent {\bf Remark.}\\
A classic example of a continuous random variable whose expected value doesn't exist is a rv having the {\bf (standard)} {\bf\em Cauchy pdf}\label{Cauchypdf}:
$$f(x) = \dfrac{1}{\pi(1+x^2)}\quad\mbox{for }-\infty < x < \infty.$$
In this case
$$E(X^+) = E(X^-)=\frac 1{\pi}\int_0^{\infty}x\cdot\frac 1{1+x^2}\,dx = \frac 1{2\pi}\left(\left. \ln(1+x^2) \right|_{x=0}^{\infty} \right)=+\infty.$$



\newpage





\noindent {\bf Example.}\\
Suppose $X\sim\mbox{exp}(\lambda)$. Compute $E(X)$.\\

\noindent SOLUTION:\\
Recall the pdf of the exp$(\lambda)$ is
$\displaystyle f(x) = \left\{ \begin{array}{cl} \lambda e^{-\lambda x} & \mbox{for }x\ge 0 \\ 0 & \mbox{elsewhere} \end{array} \right..$ The support
of this pdf is $[0,\infty)$. Therefore,
\begin{eqnarray*}
\mu=E(X) &=& \int_{-\infty}^{\infty}xf(x)\,dx \\
&=& \int_{-\infty}^{0}x\cdot 0\,dx + \int_{0}^{\infty}x\cdot \lambda e^{-\lambda x}\,dx \quad (\mbox{only need integrate over support of $f$})\\
&=& \lambda \int_{0}^{\infty}\underbrace{x}_{=:u} \underbrace{e^{-\lambda x}\,dx}_{=:dv}\qquad\quad(\mbox{use an integration by parts})\\
&=& \underbrace{\left. -xe^{-\lambda x}\right|_{x=0}^{\infty}}_{=0} - \int_0^{\infty}-e^{-\lambda x}\,dx\\
&=& \int_0^{\infty}e^{-\lambda x}\,dx \\
&=& \left. - \frac {e^{-\lambda x}}{\lambda}\right|_{x=0}^{\infty} = \frac 1{\lambda}.\\
\end{eqnarray*}
We've showed that the {\bf\em mean of an exp$(\lambda)$ is the reciprocal of $\lambda$}: $\mu=\frac {1}{\lambda}$.




\vskip 1 in


\noindent The law of the unconscious statistician that we learned for discrete rvs also holds true for continuous rvs.
A proof, however, is a bit beyond the scope of this course.\\

\noindent {\bf Law of the Unconscious Statistician (LOTUS) - continuous case}\label{lotuscontinuous}\\
Suppose $X$ is a continuous random variable with pdf $f(x)$ and $g=g(x)$ is a function. Then
$$E[g(X)] = \int_{-\infty}^{\infty} g(x)f(x) \,dx,$$
assuming this expected value exists.\\


\newpage





\noindent {\bf Example.} (the {\bf\em uniform}$(a,b)$ {\bf\em distribution})\label{d:uniformabdist}\\
Let $-\infty < a<b<\infty$. A continuous rv $X$ is said to be {\bf\em uniformly distributed on the interval} $[a,b]$ if
the pdf of $X$ is given by
$$f(x) = \left\{\begin{array}{cl} \dfrac 1{b-a} & \mbox{for }a\le x\le b \\ 0 & \mbox{elsewhere} \end{array}\right.,$$
or, using the {\em indicator notation}: $f(x) =\frac 1{b-a}{\bf 1}_{[a,b]}(x)$.  This distribution is the continuous analog of equally likely: drawing an observation from
the interval $[a,b]$ equally likely at random.  Please compute the mean and variance of the uniform$(a,b)$.\\

\noindent SOLUTION:
\begin{eqnarray*}
E(X) &=& \int_{-\infty}^{\infty} xf(x)\,dx = \int_a^b x\cdot \frac 1{b-a}\,dx = \left.\frac 1{b-a}\cdot \frac {x^2}2\right|_{x=a}^{x=b} \\
&=& \frac {b^2-a^2}{2(b-a)} = \frac {a+b}2,\end{eqnarray*}
and using LOTUS,
\begin{eqnarray*}
E(X^2)&=&\int_a^bx^2\frac 1{b-a}\,dx = \left.\frac 1{b-a}\cdot \frac {x^3}3\right|_{x=a}^{x=b}\\
&=& \frac {b^3-a^3}{3(b-a)}=\frac {(b-a)(b^2+ab+a^2)}{3(b-a)}=\frac {b^2+ab+a^2}{3}.\end{eqnarray*}
Finally,
$$Var(X) = E(X^2)-\big(E(X)\big)^2 = \frac {b^2+ab+a^2}{3}-\big(\frac {a+b}2\big)^2 = \cdots = \frac {(b-a)^2}{12},$$
the basic algebra involved is left to the reader.\\


\includegraphics*[-40,0][350,200]{uniformabpdf1.jpg}
\begin{center}{\bf Figure.} Graph of the pdf of a uniform$(a,b)$. Mean is the midpoint of the interval $[a,b]$, $\mu=\frac {a+b}2$,
which should not be surprising -- it's exactly where you'd think the fulcrum of the see-saw should go!\end{center}


\newpage

\noindent {\bf Moment-generating functions - revisited}\label{mgf-part2}\\
For some pdfs, the moment generating function will exist, and when it does it can be very useful.\\

\noindent Recall the definition from page \pageref{mgf-part1}:\\
\noindent If the following expected value thought of as a function of $\theta$:
$$M(\theta) = M_{X}(\theta) = E(e^{\theta X})$$
exists and is finite for $\theta$ in an open neighborhood of $\theta=0$, i.e., $E(e^{\theta X})<\infty$ for $\theta\in (-\varepsilon,\varepsilon)$ for
some $\varepsilon >0$, then we call $M(\theta)$ the {\bf \em moment-generating function (MGF) of $X$} or {\bf\em of its distribution}.\\

\vskip .5 in

\noindent {\bf Example.}\\
Compute the moment generating function of $X\sim \mbox{exp}({\lambda}$).\\

\noindent SOLUTION:\\
\begin{eqnarray*}
M(\theta) & = & E\big(e^{\theta X}\big)\\
&=& \int_0^{\infty} e^{\theta x}\cdot\lambda e^{-\lambda x}\,dx \\
&=& \lambda \int_0^{\infty} e^{-(\lambda -\theta) x}\,dx \qquad\quad (\mbox{important to have $\theta < \lambda$ in this step!})\\
&=& \left. -\frac {\lambda}{\lambda-\theta} e^{-(\lambda -\theta) x}\right|_{x=0}^{\infty} \\
&=& \frac {\lambda}{\lambda-\theta},
\end{eqnarray*}
and, sometimes, it is convenient to write this as
$$M(\theta) = \Big(1 - \frac {\theta}{\lambda}\Big)^{-1}.$$
It is important to reiterate that this MGF is only defined for values of $\theta < \lambda$,
but since $\lambda >0$ this still provides us an open neighborhood of $\theta=0$
where $M(\theta)$ exists and is finite. So this is the MGF of the exp$(\lambda)$ distribution.\\


\includegraphics*[-10,0][400,100]{mgfdomainexplambda.jpg}

\newpage


\noindent {\bf Remark.} (The MacLaurin expansion of the MGF)\label{maclaurinexpofmgf}\\
Recall the MacLaurin expansion
$$e^{\theta X} = 1 + \theta X + \frac {\big(\theta X\big)^2}{2!}+ \frac {\big(\theta X\big)^3}{3!}+ \frac {\big(\theta X\big)^4}{4!} + \cdots.$$
Taking the expected value on both sides and using linearity of expectation, it follows
$$M(\theta) = 1 + E(X)\theta + E(X^2)\frac {\theta^2}{2!} + E(X^3)\frac {\theta^3}{3!} + E(X^4)\frac {\theta^4}{4!} + \cdots,$$
and we see the $k$th moment $E(X^k)$ is really just the coefficient of the $\frac {\theta^k}{k!}$-term in this MacLaurin expansion.\\

\vskip .5 in


\noindent {\bf Remark.}(General formula for the moments of an exp$(\lambda)$) \label{momentsexp}\\
From the geometric series formula, if $\left|\frac {\theta}{\lambda}\right|<1$, then the MGF of the exp$(\lambda)$ can be written as
\begin{eqnarray*}
M(\theta)&=&\Big(1 - \frac {\theta}{\lambda}\Big)^{-1} = 1 + \frac {\theta}{\lambda} + \left( \frac {\theta}{\lambda} \right)^2 + \left( \frac {\theta}{\lambda}\right)^3 +
\left( \frac {\theta}{\lambda}\right)^4 + \cdots\\
&=& 1 + \frac 1{\lambda}\theta + \frac {2!}{\lambda^2}\frac {\theta^2}{2!} + \frac {3!}{\lambda^3}\frac {\theta^3}{3!} + \frac {4!}{\lambda^4}\frac {\theta^4}{4!} + \cdots.
\end{eqnarray*}
On the other hand, from the previous remark
$$M(\theta) = 1 + E(X)\theta + E(X^2)\frac {\theta^2}{2!} + E(X^3)\frac {\theta^3}{3!} + E(X^4)\frac {\theta^4}{4!} + \cdots.$$
So, by matching coefficients of like-powers of $\theta$, we see when $X\sim \mbox{exp}(\lambda),$
$$E(X^k) = \dfrac {k!}{\lambda^k}.$$
In particular, we immediately get
$$E(X) = \frac 1{\lambda},\quad E(X^2) = \frac 2{\lambda^2} \quad\implies\quad Var(X)=\frac 2{\lambda^2}-\left(\frac 1{\lambda}\right)^2=\frac 1{\lambda^2}.$$


\newpage
\noindent {\bf Other moment-related quantities.}\label{othermomemtquantities}\\

\noindent {\bf The $z$-score.} \label{d:z-score}\\
Suppose $X$ is a random variable with mean $\mu$ and standard deviation $\sigma$.\\
\noindent We define the $z$-score of $X$ to be
$$Z:=\dfrac {X-\mu}{\sigma},$$
and it measures the number of standard deviations $X$ is away from its mean.\\
Note that the numerator and denominator are both in units of the random variable and, therefore, the $z$-score is a dimensionless quantity - useful for comparing different random variables and distributions.

\vskip .5 in


\noindent {\bf Centered moments.}\label{centredmoments}\\
If $X$ is a random variable with mean $\mu$, then $E\big((X-\mu)^k\big)$ is called the $k$th centered (or central) moment.
The variance $\sigma^2=Var(X)$ is the 2nd centered moment.

\vskip .5 in

\noindent {\bf Remark.}\\
\noindent I've mentioned that the moments of a probability distribution reveal information about the distribution -
the more moments we know, the more information we have about the distribution. The 1st moment is the mean and tells us where the distribution is
{\em located} on the real line, the 2nd centered moment is the variance and tells us how spread out the values of the random variable are from its mean.
There are two other measures related to higher moments that I now want to tell you about. They are the {\bf\em skewness} of the distribution and
the {\bf\em kurtosis} of the distribution.

\vskip .5 in

\noindent {\bf Skewness and Kurtosis.}\label{skewnessandkurtosis}\\
Let $X$ be a random variable with mean $\mu$ and standard deviation $\sigma$.
We define the {\bf\em skewness} of the distribution of $X$ by
$$\tilde{\mu}_3 := E\left( \left( \frac {X-\mu}{\sigma}\right)^3\right),$$
and is a measure of asymmetry in the distribution;
we define the {\bf\em kurtosis}$^*$ of the distribution of $X$ by
$$\tilde{\mu}_4 := E\left( \left( \frac {X-\mu}{\sigma}\right)^4\right),$$
and is a measure of how ``heavy" the ``tails" of the distribution are.\\
$^*$ The kurtosis is usually a value which is compared to that of the normal distribution
which has kurtosis equal to 3. In fact, some authors define the kurtosis instead by $\tilde{\mu}_4-3$, i.e., subtracting 3 from the quantity given,
which, when positive, tells us the tails are heavier than that of a normal distribution, and, when negative, that tails are lighter than that of a normal
distribution.

\newpage


\noindent {\bf Remark.} (skewness vs.~actual symmetry)\\
The concept of symmetry and the measure we are calling skewness are {\em different}.
A continuous random variable $X$ with pdf $f$ is said to be {\bf\em symmetric} if the graph of the pdf is symmetric about some line $x=c$, i.e.,
if there is a constant $c$ such that
$$f(c-x) = f(c+x)\quad \mbox{for all }x\ge 0.$$
If the pdf has a mean $\mu$ that exists and is finite, then the value of $c$ above is $\mu$.
However, note that the Cauchy pdf on page \pageref{Cauchypdf} is symmetric
because $f(0+x)=f(0-x)$ for all $x\ge 0$, but the Cauchy has no mean and, therefore, $\tilde{\mu}_3$ is not defined for the Cauchy!
Here are pictures of some symmetric pdfs:\\
\includegraphics*[-10,0][450,180]{symmetricpdfs.jpg}

\noindent It is a fact that if a pdf is symmetric and possesses a third moment,
then the skewness $\tilde{\mu}_3 = 0$. Can you prove this?  However, there are pdfs with $\tilde{\mu}_3=0$ that are {\em not} symmetric.\\

\vskip .5 in

\noindent {\bf Exercise for the student.}\\
For the exp$(\lambda)$ pdf, verify $\tilde{\mu}_3=2$ and $\tilde{\mu}_4=9$.\\
The skewness $\tilde{\mu}_3=2 >0$ means the exp$(\lambda)$ distribution is {\em positively skewed}
which loosely means the random variable is more likely to take on larger values than smaller values (see the exp$(0.1)$ pdf in the figure on page \pageref{exppdfpic}).
The kurtosis $\tilde{\mu}_4=9$ is greater than 3 which means the exp$(\lambda)$ distribution has heavier tails than the normal distribution.\\







\newpage

\noindent In order to introduce commonly used pdfs in probability,
we need to take this short mathematical digression and introduce Euler's Gamma function.\\

\noindent {\bf Euler's Gamma function.}\label{d:eulergamma}\\
For real $\alpha > 0$, we define the function
$$\Gamma(\alpha) = \int_0^{\infty} u^{\alpha-1}e^{-u}\,du.$$
This function is, in fact, defined for values of all $\alpha$ in the complex plane (with the exception of the zero and negative even integers), but, in this course, we only need
the behavior of this function for positive real values $\alpha$.\\

\vskip .2 in

\noindent Let's compute $\Gamma(\alpha)$ for some values of $\alpha>0$.\\

\noindent $\alpha =1:$
\begin{eqnarray*}
\Gamma(1) &=& \int_0^{\infty}u^{1-1}e^{-u}\,du = \int_0^{\infty}e^{-u}\,du = 1.\end{eqnarray*}

\noindent $\alpha = 2:$
\begin{eqnarray*}
\Gamma(2)
&=& \int_0^{\infty}u^{2-1}e^{-u}\,du \\
&=& \int_0^{\infty}ue^{-u}\,du \\
&=& \underbrace{\left.-ue^{-u}\right|_{u=0}^{\infty}}_{=0}+ \underbrace{\int_0^{\infty} e^{-u}\,du}_{=:\Gamma(1)=1}\qquad \quad(\mbox{used an integration by parts})\\
&=& 1.\end{eqnarray*}

\noindent In fact, when $\alpha>0$:
\begin{eqnarray*}
\Gamma(\alpha+1)
&=& \int_0^{\infty}u^{\alpha}e^{-u}\,du \\
&=& \underbrace{\left.-u^{\alpha}e^{-u}\right|_{u=0}^{\infty}}_{=0}+ \alpha \underbrace{\int_0^{\infty} u^{\alpha-1}e^{-u}\,du}_{=:\Gamma(\alpha)}\qquad \quad(\mbox{used an integration by parts})\\
&=& \alpha \Gamma(\alpha),\end{eqnarray*}
and we arrive at a very useful relationship\dots\\

\noindent {\bf The Reduction formula of the Gamma function.}\label{reductionformula}\\
For $\alpha >0$,
$$\Gamma(\alpha+1)=\alpha \Gamma(\alpha).$$



\newpage


\noindent {\bf Remark.} (Euler's Gamma generalizes the notion of {\em factorial} to positive reals)\\
The reduction formula can also be stated as $\Gamma(\alpha)=(\alpha -1)\Gamma(\alpha - 1)$,
but we would now require $\alpha > 1$ since $\Gamma(\alpha - 1)$ only makes sense when $\alpha-1>0$.  The reduction formula is
very useful because it allows us to compute $\Gamma(x)$ for any positive real $x$ as a function on $\Gamma(y)$, where $y\in (0,1]$:
\begin{eqnarray*}
\Gamma(5) &=& 4\Gamma(4) = 4\cdot 3\Gamma(3) = 4\cdot 3\cdot 2\Gamma(2) = 4\cdot 3\cdot 2\cdot 1\underbrace{\Gamma(1)}_{=1}.\\
&=& 4!\\
\end{eqnarray*}
and, for integer $n\ge 1$,
$$\Gamma(n) = (n-1)!.$$
In this sense, we see Euler's Gamma function agrees with the factorial on integers.  Even more:
$$\Gamma(4.6) = (3.6)(2.6)(1.6)(0.6)\Gamma(0.6),$$
and, if needed, we can look up the value of $\Gamma(0.6)$ in a table.\footnote{Abramowitz \& Stegun, {\em Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, Dover (1965)}.}\\

\vskip .5 in

\noindent {\bf Remark.}\\
We will show a bit later that
$$\Gamma(\frac 12) = \sqrt{\pi}.$$
When we combine this result with the last remark, for instance,
$$\Gamma(\frac 72) = \frac 52\cdot \frac 32\cdot \frac 12\Gamma(\frac 12) = \frac{15\sqrt{\pi}}{8}.$$

\noindent {\bf End of digression.}\\

\vskip .5 in

\noindent {\bf Remark.} (creating pdfs from nonnegative functions with finite integral)\\
If we have any real-valued function $g=g(x)$ defined on the real line which is nonnegative
and whose integral is finite and positive, say,
$$\int_{-\infty}^{\infty}g(x)\,dx = c>0,$$
then, in fact $f(x):=\frac {g(x)}{c}$ will be a pdf -- just divide both sides of the display by $c$.
Many pdfs are created this way: find a nonnegative function whose graph/shape mimics how you want a pdf to behave, then just normalize it to have a total integral equal to 1.



\newpage

\noindent Following the last remark, let's consider the function
\begin{eqnarray*}
g(x) &=& x^{\alpha -1}e^{-x/\beta} \quad \,\mbox{for }x>0\\ & = & 0 \qquad \qquad \quad \mbox{for }x\le 0,\end{eqnarray*}
where $\alpha >0$ and $\beta >0$ are fixed constants.\\

\noindent The support of this function is $[0,\infty)$ and its integral is
\begin{eqnarray*}
\int_0^{\infty} g(x)\,dx & = & \int_0^{\infty}x^{\alpha -1}e^{-x/\beta}\,dx\qquad (\mbox{make substitution }u=x/\beta)\\
&=& \int_0^{\infty} (\beta u)^{\alpha-1}e^{-u}\,\beta\,du \\
&=&\beta^{\alpha}\int_0^{\infty} u^{\alpha -1}e^{-u}\,du \\
&&\\
&=& \beta^{\alpha}\Gamma(\alpha).
\end{eqnarray*}

\bigskip

\noindent Therefore, we have \\

\noindent {\bf The Gamma$(\alpha,\beta)$ pdf.}\label{gammapdf}\\
The function
$$f(x) = \displaystyle \left\{\begin{array}{cl} \dfrac {x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)} & \mbox{for }x>0\\  & \\
0 & \mbox{for }x\le 0  \end{array}\right.$$
is a pdf.  In this context, the parameter $\alpha$ is called the {\bf\em shape}\label{gammashape} parameter and $\beta$ is called the {\bf\em scale}\label{gammascale} parameter.\\


\includegraphics*[-80,0][450,220]{gammapdfs1.jpg}
\begin{center}
{\bf Figure.} Top picture: scale fixed at $\beta=1$ and shape $\alpha$ changes from $\frac 12$ to $1$ to $2$.  Bottom picture:
shape fixed at $\alpha=2$ and scale $\beta$ changes from $\frac 12$ to $1$ to $2$.
\end{center}




\newpage

\noindent As the figure illustrates, if we fix the scale parameter, then changing the shape parameter changes the essential shape of the graph: when $0<\alpha <1$
the graph has a vertical asymptote at $x=0$; when $\alpha=1$, the graph is an exp$(\frac 1{\beta})$ pdf taking the value $\frac 1{\beta}$ at $x=0$; when $\alpha > 1$, the functions take the value 0 at $x=0$.\\

\noindent If we fix the shape parameter, then, as the scale parameter increases (decreases), the graph of the function gets stretched out (scrunched in). I.e., $\beta$ genuinely acts as a scale parameter should: when it increases, we increase the scale; when it decreases, we decrease the scale.\\

\noindent {\bf Remark.} (Inconsistencies among various definitions of the Gamma distribution)\\
Some authors prefer to use a ``rate" parameter $\lambda$ instead of the scale parameter $\beta$ when defining their Gamma distribution.
These authors define their Gamma$(\alpha,\lambda)$ pdf like this:
$$f(x) = \left\{\begin{array}{cl}\frac {\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)}&\mbox{for }x>0\\0&\mbox{for }x\le 0\end{array}\right.,$$
and, in this form, when $\alpha \in {\mathbb Z}_+$, it's also called the {\bf\em Erlang distribution}\label{erlangdist}.\\

\noindent But notice when $\alpha=1$, the Gamma$(1,\lambda)$ reduces to the pdf $f(x) = \lambda e^{-\lambda x}$ for $x>0$, i.e., the Gamma$(1,\lambda)\equiv \mbox{exp}(\lambda)$;
whereas, by {\em our} definition,
a Gamma$(1,\frac 1{\lambda})\equiv \mbox{exp}(\lambda)$. So take note: what these authors are calling a Gamma$(\alpha,\lambda)$ is really what we are calling a Gamma$(\alpha,\frac 1{\lambda})$, and, what we're calling a Gamma$(\alpha,\beta)$, these other authors would be calling a Gamma$(\alpha,\frac 1{\beta})$.  Know whether or not you are dealing with a person who uses a rate parameter instead of a scale parameter in the definition of their Gamma!\\

\includegraphics*[-80,0][280,200]{gammapdf2.jpg}
\begin{center}{\bf Figure.}\label{gammapdfsfigure} Some more graphs of Gamma$(\alpha,\beta)$ densities for various choices of $\alpha$ and $\beta$.\end{center}

\noindent Surprisingly, we will not be interested much in finding areas under these curves; although, it can be done by numerical integration methods such as trapezoidal rule or Simpson's rule if needed.  Instead, in this course, we'll be much more interested in the {\em global behavior} of the Gamma distribution - the mean, variance, moments, and MGF.\\

\newpage

\noindent {\bf Normalization ``tricks".}\label{normalizationtricks}\\

\noindent Before we investigate the global behavior of the Gamma$(\alpha,\beta)$ distribution,
I wish to introduce a {\bf\em normalization trick}
which will allow us to {\em quickly} compute integrals by
recognizing we are integrating the functional form of a known pdf over its entire support.
Although I will introduce the idea with the Gamma$(\alpha,\beta)$ pdf, the
idea extends to virtually any pdf (and pmf) we work with in this course.\\

\noindent Here's the idea for the Gamma$(\alpha,\beta)$ distribution.  For $\alpha>0$ and $\beta>0$, we know
$$\int_0^{\infty} \dfrac {x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\,dx = 1,$$
because pdfs integrate to 1 over their entire support.
$\beta^{\alpha}\Gamma(\alpha)$ is the normalizing constant, so, when multiplied to the other side, we obtain the
{\bf\em extremely useful fact:}\\
$$\int_0^{\infty} x^{\alpha-1}e^{-x/\beta}\,dx = \beta^{\alpha}\Gamma(\alpha).$$
In words: when we integrate the functional form of the Gamma$(\alpha,\beta)$ density, i.e.,
$x^{\alpha-1}e^{-x/\beta}$, over its entire support $(0,\infty)$ we get the
normalizing constant, $\beta^{\alpha}\Gamma(\alpha)$:\\
{\em mnemonic}: ``scale to the shape times Gamma of shape".

\vskip .25 in

\noindent {\bf Examples.}\\
Compute each of the following integrals:\\

\noindent (a) $\displaystyle \int_0^{\infty} x^3e^{-x/2}\,dx$\\

\noindent (b) $\displaystyle \int_0^{\infty} x^{\frac 12}e^{-2x}\,dx$\\

\noindent (c) For $n\in {\mathbb Z}_+$, simplify $\displaystyle \int_{0}^{\infty} x^ne^{-nx}\,dx$.\\

\medskip

\noindent SOLUTIONS:\\

\noindent (a) $\displaystyle \int_0^{\infty} \underbrace{x^3e^{-x/2}}_{\alpha=4,\ \beta=2}\,dx = 2^4\Gamma(4) = 16(3!) = 96.$\\

\noindent (b) $\displaystyle \int_0^{\infty} \underbrace{x^{\frac 12}e^{-2x}}_{\alpha=\frac 32,\ \beta=\frac 12}\,dx = \Big(\frac 12\Big)^{\frac 32}\Gamma(\frac 32)=
\Big(\frac 12\Big)^{\frac 32}\frac 12\underbrace{\Gamma(\frac 12)}_{\sqrt{\pi}}=\Big(\frac 12\Big)^{\frac 52}\sqrt{\pi}=\frac 14\sqrt{\frac {\pi}{2}}.$\\

\noindent (c) I get $\frac {(n-1)!}{n^n}$\dots check my work!\\
\vskip .5 in

\noindent {\bf Exercise for the student.}\\

Compute $\displaystyle \int_0^{\infty}\frac {e^{-x/2}}{\sqrt{x}}\,dx$.\\


\newpage

\noindent {\bf Moments of the Gamma$(\alpha,\beta)$.}\\

\noindent With the normalization trick and the basic properties of Euler's Gamma function we are now ready to compute the moments of the Gamma$(\alpha,\beta)$.
We'll only compute the first two moments of this distribution for now.\\

\vskip .5 in

\noindent {\bf The mean and variance of $X\sim$\,Gamma$(\alpha,\beta)$:}\\
\begin{eqnarray*}
E(X) & = & \int_0^{\infty}x\cdot \frac {x^{\alpha - 1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\,dx\\
 & = & \frac 1{\beta^{\alpha}\Gamma(\alpha)}\int_0^{\infty}\underbrace{\ x^{\alpha }e^{-x/\beta}\ }_{\stackrel{\mbox{\tiny shape=}\alpha+1,}{\mbox{\tiny scale=}\beta}}\,dx\\
 & = & \frac 1{\beta^{\alpha}\Gamma(\alpha)}\cdot \beta^{\alpha+1}\Gamma(\alpha+1) = \frac {\beta^{\alpha+1}\alpha\Gamma(\alpha)}{\beta^{\alpha}\Gamma(\alpha)} = \alpha\beta.\\
\end{eqnarray*}

\bigskip

\begin{eqnarray*}
E(X^2) & = & \int_0^{\infty}x^2\cdot \frac {x^{\alpha - 1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\,dx\\
 & = & \frac 1{\beta^{\alpha}\Gamma(\alpha)}\int_0^{\infty}\underbrace{\ x^{\alpha+1}e^{-x/\beta}\ }_{\stackrel{\mbox{\tiny shape=}\alpha+2,}{\mbox{\tiny scale=}\beta}}\,dx\\
 & = & \frac 1{\beta^{\alpha}\Gamma(\alpha)}\cdot \beta^{\alpha+2}\Gamma(\alpha+2) = \frac {\beta^{\alpha+2}(\alpha+1)\alpha\Gamma(\alpha)}{\beta^{\alpha}\Gamma(\alpha)} = \alpha(\alpha+1)\beta^2.\\
\end{eqnarray*}

\bigskip

\noindent From here, the variance is immediate:
$$Var(X) = E(X^2) - \big(E(X)\big)^2 = \alpha(\alpha+1)\beta^2 - \alpha^2\beta^2 = \alpha\beta^2.$$

\vskip .5 in

\noindent {\bf Result:}

If $X\sim \mbox{Gamma}(\alpha,\beta)$, then
$$\mu_X=E(X) = \alpha\beta \ \ \mbox{\em and}$$
$$\sigma_X^2=Var(X) = \alpha\beta^2.$$

\newpage

\noindent {\bf Moment-generating function of $X\sim$\,Gamma$(\alpha,\beta)$.}\\

\begin{eqnarray*}
M(\theta)
&=& E(e^{\theta X})\\
&=& \int_0^{\infty}e^{\theta x}\cdot \frac {x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\,dx\\
&=& \frac 1{\beta^{\alpha}\Gamma(\alpha)} \int_0^{\infty}x^{\alpha-1}e^{-x/\beta + \theta x}\,dx\\
&=& \frac 1{\beta^{\alpha}\Gamma(\alpha)} \int_0^{\infty}\underbrace{\ x^{\alpha-1}e^{-x\left(\frac 1{\beta} - \theta\right)}\,}_{\stackrel{\mbox{\tiny shape=}\alpha}{\mbox{\tiny scale=}\left(\frac 1{\beta}-\theta\right)^{-1}}}dx \qquad (\mbox{need }\theta < \frac 1{\beta})\\
&=& \frac 1{\beta^{\alpha}\Gamma(\alpha)} \left(\Big(\frac 1{\beta}-\theta\Big)^{-1}\right)^{\alpha}\Gamma(\alpha)\\
& & \\
&=& (1-\beta\theta)^{-\alpha}.\\
\end{eqnarray*}


\vskip .5 in

\noindent {\bf Result:}

If $X\sim \mbox{Gamma}(\alpha,\beta)$, then
$$M_X(\theta) = (1-\beta\theta)^{-\alpha}.$$


\vskip .5 in

\noindent {\bf Example.}\\
Use this MGF to compute the first two moments of the Gamma$(\alpha,\beta)$.\\

\noindent SOLUTION:\\
From the properties of the MGF on page \pageref{mgfproperties1} Fact 3, we can take derivatives of the MGF with respect to its argument $\theta$ and then evaluate at $\theta=0$ to recover the moments of the distribution.\\

$M(\theta) = (1-\beta\theta)^{-\alpha}$\\

$M^{\prime}(\theta) = -\alpha\cdot (1-\beta\theta)^{-\alpha-1}\cdot (-\beta) = \alpha\beta(1-\beta\theta)^{-(\alpha+1)}$

$\implies M^{\prime}(0)=\alpha\beta = E(X).$\\

$M^{\prime\prime}(\theta)=-(\alpha+1)\alpha\beta(1-\beta\theta)^{-(\alpha+2)}(-\beta)=
\alpha(\alpha+1)\beta^2 (1-\beta\theta)^{-(\alpha+2)}$

$\implies M^{\prime\prime}(0) = \alpha(\alpha+1)\beta^2 = E(X^2).$



%\newpage


%\noindent {\bf Application: The Poisson process.}\label{poissonprocess1}\\






\newpage

\noindent {\bf The Normal distribution (a.k.a.~the Gaussian distribution).}\label{normaldist}\\

\noindent $X\sim \mbox{N}(\mu,\sigma^2)$ means the continuous rv $X$ has the pdf
$$f(x) = \dfrac {e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\quad \mbox{for }-\infty < x < \infty.$$
The parameters in this pdf, $\mu$ and $\sigma^2$, will end up being the mean and the variance, respectively -- so it is only natural to use these symbols for the parameters (see the remark on page \pageref{normalmeanvar}).  This pdf is one of a few pdfs in this
course whose support is the {\em entire} real line.  Another we've talked about was the {\em Cauchy distribution} (page \pageref{Cauchypdf}).\\

\noindent Like the Gamma$(\alpha,\beta)$ we'll be interested in the global behavior of these pdfs; However,
unlike the Gamma$(\alpha,\beta)$, it will be quite important to be able to find areas under these pdfs -- and this will
require the use of a statistical table or a computing device programmed to find these areas -- and, this will come soon.\\

\noindent Here are some plots\footnote{Figure modified from https://en.wikipedia.org/wiki/Normal\underline{ }distribution.}
of the N$(\mu,\sigma^2)$ pdf for various choices of $\mu$ and $\sigma^2$:

\includegraphics*[-40,0][340,210]{normalpdfs1.jpg}
\begin{center}
{\bf Figure.} Some plots of Normal pdfs; the red one is the {\em standard normal pdf}.
\end{center}

\vskip .2 in

\noindent {\bf Remark.}\\
The Normal distribution is important for several reasons, but largely because
it happens to be (roughly speaking)
the limiting distribution of
large sums of independent random variables.  We'll see, for instance, that

$\bullet$ for {\em fixed} $p$ and {\em large} $n$, binom$(n,p)\approx \mbox{N}(np,np(1-p))$,

$\bullet$ for {\em large} $\lambda$, Poisson$(\lambda)\approx \mbox{N}(\lambda,\lambda)$,

$\bullet$ for {\em fixed} $\beta$ and {\em large} $\alpha$, Gamma$(\alpha,\beta)\approx \mbox{N}(\alpha\beta, \alpha\beta^2)^{\dagger}$,\\
\noindent and {\em much much more!}\\

\noindent $^{\dagger}$ Indeed, look at the Gamma densities with $\alpha=9$ and $\alpha=7.5$ on page \pageref{gammapdfsfigure} and you can see these are already starting to look like Normal pdfs!


\newpage

\noindent {\bf Remark.}\\
We should probably show that the Normal pdf given on page \pageref{normaldist} is {\em actually} a pdf; i.e., it's
{\em nonnegative} on the entire real line {\em and} the total integral is 1.  The clever idea in the proof is
due to C.F.~Gauss
and is one of the reasons this distribution is often called the {\bf \em Gaussian distribution}. \\

\vskip .2 in

\noindent {\bf Proof that the N$(\mu,\sigma^2)$ pdf {\em is} a pdf.}\label{normalintegratesto1}\\
\noindent Since $e^u>0$ for any real $u$, $f(x)>0$ for $x\in {\mathbb R}$.  To show ${\cal I}:=\int_{-\infty}^{\infty}f(x)\,dx = 1$,
Gauss showed that ${\cal I}^2=1$, from which it follows ${\cal I}=\pm 1$ and we can rule out $-1$ since $f(x)>0$ everywhere.
\begin{eqnarray*}
{\cal I}^2
&=& \left(\int_{-\infty}^{\infty} f(x)\,dx\right)^2 \\
&=& \int_{-\infty}^{\infty} f(x)\,dx\,\int_{-\infty}^{\infty} f(y)\,dy \\
&=& \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} f(x)f(y)\,dxdy \\
&=& \frac 1{2\pi\sigma^2}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-\frac 12\left(\frac {x-\mu}{\sigma}\right)^2}\cdot e^{-\frac 12\left(\frac {y-\mu}{\sigma}\right)^2}\,dxdy \quad\left( u=\frac {x-\mu}{\sigma}, \ v=\frac {y-\mu}{\sigma}\right) \\
&=& \frac 1{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-\frac 12u^2}\cdot e^{-\frac 12v^2}\,dudv\\
&=& \frac 1{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-\frac 12(u^2+v^2)}\,dudv\quad \left(\mbox{polar coordinates: } r^2=u^2+v^2, \ dudv=rdrd\theta\right)\\
&=& \frac 1{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty} r e^{-\frac {r^2}2}\,drd\theta\\
&=& \frac 1{2\pi}\int_{0}^{2\pi}\left(\left. -e^{-\frac {r^2}2}\right|_{r=0}^{\infty}\right)\,d\theta\\
&=& \frac 1{2\pi}\int_{0}^{2\pi}1\,d\theta \\
& & \\
&=& 1.\\
\end{eqnarray*}

\vskip .5 in


\noindent As we did with the Gamma$(\alpha,\beta)$ distribution,
we will first focus on deriving {\em global} properties of the Normal distribution.




\newpage


\noindent {\bf Properties of the N$(\mu,\sigma^2)$ distribution.}\label{propertiesofnormal}\\

\noindent {\bf Theorem.} (An affine transformation of a Normal is, again, a Normal)\\
Let $X\sim \mbox{N}(\mu,\sigma^2)$, and let $a,b$ be any constants with $a\ne 0$.  Then
$$aX + b \sim \mbox{N}(a\mu+b,a^2\sigma^2).$$

\vskip .5 in

\noindent {\bf Remark.}\\
The theorem is also true when $a=0$ if we interpret the
N$(a\mu+b,a^2\sigma^2)=\mbox{N}(b,0)$ distribution
as a point mass of 1 centered at $b$.
There are several techniques that probabilists use to find
the distribution of functions of continuous random variable(s),
but for the above theorem I will foreshadow a bit and apply the {\bf\em CDF method}.\label{cdfmethod}\\

\noindent {\bf Outline of the CDF method.}\label{cdfmethod1}\footnote{we will learn the CDF method in more detail later. See page \pageref{cdfmethod}.}\\
Suppose $Y=g(X)$, where $X$ is a continuous rv having {\em known} CDF $F_X(x)$.
The main idea in this method is to find the CDF of $Y$ in terms of the CDF of $X$, then
once we've found the CDF of $Y$, we know its derivative is the pdf of $Y$.\\

\noindent Proof.\\
\noindent Let $Y=aX+b$ and first assume $a>0$. The case $a<0$ will be an exercise.\\
\noindent Since $X$ can take on any value in ${\mathbb R}$, it follows $Y$ can take on any value in ${\mathbb R}$, too, because $a\ne 0$.
So, let $y\in {\mathbb R}$ be arbitrary.\\
\begin{eqnarray*}
F_Y\!(y) &=& P(Y\le y)\\
&=& P(aX+b\le y)\\
&=& P(aX\le y-b)\\
&=& P\!\left(X\le \frac {y-b}a\right)\qquad (\mbox{notice I used the fact that }a>0\mbox{ here})\\
&=& F_X\!\!\left(\frac {y-b}a\right),\\
\end{eqnarray*}
and we've found the CDF of $Y$ in terms of the CDF of $X$. Now, if we take the derivalive on both sides with respect to $y$, then on the left we get the pdf of $Y$, and on the
right we get (after applying the chain rule):
\begin{eqnarray*}
f_Y\!(y) &=& f_X\!\left(\frac {y-b}{a}\right)\cdot \frac 1a \\
&=& \frac 1{\sqrt{2\pi\sigma^2}}\,e^{-\frac {\left(\frac {y-b}a-\mu\right)^2}{2\sigma^2}}\cdot \frac 1a\\
&=& \frac 1{\sqrt{2\pi a^2\sigma^2}}\,e^{-\frac {(y-b-a\mu)^2}{2a^2\sigma^2}},\end{eqnarray*}
which is the pdf of a N$(a\mu+b,a^2\sigma^2)$ which was to be shown.\\






\newpage

\noindent {\bf Remark.}\\
\noindent In the proof we found that
$F_Y(y) = P\left(X\le \frac {y-b}a\right),$
which can be written equivalently as
$$F_Y(y) = \int_{-\infty}^{\frac {y-b}a} \frac {e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\,dx.$$
Integrals whose bounds (and possibly integrand) involve the variable that
we wish to take derivative against come up often in probability, and the calculus rule we appeal to is called
{\bf\em the Liebniz rule}.\\

\vskip .3 in

\noindent {\bf Calculus fact: Liebniz rule.}\label{liebnizrule}\\
$$\dfrac {d}{dy}\int_{a(y)}^{b(y)} g(x,y)\,dx = g(b(y),y)b'(y) - g(a(y),y)a'(y) + \int_{a(y)}^{b(y)}\dfrac {\partial g(x,y)}{\partial y}\,dx.$$
{\bf\em Some special cases of the Liebniz rule:}\\
$g(x,y)=g(x)$ does not depend on $y$: $\displaystyle \dfrac {d}{dy}\int_{a(y)}^{b(y)} g(x)\,dx = g(b(y))b'(y)-g(a(y))a'(y).$\\
$a(y)=a$ is constant (incl.~$-\infty$), no $y$-dependence in $g$: $\displaystyle \dfrac {d}{dy}\int_{a}^{b(y)} g(x)\,dx = g(b(y))b'(y).$\\
Both bounds are constant: $\displaystyle \dfrac {d}{dy}\int_{a}^{b} g(x,y)\,dx = \int_{a}^{b}\dfrac {\partial g(x,y)}{\partial y}\,dx.$\\


\vskip .5 in

\noindent We can apply the Liebniz rule on $$F_Y\!(y) = \displaystyle \int_{-\infty}^{\textcolor{red}{\frac {y-b}a}} \frac {e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\,dx$$
to recover the pdf of $Y$.  Let's do this now:
\begin{eqnarray*}
\frac d{dy}F_Y\!(y) & = & \frac d{dy}\int_{-\infty}^{\textcolor{red}{\frac {y-b}a}} \frac {e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\,dx\\
 & = & \frac {e^{-\frac {(\textcolor{red}{\frac {\,y-b\ }a}-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\,\sigma^2}}\cdot \underbrace{\frac d{dy}\!\left(\textcolor{red}{\frac {y-b}{a}}\right)}_{=\frac 1a}\\
 & & \\
 &=& \frac 1{\sqrt{2\pi a^2\sigma^2}}\,e^{-\frac {(y-b-a\mu)^2}{2a^2\sigma^2}},\\
\end{eqnarray*}
which confirms (in an equivalent way) what we did on the previous page.


\newpage

\noindent {\bf Exercise for the student.}\\
If $X\sim \mbox{N}(\mu,\sigma^2)$ and $a,b$ constants with $a<0$, show that $Y=zX+b\sim \mbox{N}(a\mu+b,a^2\sigma^2)$.\\
Hint: Because $a<0$, we find that, for real $y$,
$$F_Y\!(y) = P(aX\le y-b) = P(X\ge \frac {y-b}a) = \int_{\textcolor{red}{\frac {y-b}a}}^{\infty}\frac {e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\,dx
 = 1-\int_{-\infty}^{\textcolor{red}{\frac {y-b}a}}\frac {e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\,dx.$$
Now use the Liebniz rule.\\

\vskip .5 in

\noindent After you prove the exercise, we will have shown that, if $X\sim \mbox{N}(\mu,\sigma^2)$, for any constants $a,b$ with $a\ne 0$,
$$aX+b\sim \mbox{N}(a\mu+b,a^2\sigma^2).$$
We have the immediate corollaries:\\

\vskip .5 in

\noindent {\bf Corollary.}\label{normalcor}\\
\noindent (a) If $X\sim \mbox{N}(\mu,\sigma^2)$, then the {\bf\em $z$-score}\label{zscore2} $Z:=\frac {X-\mu}{\sigma}\sim \mbox{N}(0,1)$.\\

\noindent (b) Conversely, if $Z\sim\mbox{N}(0,1)$, then, for any $\mu\in {\mathbb R}$ and $\sigma^2>0$, $\mu+\sigma Z\sim \mbox{N}(\mu,\sigma^2)$.\\

\noindent Proof.\\
\noindent (a) Take $a=\frac 1{\sigma}$ and $b=-\frac {\mu}{\sigma}$. Then $a\mu+b = \frac 1{\sigma}\mu + \left(-\frac {\mu}{\sigma}\right)=0$, and $a^2\sigma^2 = \left(\frac 1{\sigma}\right)^2\sigma^2=1$.\\

\noindent (b) $Z\sim\mbox{N}(0,1)$, so $aZ+\mu\sim$\,N$(\sigma\cdot 0+\mu,\sigma^2(1)^2) = \mbox{N}(\mu,\sigma^2)$.\\

\vskip .3 in

\noindent {\bf Remark.}\\
We call the $\mbox{N}(0,1)$ the {\bf \em standard Normal distribution}\label{stdnormaldist}. Its pdf is
$$\varphi(x) = \dfrac {e^{-\frac 12 x^2}}{\sqrt{2\pi}}\quad\mbox{for }-\infty < x < \infty,$$
and its CDF is
$$\Phi(x) := \int_{-\infty}^x \varphi(u)\,du.$$
The Greek letters $\varphi$ and $\Phi$ are {\em reserved} letters for the standard Normal pdf and CDF, respectively.
Also, for standard Normal rvs we will typically use the capital letter $Z$ or $Z_1,Z_2,\dots$, etc.\\

\noindent The corollary clearly shows that we can transform any Normal distribution into
a {\bf\em standard Normal distribution}, i.e., a N$(0,1)$ distribution, and vice-versa.  Thus, any probability involving a N$(\mu,\sigma^2)$ rv
can be transformed into a probability involving a standard normal rv.  Therefore, if we tabulate the CDF of a standard Normal rv we can use this {\em one table} to tabulate the CDF of every Normal rv.



\newpage

\includegraphics*[-70,0][260,180]{standardnormalpdf.jpg}
\begin{center}
{\bf Figure.} Plot of the standard normal pdf $\varphi(x)$. See also the \textcolor{red}{red} curve on page \pageref{normaldist}.
\end{center}


\vskip .3 in

\noindent {\bf Symmetry of the Normal pdfs.}\label{normalsymmetry}\smallskip

\noindent Notice $\varphi(-x) = \dfrac {e^{-\frac 12 (-x)^2}}{\sqrt{2\pi}}=\dfrac {e^{-\frac 12 x^2}}{\sqrt{2\pi}}=\varphi(x)$, which implies the standard Normal distribution is
a {\bf\em symmetric distribution}, in fact, it is symmetric about $x=0$.\\\medskip
{\em Do you see why this implies $\Phi(0)=\frac 12$ ?}\\

\vskip .3 in

\noindent {\bf Exercise for the student.}\\
Let $Z\sim\mbox{N}(0,1)$.\\
(a) Show that $E(Z)$ exists. Then show $E(Z)=0$.\\
(b) Show $E(Z^2)=1$.\\
(c) If $X\sim\mbox{N}(\mu,\sigma^2)$, then explain why the pdf of $X$ is also symmetric (about $x=\mu$, in fact).\\

\noindent SOLUTION to part (b):\\
Since $Z^2\ge 0$ there is no issue with $E(Z^2)$ not existing; it may be infinite but we can just compute it to see.
\begin{eqnarray*}
E(Z^2) &=& \int_{-\infty}^{\infty}z^2 \, \dfrac {e^{-\frac 12 z^2}}{\sqrt{2\pi}}\,dz\\
&=& \frac 2{\sqrt{2\pi}}\int_{0}^{\infty}z\left(z\,e^{-\frac 12 z^2}\right)\,dz\qquad \left(\begin{array}{ll}u=z, & dv = ze^{-\frac 12z^2}\,dz \\ du=dz & v = -e^{-\frac 12z^2}\end{array}\right)\\
&=& \frac 2{\sqrt{2\pi}}\left(\underbrace{\left. -ze^{-\frac 12z^2}  \right|_{z=0}^{\infty}}_{=0} \right) + \underbrace{\frac 2{\sqrt{2\pi}}\int_{0}^{\infty} e^{-\frac 12 z^2}\,dz}_{=1}\\
&&\\
&=&1.
\end{eqnarray*}
Since $E(Z^2)=1$ and (you showed) $E(Z)=0$, it follows $Var(Z) = E(Z^2)-\big(E(Z)\big)^2=1$.\\





\newpage
\noindent {\bf Remark.} (In a N$(\mu,\sigma^2)$, $\mu$ is the mean and $\sigma^2$ is the variance)\label{normalmeanvar}\\
By the corollary on page \pageref{normalcor} ($Z\sim\mbox{N}(0,1)\implies X=\sigma Z+\mu\sim \mbox{N}(\mu,\sigma^2)$) and,
by the property of expected value on page \pageref{expectedvalueproperties},
$$E(X) = E(\sigma Z+\mu) = \sigma E(Z) + \mu = \sigma\cdot 0 + \mu = \mu,$$
and, by the property of variance on page \pageref{propertyofvariance},
$$Var(X) = Var(\sigma Z+\mu) = \sigma^2Var(Z) = \sigma^2.$$
Therefore, the two parameters defining the $\mbox{N}(\mu,\sigma^2)$ distribution {\em are} {\bf\em the mean} and {\bf\em the variance}.\\


\vskip .5 in

\noindent {\bf Moment-generating function of a standard Normal.}\label{mgfnormal01}\\
Let $Z\sim\mbox{N}(0,1)$. Then the MGF of $Z$ is defined for {\em all} real $\theta$ and
$$M_Z(\theta) = E(e^{\theta Z}) = e^{\frac {\theta^2}2}.$$

\bigskip

\noindent Proof.
\begin{eqnarray*}
M(\theta)
&=& E(e^{\theta Z}) = \frac 1{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{\theta z}\cdot e^{-\frac 12 z^2}\,dz\\
&=& \frac 1{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac 12 (z^2-2\theta z)}\,dz\\
&=& \frac 1{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac 12 (z^2-2\theta z \textcolor{red}{+ \theta^2})+\textcolor{red}{\frac 12\theta^2}}\,dz\quad(\mbox{completed the square})\\
&=& \frac 1{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac 12 (z-\theta)^2}e^{\frac 12\theta^2}\,dz\\
&=& \frac {e^{\frac {\theta^2}2}}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac 12 (z-\theta)^2}\,dz\qquad (u=z-\theta,\ du=dz)\\
&=& e^{\frac {\theta^2}2}\underbrace{\int_{-\infty}^{\infty} \dfrac {e^{-\frac 12 u^2}}{\sqrt{2\pi}}\,du}_{=1} \quad = \quad e^{\frac {\theta^2}2}.\\
\end{eqnarray*}


\newpage


\noindent {\bf Moments of the standard Normal.}\\
Using the same idea we used to get a general formula for the moments of an exp$(\lambda)$ (see page \pageref{momentsexp}), we can find a
general formula for the moments of a standard normal.\\
The MacLaurin expansion for $M_Z(\theta)$ is
\begin{eqnarray*}
e^{\frac {\theta^2}2} &=& 1 + \left(\frac {\theta^2}2\right) + \frac 1{2!}\left(\frac {\theta^2}2\right)^2 + \frac 1{3!}\left(\frac {\theta^2}2\right)^2 + \frac 1{4!}\left(\frac {\theta^2}2\right)^2 + \cdots\\
&=& 1 + \frac {\theta^2}{2} + \frac {\theta^4}{2!2^2} + \frac {\theta^6}{3!2^3} + \frac {\theta^8}{4!2^4} + \cdots\\
&=& 1 + \frac {\theta^2}{2!} + \frac {4!}{2!2^2}\cdot\frac {\theta^4}{4!} + \frac {6!}{3!2^3}\cdot\frac {\theta^6}{6!} + \frac {8!}{4!2^4}\cdot \frac {\theta^8}{8!} + \cdots\\
&=& 1 + \underbrace{0}_{=E(Z)}\cdot \theta + \underbrace{1}_{=E(Z^2)}\cdot \frac {\theta^2}{2!} + \underbrace{0}_{=E(Z^3)}\cdot \frac {\theta^3}{3!} + \underbrace{\frac {4!}{2!2^2}}_{=E(Z^4)}\cdot\frac {\theta^4}{4!} + \underbrace{0}_{=E(Z^5)}\cdot \frac {\theta^5}{5!} + \underbrace{\frac {6!}{3!2^3}}_{=E(Z^6)}\cdot\frac {\theta^6}{6!} + \cdots\\
\end{eqnarray*}
from which it follows the odd moments of a standard Normal vanish and the even moments are $(2k)!/[k!2^k]$: For $k=1,2,3,\dots$,
$$E(Z^{2k-1})=0$$
and
$$E(Z^{2k})=\dfrac{(2k)!}{k!2^k}.$$


\vskip .5 in

\noindent {\bf MGF of a N$(\mu,\sigma^2)$.}\\
From the corollary on page \pageref{normalcor}, once we have the MGF for a standard Normal we also have it for a N$(\mu,\sigma^2)$, since
$Z\sim \mbox{N}(0,1)\implies \sigma Z+\mu\sim \mbox{N}(\mu,\sigma^2)$:  If $X\sim \mbox{N}(\mu,\sigma^2)$, then
$$M_X(\theta) = E(e^{\theta X}) = E(e^{\theta (\sigma Z + \mu)}) =  E(e^{\theta\sigma Z}e^{\theta\mu}) = e^{\theta\mu}\underbrace{E(e^{(\theta\sigma)Z})}_{=M_Z(\theta\sigma)}=
e^{\theta\mu}e^{\frac {\theta^2\sigma^2}2}$$
So, if $X\sim \mbox{N}(\mu,\sigma^2)$, then
$$M_X(\theta) = e^{\theta\mu + \frac {\theta^2\sigma^2}2}.$$


\newpage

\noindent {\bf Computing probabilities involving a standard Normal rv.}\label{normallocalproperties}\\
When $Z$ is standard Normal, i.e., $Z\sim \mbox{N}(0,1)$, we have the CDF of $Z$:
$$\Phi(x) = \int_{-\infty}^x \dfrac {e^{-\frac 12u^2}}{\sqrt{2\pi}}\,du,$$
and on page \pageref{normalintegratesto1} we showed $\Phi(\infty) = 1$, and on page \pageref{normalsymmetry} we showed $\Phi(0)=\frac 12$.
But, unfortunately, these are the only values where $\Phi$ can be computed exactly, and, if we {\em needed} the value of $\Phi(x)$ for any other $x$
we can only approximate it.  \\

\noindent Many calculators and computers now have the functionality to give the value of $\Phi(x)$ for any desired $x$ to many decimal-place accuracy.
By the way, I'll point out that once we can approximate $\Phi(x)$, we can also approximate the CDF of {\em any} Normal rv $X\sim\mbox{N}(\mu,\sigma^2)$ by
$$F_X(x) = P(X\le x) = P(\sigma Z+\mu\le x) = P\!\left(Z\le \frac {x-\mu}{\sigma}\right) = \Phi\!\left(\frac {x-\mu}{\sigma}\right).$$

\noindent Since it will be necessary to evaluate probabilities involving Normal rvs, I'll present a statistical table with approximate values for
$\Phi(x)$, where $x$ will range from $-3.49$ to $3.49$ in increments of $.01$. \\


\vskip .5 in

\noindent {\bf Examples.}\\
Verify the following using the standard Normal table on page \pageref{standardnormaltable}:\\
\noindent (a) $P(Z\le 1.96) = .9750$\\
\noindent (b) $P(Z\le -1.96)= .0250$\\
\noindent (c) $P(-1.96\le Z\le 1.96) = .9500$\\

\includegraphics*[0,0][500,150]{areasundernormal1.jpg}

\vskip .5 in

\noindent By the symmetry of the standard Normal pdf, it should be clear that $P(Z\ge 1.96) = P(Z\le -1.96) = .0250$, this can also be seen by
the complementary rule: $P(Z\ge 1.96) = 1-P(Z<1.96) = 1-.9750=.0250$.\\



\newpage

\vskip -.7 in

\includegraphics*[100,30][600,770]{standard-normal-table2.pdf}\label{standardnormaltable}


\newpage

\noindent {\bf Computing probabilities with arbitrary Normals.}\\
\noindent Now that we know how to use a standard Normal table, how can it be used to compute probabilities involving an arbitrary $X\sim \mbox{N}(\mu,\sigma^2)$ rv?\\

\noindent Recall the CDF of $X$ is $F_X(x) = \Phi\!\left(\frac {x-\mu}{\sigma}\right),$ i.e., $\frac {X-\mu}{\sigma}=Z$ is a standard Normal. Let's do an example:\\

\vskip .5 in

\noindent {\bf Example.}\\
Assume the adult male height $X$ (in inches) of a certain population
is normally distributed with a mean of $70$ inches and a standard deviation of $\sigma=4$ inches (i.e, variance
$\sigma^2=16$ inches$^2$) -- a long-winded way of saying $X\sim\mbox{N}(70,16)$.\\
(a) What proportion of the population is taller than 76 inches?\\
(b) What proportion of the population is between 66 and 74 inches?\\

\noindent SOLUTION:\\
(a) $P(X > 76) = P\!\left(\frac {X-\mu}{\sigma} > \frac {76 - 70}{4}\right) = P(Z > 1.50) = 1-P(Z\le 1.50) = 1-.9332 = .0668$; approximately 6.68\% of the population is
taller than 76 inches.\\
(b) $P(66\le X\le 74) = P\!\left(\frac {66-70}{4}\le \frac {X-\mu}{\sigma} \le \frac {74-70}{4}\right) = P(-1.00\le Z\le 1.00) = P(Z\le 1.00)-P(Z\le -1.00) = .8413-.1587=.6826$;
approximately 68.26\% of the population is between 66 and 74 inches.\\



\newpage



\noindent {\bf Extreme behavior of random variables.} ({\bf\em This is a digression.})\\

\noindent Sometimes it is important to understand the extreme behavior of random variables, i.e., the probability the rv takes extremely large (or small) values.
One particular instance where this is the case is in {\bf\em Reliability theory}.\\

\vskip .5 in

\noindent Let $X$ be a continuous rv with pdf $f(x)$ and CDF $F(x)$. \\

\noindent We define the {\bf\em survival function}\label{survivalfunction}:
$$\overline{F}(x) := P(X>x),$$
as you can see $\overline{F}(x):=1-F(x)$. If $X$ represents the lifetime of a component (time to death), then $\overline{F}(x)$ is the probability the component is alive at $x$ -- it has survived to time $x$.\\

\noindent We define the {\bf\em hazard rate function}\label{hazardratefunction}:
\begin{eqnarray*}h(x) &:=& \lim_{\delta \downarrow 0} \dfrac {P(x<X\le x+\delta | X>x)}{\delta}\\
&=& \lim_{\delta \downarrow 0} \dfrac {P((x<X\le x+\delta) \cap (X>x))}{\delta P(X>x)}\\
&=& \lim_{\delta \downarrow 0} \dfrac {P(x<X\le x+\delta)}{\delta P(X>x)}\\
&=& \lim_{\delta \downarrow 0} \dfrac {F(x+\delta)-F(x)}{\delta \overline{F}(x)} = \dfrac {f(x)}{\overline{F}(x)}.\\
\end{eqnarray*}

\noindent The {\bf\em Mills ratio}\label{millsratio} is the reciprocal of the hazard rate function $h(x)$:
$$m(x) = \dfrac {\overline{F}(x)}{f(x)} = \dfrac {P(X>x)}{f(x)}.$$

\noindent Probabilists and Statisticians are often interested in large $x$ behavior of these quantities. \\


\vskip .5 in

\noindent {\bf Notation.}\\
We write $a(x)\sim b(x)$ as $x\to\infty$ to mean $$\lim_{x\to\infty} \dfrac {a(x)}{b(x)}=1.$$



\newpage

\noindent {\bf Example.}\\
Let $Z\sim \mbox{N}(0,1)$.  Show that $m(x) \sim \frac 1x$ as $x\to \infty$.\\
A consequence of this is that $m(x):=\frac {P(Z>x)}{\varphi(x)} \sim \frac 1x \implies P(Z>x)\approx \frac{\varphi(x)}x$ as $x\to\infty$.\\

\noindent SOLUTION:
\begin{eqnarray*}
P(Z>x)
&=& \frac 1{\sqrt{2\pi}}\int_x^{\infty} e^{-z^2/2}\,dz \\
&=& \frac 1{\sqrt{2\pi}}\int_x^{\infty} \frac 1z \left(z e^{-z^2/2}\right)\,dz \\
&=& \frac 1{\sqrt{2\pi}}\left( \left.-\frac 1ze^{-z^2/2}\right|_{z=x}^{\infty} - \int_x^{\infty} \frac 1{z^2} e^{-z^2/2}\,dz  \right)\\
&=& \frac 1{\sqrt{2\pi}}\left( \frac 1xe^{-x^2/2} - \int_x^{\infty} \frac 1{z^2} e^{-z^2/2}\,dz  \right)\\
&=& \frac 1x \varphi(x) - \frac 1{\sqrt{2\pi}}\int_x^{\infty}\frac 1{z^2}e^{-z^2/2}\,dz.
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
m(x) = \frac {P(Z>x)}{\varphi(x)} & = & \dfrac 1x - \dfrac {\frac 1{\sqrt{2\pi}}\int_x^{\infty}\frac 1{z^2}e^{-z^2/2}\,dz}{\varphi(x)},
\end{eqnarray*}
and, using
L'H\^{o}pital's rule, the last term tends of $0$ as $x\to\infty$ (please check this for yourself), and then this proves that
$$\lim_{x\to\infty} \dfrac {P(Z>x)}{\frac 1x\varphi(x)} = 1,$$
which basically says that, for {\em large} $x$, $P(Z>x)$ can be well-approximated by $\frac {\varphi(x)}{x}$.  In fact, the approximation is already rather good for ``small" $x$, let's see\dots.\\

\vskip .5 in

$$\begin{array}{lrr}\mbox{From the table\dots}& \qquad m(x)\varphi(x)=\frac {\varphi(x)}x & \quad \mbox{Abs.rel.error=}\left| \frac {P(Z>x)-\frac {\varphi(x)}{x}}{\frac{\varphi(x)}x} \right| \\ \hline\hline
P(Z>1) = .1587 & .241971 & .34\\
P(Z>2) = .0228 & .026995 & .16\\
P(Z>3) = .0013 & .001477 & .086\\
P(Z>4)^*= .00003167 & .00003345 & .053\\ \hline
\end{array}$$
\begin{center}
{\bf Figure.} $^*$ used software to find this value.
\end{center}

\noindent One can repeat what we did with the standard Normal above {\em ad infinitum} to show that the Mills ratio has the expansion $m(x) =\frac 1x -\frac 1{x^3}+\frac 1{x^5}-+\cdots$.



\newpage

\noindent {\bf The CDF method -- univariate case.}\label{cdfmethod}\\

\noindent The CDF method is one of several approaches\footnote{other approaches we'll learn in this course: method of Jacobians, convolution, and MGF method.} we will learn that solves the following {\bf\em general problem:}

\begin{center}Suppose $X$ is a {\em continuous} rv with pdf $f_X(x)$.  Find the pdf of $Y=g(X)$.\end{center}

\noindent In fact, the CDF method can also handle the {\bf\em multivariate case} where we have {\em more than one} jointly continuous rvs $X_1,X_2,\dots,X_n$ and we wish to
find the pdf of some function of them: $Y=g(X_1,X_2,\dots,X_n)$.  We will demonstrate this {\em after} we've introduced the concept of jointly distributed rvs. But, in this section, we will concentrate on the {\bf \em univariate case}, i.e., the case involving only one random variable.\\

\vskip .5 in


\noindent We've already used this method back on page \pageref{cdfmethod1} to show when $X$ has the pdf
$$f_X(x) = \frac {e^{-\frac {-(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\quad \mbox{for }-\infty<x<\infty,$$
then $Y=g(X):=aX+b$ has the pdf
$$f_Y(y) = \frac {e^{-\frac {\left(x-(a\mu + b)\right)^2}{2a^2\sigma^2}}}{\sqrt{2\pi a^2\sigma^2}}\quad \mbox{for }-\infty < y<\infty.$$




\vskip .5 in

\noindent {\bf Steps to apply in the CDF method.}\\

\noindent {\bf step 1.} Identify the image of $Y=g(X)$, i.e., the support of $f_Y(y)$.\medskip

Since $f_Y(y)\equiv 0$ for $y$ not in the support, we can concentrate solely on $y\in \mbox{supp}(f_Y)$.\\

\noindent {\bf step 2.} For $y\in \mbox{supp}(f_Y)$, compute the CDF of $Y$: $F_Y(y) := P(Y\le y)=P(g(X)\le y)$.\medskip

Here we need to manipulate the CDF $P(g(X)\le y)$ into one that involves only the

CDF of $X$.\\

\noindent {\bf step 3.} Once we have the CDF of $Y$ in terms of the CDF of $X$, take the derivative: \medskip

We may need to use a chain rule or a Liebniz rule to take this derivative.

$f_Y(y) = \frac d{dy}F_Y(y)$ for $y\in \mbox{supp}(f_Y(y))$,

$f_Y(y)=0$ for $y\not\in \mbox{supp}(f_Y(y)).$



\newpage

\noindent {\bf Example.}\\
Suppose $Z\sim \mbox{N}(0,1)$.  Find the pdf of $Y=Z^2$.\\

\noindent SOLUTION:\\
Here, $Y=g(Z)$, where $g(z) = z^2$.  Since a standard Normal rv takes values
from $-\infty < z < \infty$, it follows that $y=g(z) = z^2$ takes values from $0\le y<\infty$.
Thus, supp$(f_Y) = [0,\infty)$.
Right away, this tells us that $f_Y(y)=0$ for $y\le 0$, and we can concentrate on $y> 0$ (don't forget: $Y$ is also a continuous rv, so $P(Y=0)=0$).\\

\noindent Now, let $y> 0$.
\begin{eqnarray*}
F_Y(y) & = & P(Y\le y)\\
&=& P(Z^2\le y)\\
&=& P(|Z|\le \sqrt{y}) \qquad (\mbox{remember! }\sqrt{Z^2}=|Z|.)\\
&=& P(-\sqrt{y}\le Z\le \sqrt{y})\\
&=& \int_{-y^{\frac 12}}^{y^{\frac 12}} \dfrac {e^{-z^2/2}}{\sqrt{2\pi}}\,dz.\\
\end{eqnarray*}
At this point you can either recognize that this integral is really just $\Phi(y^{\frac 12}) - \Phi(-y^{\frac 12})$ (and then you can take the derivative in $y$ not forgetting to use the chain rule), or, you can (equivalently) just use the Liebniz rule
to take the derivative in $y$ which I do now:
\begin{eqnarray*}f_Y(y)=\frac d{dy}\int_{-y^{\frac 12}}^{y^{\frac 12}} \dfrac {e^{-z^2/2}}{\sqrt{2\pi}}\,dz
&=& \dfrac {e^{-\left(y^{\frac 12}\right)^2/2}}{\sqrt{2\pi}}\cdot \frac d{dy}\left(y^{\frac 12}\right) -
\dfrac {e^{-\left(-y^{\frac 12}\right)^2/2}}{\sqrt{2\pi}}\cdot \frac d{dy}\left(-y^{\frac 12}\right)\\
&=& \dfrac {e^{-y/2}}{\sqrt{2\pi}}\cdot \underbrace{\frac d{dy}\left(y^{\frac 12}\right)}_{=\frac {y^{-\frac 12}}2} +
\dfrac {e^{-y/2}}{\sqrt{2\pi}}\cdot \underbrace{\frac d{dy}\left(y^{\frac 12}\right)}_{=\frac {y^{-\frac 12}}2}\\
&=& \dfrac {y^{-\frac 12}e^{-y/2}}{2^{\frac 12}\sqrt{\pi}} =  \dfrac {y^{\frac 12 - 1}e^{-y/2}}{2^{\frac 12}\Gamma(\frac 12)},
\end{eqnarray*}
which {\em hopefully} you recognize as the pdf of a Gamma$(\frac 12,2)$.\\

\noindent Thus, we've shown
$$f_Y(y) = \left\{\begin{array}{cl} \dfrac {y^{\frac 12 - 1}e^{-y/2}}{2^{\frac 12}\Gamma(\frac 12)} & \mbox{for }y>0 \\  & \\ 0 & \mbox{for }y\le 0 \end{array}\right..$$
Statisticians call this distribution the {\bf\em Chi-square with 1 degree of freedom}\label{chisquare1} -- it is the distribution of the square of a standard Normal.\\

\newpage

\noindent {\bf Strictly monotone transformations of a continuous rv.}\\

\noindent The CDF method is used to prove the following theorem:\\

\noindent {\bf Theorem.}\\
Suppose $X$ is a continuous rv with pdf $f_X(x)$ and $Y=g(X)$ with $g=g(x)$ being strictly monotone. Then
$$f_Y\!(y) = f_X\!\left(g^{-1}(y)\right)\cdot \left|\frac {dg^{-1}(y)}{dy} \right|.$$

\vskip .5 in

\noindent {\bf Math facts.}\\
A function $g=g(x)$ is strictly increasing (resp., strictly decreasing) means if $a<b$ then $g(a)<g(b)$ (resp., $g(a)>g(b)$); and,
we call such functions {\bf\em strictly monotone}.\label{strictlymonotone}\\
FACT: If $g=g(x)$ is strictly increasing (resp., strictly decreasing), then $g^{-1}=g^{-1}(y)$ exists and is strictly increasing (resp., strictly decreasing).\\

\vskip .5 in

\noindent Proof.\\
Let's first prove this in the case that $g=g(x)$ is strictly increasing.\\
Let $I = \mbox{supp}(f_X)$, then $g(I) = \{y: y=g(x)\ \mbox{for some }x\in\mbox{supp}(f_X)\}=\mbox{supp}(f_Y)$.  Take any $y\in \mbox{supp}(f_Y)$.
\begin{eqnarray*}
F_Y(y) & = & P(Y\le y)\\
&=& P(g(X)\le y)\\
&=& P(X\le g^{-1}(y))\quad (\mbox{$g$ increasing }\implies g(X)\le y\implies g^{-1}(g(X))\le g^{-1}(y))\\
&=&F_X(g^{-1}(y)).\\
\end{eqnarray*}
Thus, $F_Y(y)=F_X(g^{-1}(y))$.  Now, since the derivative of the CDF is the pdf, we have
\begin{eqnarray*}f_Y\!(y) &=& \dfrac {dF_Y(y)}{dy}\\
&=&\dfrac {dF_X(g^{-1}(y))}{dy}=f_X\!(g^{-1}(y))\cdot \dfrac {g^{-1}(y)}{dy},\quad (\mbox{$g^{-1}$ increasing}\implies \frac {dg^{-1}(y)}{dy}>0)
\end{eqnarray*}
where we used the chain rule in the last step. Alternatively, we could've written
$$F_Y(y) = \int_{-\infty}^{g^{-1}(y)}f_X(x)\,dx,$$
and appealed to the Liebniz rule.\\


\newpage

\noindent Suppose, instead, $g=g(x)$ is strictly decreasing.\\
Then
\begin{eqnarray*}
F_Y(y) & = & P(Y\le y)\\
&=& P(g(X)\le y)\\
&=& P(X\ge g^{-1}(y))\quad (\mbox{$g$ decreasing }\implies g(X)\le y\implies g^{-1}(g(X))\ge g^{-1}(y))\\
&=&\int_{g^{-1}(y)}^{\infty} f_X(x)\,dx,\\
\end{eqnarray*}
and appealing to the Liebniz rule,
\begin{eqnarray*}
f_Y(y) &=& \dfrac {d}{dy} \int_{g^{-1}(y)}^{\infty} f_X(x)\,dx \\
&=& -f_X(g^{-1}(y))\cdot \dfrac {dg^{-1}(y)}{dy} \\
&=& f_X(g^{-1}(y))\cdot \dfrac {d\left(-g^{-1}(y)\right)}{dy}.\quad (\mbox{$g^{-1}$ decreasing }\implies\frac {d\left(-g^{-1}(y)\right)}{dy}>0)\\
\end{eqnarray*}
Since we get a pdf for $Y$ regardless of whether $g=g(x)$ strictly increasing or strictly decreasing, we can write the resulting pdf for $Y$ as:
$$f_Y(y) = f_X(g^{-1}(y))\cdot \left|\dfrac {dg^{-1}(y)}{dy}\right|.$$
$\hfill \Box$


\vskip .5 in

\noindent {\bf Example.}\\
Let's show, again, that if $X\sim \mbox{N}(\mu,\sigma^2)$, then $Z=\frac {X-\mu}{\sigma}\sim \mbox{N}(0,1)$.\\

\noindent SOLUTION:\\
$f_X(x) = \frac 1{\sqrt{2\pi\sigma^2}}\,e^{-\frac {(x - \mu)^2}{2\sigma^2}}$\\
$z=g(x) = \frac {x-\mu}{\sigma} \implies g^{-1}(z) = \sigma z+\mu \implies \frac d{dz}\left(g^{-1}(z)\right)=\sigma$.\\
\begin{eqnarray*}
f_Z(z) &=& f_X(g^{-1}(z))\cdot \left|\frac d{dz}\left(g^{-1}(z)\right)\right|\\
&=& \frac 1{\sqrt{2\pi\sigma^2}}\,e^{-\frac {(\sigma z + \mu - \mu)^2}{2\sigma^2}}\cdot \left|\sigma \right|\quad (|\sigma|=\sigma\mbox{ since }\sigma>0)\\
&=& \frac {\sigma}{\sqrt{2\pi\sigma^2}}\,e^{-\frac {z^2}{2}}\\
&=& \frac {1}{\sqrt{2\pi}}\,e^{-\frac {z^2}{2}} = \varphi(z),\\
\end{eqnarray*}
and we see the $z$-score of a Normal is a standard Normal.


\newpage

\noindent {\bf Remark.}\\
The $z$-score of a Normal rv will still have a Normal distribution. This isn't necessarily true of other distributions.
For example, the $z$-score of a Gamma$(\alpha,\beta)$ will {\em not} have a Gamma distribution as the next example demonstrates.\\

\vskip .5 in

\noindent {\bf Example.}\\
Let $X\sim \mbox{Gamma}(\alpha,\beta)$. Then the mean of $X$ is $\mu=\alpha\beta$ and the variance of $X$ is $\sigma^2=\alpha\beta^2$, i.e., $\sigma = \beta\sqrt{\alpha}$.
Find the pdf of $Z = \frac {X-\alpha\beta}{\beta\sqrt{\alpha}}$ and show that it is {\em not} Gamma distributed.\\

\noindent SOLUTION:\\
For $x>0$, $f_X(x) = \frac {x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}$.\\
$z = \frac {x-\alpha\beta}{\beta\sqrt{\alpha}} \implies z>-\sqrt{\alpha}$ when $x>0$. So, it appears that supp$(f_Z)$ is {\em not} $[0,\infty)$! Not a good sign if we are hoping for a Gamma!!\\

\noindent $g^{-1}(z) = \beta\sqrt{\alpha}z+\alpha\beta \implies \frac d{dz}(g^{-1}(z))=\beta\sqrt{\alpha}>0$.\\
So, for $z>-\sqrt{\alpha}$,
\begin{eqnarray*}
f_Z(z) & = & f_X(\beta\sqrt{\alpha}z+\alpha\beta)\cdot \beta\sqrt{\alpha}\\
&=& \frac {(\beta\sqrt{\alpha}z+\alpha\beta)^{\alpha-1}e^{-(\beta\sqrt{\alpha}z+\alpha\beta)/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\\
&=& \frac {\beta^{\alpha-1}(\sqrt{\alpha})^{\alpha-1}(z+\sqrt{\alpha})^{\alpha-1}e^{-(\sqrt{\alpha}z+\alpha)}}{\beta^{\alpha}\Gamma(\alpha)}\\
&=& \frac {\alpha^{\frac{\alpha-1}2}(z+\sqrt{\alpha})^{\alpha-1}e^{-\sqrt{a}(z+\sqrt{\alpha})}}{\Gamma(\alpha)}\\
\end{eqnarray*}
and this pdf is positive, for instance, when $z=-\frac 12\sqrt{\alpha}$ and, thus, it's support is not the nonnegative reals {\em and}, therefore, cannot be Gamma distributed.



\vskip .5 in

\noindent {\bf Exercise for the student.}\\
Suppose $X\sim \mbox{exp}(1)$, i.e., $X$ is the continuous rv with pdf
$$f(x) = e^{-x}\quad \mbox{for }x>0,$$
sometimes called the {\bf\em unit exponential distribution}.\label{unitexp} Fix $\nu,\alpha,\beta\in {\mathbb R}$ with $\alpha>0$ and $\beta>0$.
Find the pdf of
$$Y = \nu + \alpha X^{\frac 1{\beta}}.$$
The distribution of $Y$ is called the {\bf\em Weibull distribution}\label{weibulldist}.\\

ANSWER:
$$f_Y(y) = \left\{\begin{array}{cl} \frac {\beta(y-\nu)^{\beta-1}}{\alpha^{\beta}}\,e^{-\left(\frac {y-\nu}{\alpha}\right)^{\beta}}  & \mbox{for }y>\nu\\ 0 & \mbox{for }y\le \nu\end{array} \right..$$




\newpage

\noindent {\bf Example.} (the log-normal distribution)\label{lognormal}
Suppose $X\sim \mbox{N}(\mu,\sigma^2)$. Then the rv $Y=e^X$ is said to have a {\bf\em log-normal distribution with parameters $\mu$ and $\sigma^2$}.
Find the pdf of the log-normal.\\

\noindent SOLUTION:\\
$y=g(x)=e^x$ is a strictly increasing function of $x$ and, since supp$(f_X)=(-\infty,\infty)$, supp$(f_Y) = (0,\infty)$.\\
$g^{-1}(y) = \ln(y) \implies \frac d{dy}(g^{-1}(y))=\frac 1y >0$ when $y>0$.\\
So, for $y>0$,
$$f_Y(y)
= f_X(\ln(y))\cdot\frac 1y\\
= \frac {e^{-\frac {(\ln(y) - \mu)^2}{2\sigma^2}}}{y\sqrt{2\pi\sigma^2}}.
$$
The pdf of the log-normal with parameters $\mu$ and $\sigma^2$ is, therefore,
$$f_Y(y) = \left\{ \begin{array}{cl} \dfrac {e^{-\frac {(\ln(y) - \mu)^2}{2\sigma^2}}}{y\sqrt{2\pi\sigma^2}} & \mbox{for }y>0 \\ 0 & \mbox{for }y\le 0   \end{array} \right..$$


\vskip .5 in

\noindent {\bf Example.} (the Cauchy distribution)\label{Cauchypdf2}\\
Suppose $\Theta \sim \mbox{uniform}(-\frac {\pi}2, \frac {\pi}2)$ and fix $x_0\in{\mathbb R}$ and $\gamma >0$.\\
Find the pdf of $X=x_0+\gamma\tan(\Theta)$.\\

\noindent SOLUTION:\\
\includegraphics*[-90,0][400,100]{cauchygeometrypic.jpg}


\noindent $f_{\Theta}(\theta)=\frac 1{\pi}$ for $-\frac {\pi}2 < \theta < \frac {\pi}2$.

\noindent As $\theta$ ranges from $-\frac {\pi}2$ to $\frac {\pi}2$, $x$ ranges from $-\infty$ to $\infty$. Moreover,

\noindent $x=g(\theta)=x_0+\gamma \tan(\theta) \implies \theta = g^{-1}(x)=\tan^{-1}\left(\frac {x-x_0}{\gamma}\right)\in (-\frac {\pi}2,\frac {\pi}2)$ when $x\in {\mathbb R}
\implies \frac {d}{dx}g^{-1}(x) =\frac 1{1+\left(\frac {x-x_0}{\gamma}\right)^2}\cdot \frac 1{\gamma}$.\\

\noindent Therefore, for $-\infty < x < \infty$,
$$f_X(x) = \underbrace{f_{\Theta}\left(\tan^{-1}\left(\frac {x-x_0}{\gamma}\right)\right)}_{=\frac 1{\pi}} \cdot \left(\frac 1{1+\left(\frac {x-x_0}{\gamma}\right)^2}\right)\cdot\frac 1{\gamma}
=\dfrac 1{\pi\gamma\left( 1 + \left( \frac {x-x_0}{\gamma}\right)^2\right)},$$
is the {\bf\em Cauchy distribution with parameters $x_0$ and $\gamma$} abbreviated $X\sim$\,Cauchy$(x_0,\gamma)$.
When $x_0=0$ and $\gamma=1$, we call the $\mbox{Cauchy}(0,1)$ the {\bf\em standard Cauchy distribution}.


\newpage



\begin{center}{\bf \Large V. Jointly distributed random variables.}\end{center}


\newpage


\noindent {\bf Jointly distributed random variables.}\\

\noindent A natural extension to univariate probability, i.e., analysis of a single rv on $\Omega$,
is to multivariate probability, i.e., {\em many} rvs on $\Omega$.
When a collection of random variables are all defined on the
{\em same sample space} we say the random variables are {\bf\em jointly distributed}\label{jointlydistributed} and
that there is a sample space $\Omega$ that supports the joint collection.\\

\noindent {\bf Some examples of jointly distributed rvs.}\\
$\bullet$ If the experiment is to roll a fair 6-sided die repeatedly and we define rvs
$X_i$ to be the value on the $i$th roll ($i=1,2,3,\dots$), then this is a jointly distributed collection; in fact, since
all these random variables are discrete we will say they are {\bf\em jointly discrete}.\\

\noindent $\bullet$ An experiment might select
an individual within a certain population and, for this individual,
we might measure their serum creatine level $X$,
their heart rate $Y$,
and their blood pressure $Z$. In this case
the collection $X,Y,Z$ would be jointly distributed;
in fact, {\bf\em jointly continuous} if these are all continuous rvs.\\

\noindent $\bullet$ An experiment might be to throw a dart at a dartboard $n$ times.
$X$ (resp., $Y$) returns the $x$-coordinate (resp., $y$-coordinate) of where the dart lands on the dartboard.
$N$ might be the rv that counts the number of times the dart ``misses" the dartboard completely.
The collection $X,Y,N$ are jointly distributed, but because these rvs are of mixed type ($N$ is discrete whereas $X$ and $Y$ are continuous)
we cannot say they are jointly discrete nor can we say they are jointly continuous, so we'll just say they are jointly distributed.\\

\vskip .5 in

\noindent {\bf Jointly discrete rvs.}\label{jointlydiscrete}\\

\noindent When we have a collection of discrete rvs all defined on the
same sample space $\Omega$ we say these rvs are jointly discrete. To keep notation under control
I will present the theory in the case of only {\em two} jointly discrete rvs, say $X$ and $Y$.\\

\noindent If $X$ and $Y$ are jointly discrete, we define the {\bf\em joint probability mass function} or, simply,
the {\bf\em joint pmf}\label{jointpmf} by
$$p_{X,Y}(x,y)=P(X=x,Y=y).$$
\noindent The event $(X=x,Y=y)$ is shorthand for $\{\omega\in \Omega :X(\omega)=x\}\cap \{\omega\in \Omega: Y(\omega)=y\}$.






\includegraphics*[-15,0][400,120]{jointdiscretevenn1.jpg}
\begin{center}
{\bf Figure.} Each discrete rv $X$ and $Y$ partitions $\Omega$, so do their intersections.
\end{center}


\newpage

\noindent {\bf Example and discussion.}\\
Suppose we have a box with 12 balls of which 3 are red, 4 are white, 5 are blue.
We uniformly at random select 2 balls. Let $X$ be the number of red balls, $Y$ the number of blue balls.
Construct the joint pmf of $X$ and $Y$.\\

\noindent SOLUTION:\\
In this problem we seem to be only interested in the numbers of red and blue balls,
but when we are drawing balls from this box it's fairly clear that we can draw red,
blue and white balls.  Since we are only drawing $n=2$ balls and there are 3 red balls,
$X$ can take the values $0,1,2$; similarly, there are 5 blue balls, so $Y$ can take the values $0,1,2$ as well:
$$X\in \{0,1,2\},\quad Y\in\{0,1,2\}.$$
There are 12 balls total and selecting $n=2$ uniformly at random means all $|\Omega|={12\choose 2}$ sample points are equally likely -- I choose to ignore the order in which the balls are selected.
The event $(X=x,Y=y)$ means that in our sample of size 2 we drew $x$ red balls {\em and} $y$ blue balls, {\em and}, therefore, $2-x-y$ balls are
neither red nor blue (i.e., white); thus,
when ignoring the order in which the balls are selected,
every collection of 2 balls can be decomposed in the union of 3 mutually exclusive events:
the subset of red marbles drawn, the subset of blue marbles drawn, and the subset of marbles that are neither red nor blue -- we allow some of these
subsets to be empty, of course.
$$P(X=x,Y=y) = \dfrac {{3\choose x}{5\choose y}{4\choose 2-x-y}}{{12\choose 2}}.$$
\noindent I'll mention something that is hopefully obvious: in this problem $x+y$ needs to be less than of equal to 2 since we are only drawing 2 balls; and,
in the case where $x+y>2$ we necessarily have $P(X=x,Y=y)=0$.  Another way to have seen this is that when $x+y>2$ the last binomial coefficient in the numerator is ${4\choose 2-(x+y)}$ which would say we are selecting a {\em negative} number of objects from 4 objects and there must be 0 ways to do this!\medskip

\noindent Therefore, we get the following joint pmf:
$$\begin{array}{c||c|c|c|} p_{X,Y}(x,y)  & y=0 & y=1 & y=2 \\
& & & \\ \hline\hline
& & & \\
x=0 & \dfrac {{3\choose 0}{5\choose 0}{4\choose 2}}{{12\choose 2}}=\dfrac {6}{66} & \dfrac {{3\choose 0}{5\choose 1}{4\choose 1}}{{12\choose 2}}= \dfrac {20}{66}& \dfrac {{3\choose 0}{5\choose 2}{4\choose 0}}{{12\choose 2}}= \dfrac {10}{66}\\
& & & \\ \hline
& & & \\
x=1 & \dfrac {{3\choose 1}{5\choose 0}{4\choose 1}}{{12\choose 2}}= \dfrac {12}{66}& \dfrac {{3\choose 1}{5\choose 1}{4\choose 2}}{{12\choose 0}}= \dfrac {15}{66}& 0\\
& & & \\ \hline
& & & \\
x=2 & \dfrac {{3\choose 2}{5\choose 0}{4\choose 0}}{{12\choose 2}}= \dfrac {3}{66}& 0 & 0 \\
& & & \\ \hline \end{array}$$

\newpage

\noindent {\bf Remark.}\\
By the way, the probability distribution that we just constructed is an example of the {\bf\em multivariate hypergeometric distribution}.\label{multivariatehypergeom1}
We will develop this distribution in a little more detail on page \pageref{multivariatehypergeom}.\\

\vskip .5 in



\noindent {\bf Computing probabilities involving jointly discrete rvs.}\\
Analogous to the univariate discrete rv case, once we have the joint pmf
computing probabilities involves summing the probability masses at each point $(x,y)$ that belongs to the event.\\

\vskip .5 in

\noindent {\bf Example (continued).} \\
\noindent Following up with the example on the last page\dots \\
Now that we found the joint pmf of $X$ and $Y$, let's compute

\noindent (a) $P(X\le 1,Y\le 1)$\\
\noindent (b) $P(X+Y\le 1)$\\
\noindent (c) $P(X\le 1)$\\
\noindent (d) $P(Y\le 1)$\\

\noindent SOLUTION:\\
\noindent (a) The event $(X\le 1, Y\le 1)$ is $(x,y)=(0,0),(0,1),(1,0),(1,1)$; therefore, we just need to add the probability masses at these 4 points:
$$\frac {6}{66}+\frac {20}{66}+\frac {12}{66}+\frac {15}{66} =  \frac {53}{66}.$$

\noindent (b) The event $(X+Y\le 1)$ is $(x,y)=(0,0),(0,1),(1,0)$; therefore, we just need to add the probability masses at these 3 points:
$$\frac {6}{66}+\frac {20}{66}+\frac {12}{66} =  \frac {38}{66}.$$

\noindent (c) The event $(X\le 1)$ is $(x,y)=(0,0),(0,1),(0,2),(1,0),(1,1),(1,2)$; now there are 6 probability mass to add:
$$\frac {6}{66}+\frac {20}{66}+\frac {10}{66}+\frac {12}{66} + \frac {15}{66} +0=  \frac {63}{66}.$$

\noindent (d) The event $(Y\le 1)$ is $(x,y)=(0,0),(0,1),(1,0),(1,1),(2,0),(2,1)$; and, again, 6 (different) probability masses to add:
$$\frac {6}{66}+\frac {20}{66}+\frac {12}{66}+\frac {15}{66} +\frac 3{66}+0=  \frac {56}{66}.$$


\newpage


\noindent {\bf Remark.}\\
The joint pmf of a collection of rvs completely describes the probabilistic behavior (distribution) of the collection as a whole.
What's important to know is that we can recover the distribution of any subcollection
of these rvs (i.e, its {\bf\em marginal distribution})\label{marginalpmf} from the joint pmf of the original collection.\\

\vskip .5 in

\noindent {\bf Example (continued still).}\\
\noindent Still following up with the last example\dots \\
Use the joint pmf to find the marginal pmf of $X$ and the marginal pmf of $Y$.\\

\noindent SOLUTIONS:\\
For $x=0,1,2$, $P(X=x) = \sum_y P(X=x,Y=y) = p_{X,Y}(x,0)+p_{X,Y}(x,1)+p_{X,Y}(x,2)$:
$$\begin{array}{c||c|c|c|} x & 0 & 1 & 2 \\ \hline\hline p_X(x) & \frac {36}{66} & \frac {27}{66} & \frac 3{66}\end{array}$$

\noindent For $y=0,1,2$, $P(Y=y)=\sum_x P(X=x,Y=y) = p_{X,Y}(0,y)+p_{X,Y}(1,y)+p_{X,Y}(2,y)$:
$$\begin{array}{c||c|c|c|} x & 0 & 1 & 2 \\ \hline\hline p_X(x) & \frac {21}{66} & \frac {35}{66} & \frac {10}{66}\end{array}$$

\bigskip

\noindent {\bf Finding the marginal pmf.}\\
\noindent Basically what we did to find the marginal pmf of a subset of variables is
we went back into the original joint pmf and, for each fixed choice of the values for the variables in the subset we simply ``summed out" the {\em other} variables not
in this subset.  Below, in \textcolor{red}{red} ink we have the \textcolor{red}{marginal pmf of $X$}, in \textcolor{blue}{blue} ink we have the \textcolor{blue}{marginal pmf of $Y$}:

$$\begin{array}{c||c|c|c|c} p_{X,Y}(x,y)  & y=0 & y=1 & y=2 & \quad \\
& & & &\\ \hline\hline
& & & &\\
x=0 & \dfrac {{3\choose 0}{5\choose 0}{4\choose 2}}{{12\choose 2}}=\dfrac {6}{66} & \dfrac {{3\choose 0}{5\choose 1}{4\choose 1}}{{12\choose 2}}= \dfrac {20}{66}& \dfrac {{3\choose 0}{5\choose 2}{4\choose 0}}{{12\choose 2}}= \dfrac {10}{66} & \color{red}{\dfrac {36}{66}=P(X=0)}\\
& & & &\\ \hline
& & & &\\
x=1 & \dfrac {{3\choose 1}{5\choose 0}{4\choose 1}}{{12\choose 2}}= \dfrac {12}{66}& \dfrac {{3\choose 1}{5\choose 1}{4\choose 2}}{{12\choose 0}}= \dfrac {15}{66}& 0 & \color{red}{\dfrac {27}{66}=P(X=1)}\\
& & & &\\ \hline
& & & &\\
x=2 & \dfrac {{3\choose 2}{5\choose 0}{4\choose 0}}{{12\choose 2}}= \dfrac {3}{66}& 0 & 0 & \color{red}{\dfrac {3}{66}=P(X=2)}\\
& & & &\\ \hline
& & & &\\
& \color{blue}{\dfrac {21}{66}=P(Y=0)} & \color{blue}{\dfrac {35}{66}=P(Y=1)} & \color{blue}{\dfrac {10}{66}=P(Y=2)} \end{array}$$







\newpage

\noindent {\bf Exercise for the student.}\\
We have a fair coin. In each round, three people $1,2,3$ in this order
take turns flipping the coin, and, after person 3 has flipped,
the coin goes back to person 1 and we enter the next round; and, this repeats {\em ad infinitum}.
We define the random variable $X = i$ if person $i$ is the first to flip a head.
We define $Y$ to be the round on which a person flips the first head.
Construct the joint pmf of $X,Y$.\\
Then, from this joint pmf, find the marginal pmfs for each of $X$ and $Y$.\\

ANSWER:
$$\begin{array}{c||c|c|c|c|c|c|c}P(X=x,Y=y) & y=1 & y=2 & y=3& \cdots & y=n& \cdots & \color{red}{P(X=x)}\\ \hline\hline
& & & & & & \\
x=1 & \frac 12             & \big(\frac 12\big)^4 & \big(\frac 12\big)^7 & \cdots & \big(\frac 12\big)^{1+3(n-1)} & \cdots & \color{red}{\frac 47} \\
& & & & & & \\ \hline
& & & & & & \\
x=2 & \big(\frac 12\big)^2 & \big(\frac 12\big)^5 & \big(\frac 12\big)^8 & \cdots & \big(\frac 12\big)^{2+3(n-1)} & \cdots & \color{red}{\frac 27} \\
& & & & & & \\ \hline
& & & & & & \\
x=3 & \big(\frac 12\big)^3 & \big(\frac 12\big)^6 & \big(\frac 12\big)^9 & \cdots & \big(\frac 12\big)^{3n}       & \cdots & \color{red}{\frac 17} \\
& & & & & & \\ \hline
& & & & & & \\
\color{blue}{P(Y=y)} & \color{blue}{\frac 78} & \color{blue}{\frac 78\big(\frac 18\big)} & \color{blue}{\frac 78\big(\frac 18\big)^2} & \cdots & \color{blue}{\frac 78\big(\frac 18\big)^{n-1}} & \cdots \end{array}$$
What's really interesting in this example is that, unlike the previous example,
$$P(X=x,Y=y)=P(X=x)P(Y=y) \quad \mbox{for {\em all} choices of $x$ and $y$.}$$
We will learn that this means the random variables $X$ and $Y$ are {\bf\em independent}: knowledge of the value of one of these rvs will not influence the probability of the other.  To compare you should note that in the red/blue balls example, our joint pmf doesn't have this property, and, therefore, those rvs are {\em dependent}.\\

\vskip .3 in

\noindent {\bf Exercise for the student.}\\
Show that the following function is a joint pmf, namely, when we sum all the probability masses we get 1:
$$p_{X,Y}(x,y) = \frac {2y}{x(x+1)}\Big(\frac {1}2\Big)^x\quad\mbox{for }x=1,2,3,\dots\ ;\ y=1,2,\dots,x.$$
The formula $\sum_{y=1}^x y = \frac {x(x+1)}2$ might be helpful.  Can you prove this?\\

\vskip .3 in

\noindent {\bf Concluding remarks.}\\
Thus far, I only presented examples involving {\em two} jointly discrete rvs; this was primarily to simplify presentation and notation. But,
I would like to now present two common models of joint pmfs that involve more than two rvs typically.  The first is the
multivariate hypergeometric distribution and the other
is the multinomial distribution.




\newpage

\noindent {\bf Multivariate hypergeometric distribution.}\label{multivariatehypergeom}\\
As the name suggests this distribution is an extension of the hypergeometric distribution to $k>2$ random variables. The situation is this:\\


\noindent We have a {\em finite} population of $N$ distinct objects.  We suppose these objects are comprised of $k$ types.  There are
$N_i$ objects of type $i$ ($i=1,2,\dots,k$), where $N_1+N_2+\cdots +N_k=N$.
We sample uniformly at random $n$ from the population {\em without replacement}
and let $X_i$ denote the number of type $i$'s in our sample.\\
Then, if $n_1+n_2+\cdots+n_k=n$,
$$P(X_1=n_1,X_2=n_2,\dots,X_k=n_k) = \dfrac {{N_1\choose n_1}{N_2\choose n_2}\cdots {N_k\choose n_k}}{{N\choose n}}.$$


\vskip .5 in

\noindent {\bf Facts regarding the multivariate hypergeometric.}\\
When $k=2$, the above definition reduces to the ordinary hypergeometric  --
the $k=2$ types would arbitrarily be called {\em successes} and {\em failures}.
The univariate marginal distributions of a multivariate hypergeomtric are ordinary hypergeometric distributions.\\

\vskip .5 in

\noindent {\bf Example.}\\
A bin has 1000 components of which $750$ are superior quality, $200$ are above average quality, $40$ are good quality, and $10$ are poor quality.
We uniformly at random sample $8$ components. Let $X_1$ be the number of superior, $X_2$ the number of above average, $X_3$ the number of good, and $X_4$
the number of poor quality components in the sample. Compute the probability we get 2 of each quality classification.  How about 6 superior, 2 above average, 0 good and 0 poor? \\

\noindent SOLUTION:\\
$$P(X_1=2,X_2=2,X_3=3,X_4=2) = \dfrac {{750 \choose 2}{200\choose 2}{40\choose 2}{10\choose 2}}{{1000\choose 8}}\approx 0.00000813551.$$

$$P(X_1=6,X_2=2,X_3=0,X_4=0) = \dfrac {{750 \choose 6}{200\choose 2}{40\choose 0}{10\choose 0}}{{1000\choose 8}}\approx 0.19993657.$$


\newpage

\noindent {\bf Multinomial distribution.}\label{multinomialdist}\\
As the name suggests this is an extension of the binomial distribution to $k>2$ types.  As an experiment leading to
this distribution we can reconsider the multivariate hypergeometric experiment only this time we sample {\em with} replacement. Alternatively,
we can think of our population as {\em infinite} but having proportions $p_i$ of type $i$ in it ($i=1,2,\dots,k$), where $p_1+p_2+\cdots + p_k=1$.\\

\noindent We randomly (draw independently one at a time) sample $n$ from the population, and let $X_i$ be the number of type $i$ in our sample.
If $n_1+n_2+\cdots + n_k=n$, then
$$P(X_1=n_1,X_2=n_2,\dots,X_k=n_k) = \dfrac {n!}{n_1!n_2!\cdots n_k!}p_1^{n_1}p_2^{n_2}\cdots p_k^{n_k}.$$

\vskip .5 in

\noindent {\bf Facts regarding the multinomial distribution.}\\
When $k=2$ this is the binomial distribution as long as we interepret only one of the two rvs as counting the number of ``successes" -- the other would be counting the number of ``failures". The bivariate marginal pmfs of the multinomial distribution, i.e., the marginal pmfs involving any two rvs amongst $X_1,X_2,\dots,X_k$, are binomial.\\
{\bf\em The multinomial theorem:}\label{multinomialtheorem}
$$(x_1+x_2+\cdots +x_k)^n = \sum_{n_1}\sum_{n_2}\cdots\sum_{n_k} \frac {n!}{n_1!n_2!\cdots n_k!}x_1^{n_1}x_2^{n_2}\cdots x_k^{n_k},$$
where the sum extends over all nonnegative integers $n_1,n_2,\dots,n_k$ that sum to $n$.\\

\vskip .5 in

\noindent {\bf Remark.}\\
Much of what we learned regarding the similarities between the ordinary hypergeometric and binomial distributions goes over to these multivariate versions.\\

\vskip .5 in

\noindent {\bf Example.}\\
We have a weighted 6-sided die (not fair).  On any roll, face $i$ comes up with probability $p_i=\frac i{21}$ (notice that $p_1+p_2+\cdots + p_6=1$ here).
We roll this die 210 times. Compute the probability that the number of times we see each face follows these proportions directly, i.e., $X_i=10i$ for $i=1,2,\dots,6$.\\

\noindent SOLUTION:\\

$P(X_1=10,X_2=20,X_3=30,X_4=40,X_5=50,X_6=60)$\\

\hskip .75 in $= \dfrac {210!}{10!20!30!40!50!60!}\big( \frac 1{21}\big)^{10}\big( \frac 2{21}\big)^{20}\big( \frac 3{21}\big)^{30}\big( \frac 4{21}\big)^{40}\big( \frac 5{21}\big)^{50}\big( \frac 6{21}\big)^{60} \approx 0.000005349.$








\newpage

\noindent {\bf Jointly continuous rvs}\label{jointlycontinuous}\\
\noindent This section is a natural extension to what we did with univariate continuous rvs. There, we defined a continuous rv as one with
continuous CDF on ${\mathbb R}$.  We'll do the same here.\\

\noindent A collection of rvs $X_1,X_2,\dots,X_n$ is said to be {\bf\em jointly continuous} provided they are all defined on the same sample space {\em and}
the joint CDF:
$$F_{X_1,X_2,\dots,X_n}(x_1,x_2,\dots,x_n) = P(X_1\le x_1,X_2\le x_2,\dots, X_n\le x_n)$$
is a continuous function on ${\mathbb R}^n$.\\


\vskip .2 in

\noindent {\bf\em Existence and characterization of the joint pdf:}\\
Just as we did with the assumption on page \pageref{pdfexistassumption}, we will assume
throughout this course that the {\bf\em joint probability density function} or, simply, the
{\bf\em joint pdf:}\label{jointpdf}
$$f_{X_1,X_2,\dots,X_n}(x_1,x_2,\dots,x_n) = \dfrac {\partial^n}{\partial x_1\partial x_2\cdots\partial x_n}F_{X_1,X_2,\dots,X_n}(x_1,x_2,\dots,x_n)$$
exists for (almost) all $(x_1,x_2,\dots,x_n)\in {\mathbb R}^n$.
Under this assumption, the resulting joint pdf can be characterized by these two properties:\\

1. $f_{X_1,X_2,\dots,X_n}(x_1,x_2,\dots,x_n)$ is {\em defined} on all of ${\mathbb R}^n$ {\em and} is {\em nonnegative}; and\\

2. $\displaystyle \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty} f_{X_1,X_2,\dots,X_n}(x_1,x_2,\dots,x_n)\,dx_1dx_2\cdots dx_n = 1$.\\

\noindent When we have such a collection of jointly continuous rvs its joint pdf is either given to us or is modeled by the specific situation at hand -- like
the collection of rvs are independent or we can model the conditional behavior of some of these rvs given the others, etc. More on this later!\\


\newpage




%\includegraphics*[-320,0][200,160]{jointcdfpic.jpg}



%\noindent Let's see how one could get the joint pdf from the joint CDF

%\noindent in the 2-dim.~case:\medskip


%\begin{eqnarray*}
%\dfrac {\partial}{\partial x_2} F_{X_1,X_2}(x_1,x_2) &=& \lim_{h_2\to 0+}\dfrac {F_{X_1,X_2}(x_1,x_2)-F_{X_1,X_2}(x_1,x_2-h_2)}{h_2}\\
%\end{eqnarray*}
%and
%\begin{eqnarray*}
%\dfrac {\partial}{\partial x_1} \dfrac {\partial}{\partial x_2}F_{X_1,X_2}(x_1,x_2) &=& \lim_{h_1\to 0+}\dfrac {\frac {\partial}{\partial x_2}F_{X_1,X_2}(x_1,x_2)-\frac {\partial}{\partial x_2}F_{X_1,X_2}(x_1-h_1,x_2)}{h_1}.\\
%\end{eqnarray*}
%Now substitute what we know about $\frac {\partial}{\partial x_2}F_{X_1,X_2}(x_1,x_2)$ and $\frac {\partial}{\partial x_2}F_{X_1,X_2}(x_1-h_1,x_2)$ from the previous display:\\

%\noindent $\dfrac {\partial}{\partial x_1} \dfrac {\partial}{\partial x_2}F_{X_1,X_2}(x_1,x_2) =$ \vskip .2 in

%\noindent $\displaystyle \lim_{\stackrel {h_1\to 0+}{h_2\to 0+}}\dfrac {\displaystyle F_{X_1,X_2}(x_1,x_2)-F_{X_1,X_2}(x_1,x_2-h_2)-\left[F_{X_1,X_2}(x_1-h_1,x_2)-F_{X_1,X_2}(x_1,x_2-h_2)\right]}{h_1h_2}$\\

%\noindent The numerator of the difference quotient reduces to
%$$P(x_1-h_1<X_1\le x_1,x_2-h_2<X_2\le x_2)$$
%since, from the picture up top, \\

%$F_{X_1,X_2}(x_1,x_2)$ is the probability of all 4 colored regions: green, orange, yellow, blue

%$F_{X_1,X_2}(x_1,x_2-h_2)$ is the probability of the 2 colored regions: yellow, blue

%$F_{X_1,X_2}(x_1-h_1,x_2)$ is the probability of the 2 colored regions: green, orange

%$F_{X_1,X_2}(x_1-h_1,x_2-h_2)$ is probability of the 1 colored region: blue\\

%\noindent Therefore,

%$P\{green,orange,yellow,blue\} - P\{yellow,blue\} - P\{orange,blue\} + P\{blue\} = P\{green\}$.

%\newpage







\newpage

\noindent {\bf Example and discussion.}\\
Consider the function\footnote{this is the pdf of a uniform distribution on the rectangle $[0,2]\times [0,1]$\label{2duniformexample1} -- see page \pageref{2duniform}}
$$f(x,y) = \left\{\begin{array}{cl} \frac 12 & \mbox{for }0\le x\le 2, 0\le y\le 1 \\ 0 & \mbox{elsewhere}  \end{array}\right..$$
(a) Show this is a joint pdf.\\
(b) Use it to compute $P(X\le 1, Y\le \frac 12)$.\\
(c) Compute $P(X+Y\le 1)$\\
(d) Compute $P(Y\le X^2)$.\\

\includegraphics*[-100,0][300,130]{2duniformplot1.jpg}
\begin{center}{\bf Figure.} Plot of the pdf\end{center}

\noindent SOLUTION:\\
(a) To show a function of two variables is a joint pdf, we need to show two things: first, that it's defined and nonnegative everywhere in ${\mathbb R}^2$,
and, second, show that the integral of the function over ${\mathbb R}^2$ is 1.\\

\noindent The function $f(x,y)$ given is defined for all $(x,y)\in {\mathbb R}^2$ and, since $f(x,y)$ only takes on two values, namely, $0$ and $\frac 12$, and both of these are nonnegative, $f(x,y)\ge 0$ everywhere.  We now need to show
$$\displaystyle\int  \int f(x,y)\,dA = 1.$$
But from the plot of $f(x,y)$ we see a very simple geometry, the integral in this case is just the volume of the box under the orange surface over the yellow region:
$2\times 1\times \frac 12 =1$. \\

\noindent Alternatively, we could have used calculus
$$\int\int f(x,y)\,dA = \int_0^1\int_0^2 \frac 12\,dxdy = \frac 12\int_0^1\int_0^2 \,dxdy = \frac 12 \,\mbox{area}([0,2]\times [0,1]) = 1.$$


\newpage

\noindent
As nice as the plot of the surface $z=f(x,y)$ is, it's practically
{\em useless}! Definitely not needed. What will be more important is a
picture of the support of $f(x,y)$ -- in this case, the rectangle $[0,2]\times [0,1]$:

\includegraphics*[-20,0][400,100]{2duniformsupport1.jpg}

\noindent We imagine that over each of the shaded regions in the above
picture there is our surface $f(x,y)$, under which we are trying to compute the
content, i.e., ``volume" -- this content is our probability.\\

\noindent In this example, $f(x,y)=\frac 12$ is a constant, so for any ${\cal R}\subseteq [0,2]\times [0,1]$, the
integral of this function will be equal to $\frac 12\times \mbox{area}({\cal R})$.  If ${\cal R}$ has a geometric shape
whose area is easily computed then we are essentially done. This is the case, for instance, parts (b) and (c):\\

\noindent (b) $P(X\le 1,Y\le \frac 12) = \frac 12\times\mbox{area}(\mbox{blue box}) = \frac 12\times (1\times \frac 12) = \frac 14.$\\

\noindent (c) $P(X+Y\le 1) = \frac 12\times \mbox{area}(\mbox{green triangle}) = \frac 12\times (\frac 12\times 1\times 1) = \frac 14.$\\

\noindent (d) For this part we don't have a well-recognized geometric shape,
so we're going to have to resort to some calculus, i.e., we're going to need to integrate. We
need to compute
$$\int\int_{{\cal R}}\frac 12\,dxdy.$$

\noindent Finding probability is, therefore, reduced to a calculus problem.  We are integrating the function $f(x,y)=\frac 12$ over the
orange region.  We have a choice of parametrization of this integral: we can integrate, say, $dxdy$ or $dydx$.

\includegraphics*[-20,0][400,100]{2duniformsupport2.jpg}

\noindent If we parameterize $dxdy$, we get the integral:
$$\int_0^1 \int_{\sqrt{y}}^2 \frac 12\,dxdy = \frac 12 \int_0^1 2-y^{1/2}\,dy = \frac 12 \left(\left.2y - \frac 23y^{3/2}\right|_{y=0}^{y=1}\right)=\frac 23.$$
\noindent If we parameterize $dydx$, we get the integral:
$$\int_0^1 \int_{0}^{x_1^2} \frac 12\,dydx_1 + \int_1^2\int_0^1\frac 12\,dydx_2 = \frac 12\int_0^1x_1^2\,dx_1 + \frac 12\int_1^2 1\,dx_2 = \frac 16+\frac 12=\frac 23.$$



\newpage
\noindent The reason the $dxdy$ integration was ``nicer" in the last example is
because with this parametrization the {\em entrance boundary} and the {\em exit boundary}
are each described by single functions throughout the integration; whereas in the $dydx$ parametrization
the {\em exit boundary} changes it function definition depending on whether $x<1$ or $x>1$: when
$x<1$ we enter the orange region through the line $y=0$ and exit the region at $y=x^2$, but when
$x>1$ we enter the region, again, at $y=0$ but exit the region at $y=1$.\\

\noindent {\bf Example.}\label{jointpdfexample}\\
Suppose $X$ and $Y$ are jointly continuous rvs with the joint pdf
$$f_{X,Y}(x,y) = \left\{ \begin{array}{cl} e^{-y}  &  \mbox{for } 0<x<y<\infty \\ 0 & \mbox{elsewhere}    \end{array}\right..$$
(Exercise: Show this is a pdf.)  Compute

\noindent (a) $P(X+Y\le 1)$

\noindent (b) $P(\frac YX \le u)$ for $u>1$.\\

\noindent SOLUTION: \\
(a) Let's first sketch the support of the joint pdf together with the region of interest:\\

\includegraphics*[-140,0][150,160]{joint-pdf-1.jpg}

\noindent The green shaded region is the region of interest, i.e., we need to integrate the joint pdf over this region.
We have a couple of choices of parametrization of this region, $dx\,dy$ or $dy\,dx$.  By inspection of the green region
the parametrization $dy\,dx$ will lead to a single integral whereas the parametrization $dx\,dy$ will lead to two integrals -- this can be seen because
once we fix $y$ in the interval $0<y<1$, where we {\em exit} the green region in the $x$-direction will depend on whether $y<\frac 12$ or $y>\frac 12$.
So, I choose to integrate $dy\,dx$ for this problem:
\begin{eqnarray*}
P(X+Y\le 1) &=& \int_0^{\frac 12} \int_{x}^{1-x} e^{-y}\,dy\,dx\\
&=& \int_0^{\frac 12} \left. -e^{-y}\right|_{y=x}^{y=1-x}\, dx = \int_0^{\frac 12} e^{-x}-e^{-1+x}\, dx\\
&=& \left. -e^{-x}-e^{-1+x}\right|_{x=0}^{x=1} = 1-2e^{-1/2}+e^{-1}\approx .1548.\\
\end{eqnarray*}

\newpage

\noindent (b) Here's a sketch of the region where $y/x\le u$:


\includegraphics*[-140,0][150,150]{joint-pdf-2.jpg}

\noindent In this case either parametrization seems fine. As an integral using $dxdy$:
\begin{eqnarray*}
P\!\left(\frac YX\le u\right) & = &
\int_0^{\infty}\int_{y/u}^{y} e^{-y}\,dxdy\\
\end{eqnarray*}
and as an integral using $dydx$:
\begin{eqnarray*}
P\!\left(\frac YX\le u\right) & = &
\int_0^{\infty}\int_{x}^{ux} e^{-y}\,dydx.
\end{eqnarray*}
Please verify that either of these integrals gives the probability $1-\frac 1u$.  In fact, we've shown that the CDF of
$\frac YX$, namely, $F_{\frac YX}(u) = P(\frac YX\le u) = 1-\frac 1u$ for $u>1$. When $u\le 1$, $F_{\frac YX}(u) = 0$.\\


\vskip .5 in



\noindent {\bf Marginal pdf.}\label{s:marginalpdf}\\

\noindent If $X$ and $Y$ are jointly continuous with joint pdf $f_{X,Y}$, then we define
the {\em\bf marginal pdf $f_X(x)$ of $X$} by
$$f_X(x) := \int_{-\infty}^{\infty}f_{X,Y}(x,y)\,dy$$
and the {\em\bf marginal pdf $f_Y(y)$ of $Y$} by
$$f_Y(y) := \int_{-\infty}^{\infty}f_{X,Y}(x,y)\,dx.$$
Notice, for instance, to find the marginal pdf of $X$ at the value $x$ we ``integrate out" the other variable for this fixed value of $x$. Similarly, the
marginal pdf of $Y$ at the value $y$ is found by ``integrating out" the joint pdf at this fixed value of $y$.\\

\noindent Just as with the marginal pmfs for jointly discrete rvs, the marginal pdfs represent the pdf of the of rv in the absence of the other variable.

\newpage

\noindent {\bf Example.}\label{jointpdfexamplemarginals}\\
Continuing with the example on page \pageref{jointpdfexample}, we have
$$f_{X,Y}(x,y) = \left\{ \begin{array}{cl} e^{-y}  &  \mbox{for } 0<x<y<\infty \\ 0 & \mbox{elsewhere}    \end{array}\right..$$
Find the marginal pdf of $X$ and the marginal pdf of $Y$.\\

\noindent SOLUTION:\\
Let's first find the marginal pdf of $X$.  So, {\em fix} a value $x$.

Since the support of the joint pdf is the set $0<x<y<\infty$, if we pick a value of $x\le 0$, the joint pdf is 0 and, therefore, $f_X(x)=0$ when $x\le 0$.

If $x>0$, the we need to integrate out $y$ for this fixed value of $x$.  But the support requires $0<x<y<\infty$, i.e., $y$ cannot be smaller than $x$. So, when $x>0$,
$$f_X(x)= \int_x^{\infty} e^{-y}\,dy = e^{-x}.$$
Therefore,
$$f_X(x) = \left\{\begin{array}{cl} e^{-x} & \mbox{for }x>0 \\ 0 & \mbox{for }x\le 0 \end{array} \right.,$$
i.e., $X\sim \mbox{Exp}(1)$.\\

\noindent Now for the marginal of $Y$. {\em Fix} a value of $y$.  Again, if this value of $y$ is less than or equal to 0, the joint pdf will vanish and, in this case,
$f_Y(y)=0$.

On the other hand, if $y>0$, the support of the joint pdf requires $0<x<y$ so that when we integrate out the $x$ variable we just need to integrate the joint for $x$ between 0 and $y$, i.e., when $y>0$
$$f_Y(y)= \int_0^y e^{-y}\,dx = ye^{-y}.$$
Thus, our marginal pdf of $Y$ is
$$f_Y(y)= \left\{\begin{array}{cl} ye^{-y} & \mbox{for }y>0 \\ 0 & \mbox{for }y\le 0 \end{array} \right.,$$
i.e., $Y\sim \mbox{Gamma}(2,1)$.\\

\vskip .1 in

\noindent {\bf Remark.}\\
When we have {\em more than just two} jointly continuous rvs, the idea of the marginal pdfs can be extended naturally. For instance,
suppose we have five jointly continuous rvs $X_1,X_2,X_3,X_4,X_5$ with joint pdf
$f_{X_1,X_2,X_3,X_4,X_5}(x_1,x_2,x_3,x_4,x_5)$.  Then to find the marginal pdf of {\em any subset} of these five rvs we would ``integrate out"
all the other rvs not in the subset. For example,
$$f_{X_3}(x_3) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
f_{X_1,X_2,X_3,X_4,X_5}(x_1,x_2,x_3,x_4,x_5)\,dx_1dx_2dx_4dx_5$$
and
$$f_{X_2,X_4}(x_2,x_4) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
f_{X_1,X_2,X_3,X_4,X_5}(x_1,x_2,x_3,x_4,x_5)\,dx_1dx_3dx_5,$$
etc.  Analogies apply to jointly discrete rvs as well.



\newpage

\noindent {\bf Jointly distributed rvs of mixed type.}\\
There are many problems involving jointly distributed rvs that are of mixed type (say, discrete and continuous). The following example explores this situation.\\

\noindent {\bf Example.}\label{jointmixtureexample}\\
Suppose we flip a fair coin repeatedly.  Let $X$ be the trial of the first head we obtain (so that $X\sim \mbox{geometric}(\frac 12)$ - a discrete rv).
Now, given $X=x$,
suppose we win $Y$ dollars, where $Y\sim \mbox{Exp}(x)$.
In this scenario, the distribution of the amount of money we win is a {\em continuous} rv and $Y$ depends on when we flip our first head, i.e., $X$.

Fix $h>0$.
\begin{eqnarray*}
\frac {P\!\left( X=x, y-h< Y\le y \right)}h &=& P(X=x)\cdot \frac {P(y-h<Y\le y|X=x)}h\\
&=& \left(\frac 12\right)^{x}\cdot \frac {\int_{y-h}^y xe^{-xu}\,du}h\\
\end{eqnarray*}
and as $h\to 0$ the above converges to the function
$$p_{X,Y}(x,y) = \left(\frac 12\right)^{x}xe^{-xy}\quad \mbox{for }x=1,2,3,\dots;\ y>0.$$
The function $p_{X,Y}(x,y)$ is neither a joint pmf nor a joint pdf: in fact, $p_{X,Y}(x,y)$ is a pmf in the $x$ argument and is a pdf in the $y$ argument.
We will loosely call such a function the joint distribution of $X$ and $Y$.

In this example, we were given the marginal pmf of $X$, in fact, $X$ is geometric$(\frac 12)$. However, we were only given the {\em conditional} distribution of $Y$
when the value of $X$ is specified.  We now ask: what is the marginal pdf of $Y$?

Since we know the joint distribution of $X$ and $Y$, the marginal of $Y$ can be found by ``summing out" the $X$ variable:
Let $y>0$. Then
\begin{eqnarray*}
f_Y(y) &=& \sum_{x=1}^{\infty} \left(\frac 12\right)^{x}xe^{-xy}\\
&=& \left(\frac {e^{-y}}2\right)\sum_{x=1}^{\infty} x\left(\frac {e^{-y}}2\right)^{x-1}\\
&=& \left(\frac {e^{-y}}2\right)\left( 1-\frac {e^{-y}}2 \right)^{-1}\underbrace{\sum_{x=1}^{\infty} x\left( 1- \frac {e^{-y}}2 \right)\left(\frac {e^{-y}}2\right)^{x-1}}_{=\mbox{\tiny mean of a geom}(1- \frac {e^{-y}}{2})=(1- \frac {e^{-y}}{2})^{-1}}\\
&=& \left(\frac {e^{-y}}2\right)\left( 1-\frac {e^{-y}}2 \right)^{-2}.\\
\end{eqnarray*}




\newpage


\noindent {\bf Independence of random variables.}\label{independentervs}\\

\noindent A collection of random variables is called {\em\bf independent}\label{independentrvs} if and only if the joint distribution of the collection
factors into the product of its marginals.

For instance, jointly discrete rvs $X_1,X_2,\dots,X_n$ are independent means the joint pmf of $X_1,X_2,\dots,X_n$ factors in the the product of the marginal pmfs of each rv in this collection:
$$P(X_1=x_1,X_2=x_2,\dots,X_n=x_n) = P(X_1=x_1)P(X_2=x_2)\cdots P(X_n=x_n)$$
for {\em every} choice of $x_1,x_2,\dots,x_n.$
The ``for every choice" is important here because we want the functions of $x_1,x_2,\dots,x_n$ to be the same!

Similarly, jointly continuous rvs $X_1,X_2,\dots,X_n$ are independent means the joint pdf of $X_1,X_2,\dots,X_n$ is the product of the marginal pdfs of each $X_i$ in this collection:
$$f_{X_1,X_2,\dots,X_n}(x_1,x_2,\dots,x_n) = f_{X_1}(x_1)f_{X_2}(x_2)\cdots f_{X_n}(x_n)\quad \mbox{for every choice }x_1,x_2,\dots,x_n.$$

\bigskip

\noindent Random variables that are not independent are said to be {\em \bf dependent}.\label{dependentrvs}

\vskip .2 in

\noindent {\bf Example.}\\
From the example on page \pageref{jointpdfexamplemarginals}
$$f_{X,Y}(x,y) = \left\{ \begin{array}{cl} e^{-y}  &  \mbox{for } 0<x<y<\infty \\ 0 & \mbox{elsewhere}    \end{array}\right.$$
and we found the marginal pdfs to be
$$f_X(x) = \left\{\begin{array}{cl} e^{-x} & \mbox{for }x>0 \\ 0 & \mbox{for }x\le 0 \end{array} \right.$$
and
$$f_Y(y)= \left\{\begin{array}{cl} ye^{-y} & \mbox{for }y>0 \\ 0 & \mbox{for }y\le 0 \end{array} \right..$$
Notice that
$$f_X(x)f_Y(y) =
\left\{\begin{array}{cl} e^{-x}ye^{-y} & \mbox{for }x>0,\ y>0 \\ 0 & \mbox{elsewhere}\end{array} \right.$$
is positive when $x>y$ whereas $f_{X,Y}(x,y)=0$ when $x>y$. So, these functions are not the same and the rvs are dependent!\\






\newpage


\noindent {\bf Remark.}\\
When jointly discrete rvs $X$ and $Y$ are independent it would follow that
$$P(X=x,Y=y)=P(X=x)P(Y=y)\qquad \mbox{for {\em all} } x\ \ \mbox{and  }y.$$
For example, look at the following joint pmf below given of {\em independent} rvs $X$ and $Y$:

$$\begin{array}{c||c|c|c|c} p_{X,Y}(x,y)  & y=0 & y=1 & y=2 & \quad \\
& & & &\\ \hline\hline
& & & &\\
x=0 & .03 & .05 & .02 & \color{red}{.1=P(X=0)}\\
& & & &\\ \hline
& & & &\\
x=1 & .06 & .10 & .04 & \color{red}{.2=P(X=1)}\\
& & & &\\ \hline
& & & &\\
x=2 & .09 & .15 & .06 & \color{red}{.3=P(X=2)}\\
& & & &\\ \hline
& & & &\\
x=3 & .12 & .20 & .08 & \color{red}{.4=P(X=3)}\\
& & & &\\ \hline
& & & &\\
& \color{blue}{.3=P(Y=0)} & \color{blue}{.5=P(Y=1)} & \color{blue}{.2=P(Y=2)} \end{array}$$
When $X$ and $Y$ are independent the joint pmf for each fixed $x$ is proportional to the marginal pmf of $Y$.
Likewise, the joint pmf for each fixed $y$ is proportional to the marginal pmf of $X$. That is,
$$\frac {P(X=x,Y=y)}{P(X=x)} = P(Y=y|X=x)=P(Y=y)$$
and
$$\frac {P(X=x,Y=y)}{P(Y=y)} = P(X=x|Y=y)=P(X=x).$$


\newpage


\noindent {\bf Example.}\\
Suppose a rectangle has random edges lengths $X$ and $Y$. Assume $X\sim \mbox{uniform}(0,1)$ and $Y\sim \mbox{uniform}(0,1)$ are
independent. Compute the probability that the area of the rectangle is greater than $\frac 12$.\\

\noindent SOLUTION:\\
We are told that
$$f_X(x) = \left\{ \begin{array}{cl}  1 & \mbox{for }0<x<1 \\ 0 & \mbox{elsewhere}  \end{array}\right.$$
and
$$f_Y(y) = \left\{ \begin{array}{cl}  1 & \mbox{for }0<y<1 \\ 0 & \mbox{elsewhere}  \end{array}\right.$$
and, therefore, the joint pdf of $X$ and $Y$ is (by assumed independence)
$$f_{X,Y}(x,y) = \left\{ \begin{array}{cl}  1 & \mbox{for }0<x<1, \ 0<y<1 \\ 0 & \mbox{elsewhere}  \end{array}\right.$$
We are interested in computing $P(XY>\frac 12)$.  Here's a sketch of the support and the event of interest:

\includegraphics*[-140,0][130,130]{Arearect1.jpg}

\begin{eqnarray*}
P\!\left(XY>\frac 12\right) & = & \int_{\frac 12}^1\int_{\frac 1{2x}}^1 1\,dydx \\
&=& \int_{\frac 12}^1 1 - \frac 1{2x}\,dx\\
&=& \left. x - \frac 12\ln(x) \right|_{x=\frac 12}^1 \\
&=& 1 - 0 - ( \frac 12 - \frac 12\ln(\frac 12) ) = \frac 12 - \frac 12\ln(2).\\
\end{eqnarray*}


\bigskip
\noindent {\bf Exercise for the student.}\\
Continue with the above example and show that the pdf of the area $A=XY$ of the rectangle is given by $f_A(a) = \left\{ \begin{array}{cl} -\ln(a)  & \mbox{for }0<a<1\\0 & \mbox{elsewhere}\end{array}\right.$.


\newpage


\noindent {\bf Convolution.}\label{s:convolution}\\

\noindent Mathematically, if we have two real-valued functions
$f,g:{\mathbb R}\to {\mathbb R}$, the {\em \bf convolution}\label{d:convolutionintegral} $f*g$ {\em\bf of} $f$ {\em\bf and} $g$ is defined by
\begin{equation}\label{convolutionintegral}
(f*g)(x) := \int_{-\infty}^{\infty}f(u)g(x-u)\,du.
\end{equation}
Technically, (\ref{convolutionintegral}) is called the {\em \bf convolution integral of} $f$ {\em\bf and} $g$.
We will soon show that when $f$ and $g$ are the pdfs of the {\em independent} continuous rvs $X$ and $Y$, respectively, then
$f*g$ is the pdf of $X+Y$.\\


\noindent There is a discrete analog to the convolution integral but it requires the discrete rvs to be {\em integer-valued}.
Suppose $X$ and $Y$ are
{\em independent} integer-valued rvs with respective
pmfs $p_X(i) = P(X=i)$ and $p_Y(j)=P(Y=j)$.  Then we will show the pmf of $X+Y$ is given by
\begin{eqnarray}\label{discreteconvolution}
(p_X*p_Y)(k) &:=& \sum_{n=-\infty}^{\infty} p_X(n)p_Y(k-n)\\
&=& \cdots + p_X(-1)p_Y(k+1)+p_X(0)p_Y(k) + p_X(1)p_Y(k-1)+\cdots\nonumber
\end{eqnarray}
Equation (\ref{discreteconvolution}) is called the {\em\bf discrete convolution of} $p_X$ {\em\bf and} $p_Y$.\\

\vskip .7 in
\noindent {\bf The discrete convolution.}\medskip

\noindent Let's start with the discrete case first:  If $X$ and $Y$ are integer-valued, then $X+Y$ will be integer-valued.
By the law of total probability
\begin{eqnarray*}
P(X+Y=k) &=& \sum_{n=-\infty}^{\infty} P(X=n,X+Y=k)\\
&=& \sum_{n=-\infty}^{\infty} P(X=n, n+Y=k)\\
&=& \sum_{n=-\infty}^{\infty} P(X=n, Y=k-n)\\
&=& \sum_{n=-\infty}^{\infty} \underbrace{P(X=n)}_{=p_X(n)} \underbrace{P(Y=k-n)}_{=p_Y(k-n)} =:(p_X*p_Y)(k).\\
\end{eqnarray*}


\newpage


\noindent {\bf Exercise.}\\
Show $(p_X*p_Y)(k) = (p_Y*p_X)(k)$ for every $k$, and, therefore, $p_X*p_Y$ can be defined as
$$(p_X*p_Y)(k) = \sum_{u=-\infty}^{\infty}P(X=k-u)P(Y=u).$$
Hint: When we employed the law of total probability to compute $P(X+Y=k)$ above we partitioned $\Omega$ by the events $(X=n)$
for integer $n$; try partitioning $\Omega$ by the events $(Y=u)$ for integer $u$ instead.
This exercise says that convolution is a commutative operation.  The result shouldn't be shocking since
the distribution of $X+Y$ should be the same as the distribution of $Y+X$.\\

\vskip .5 in

\noindent {\bf Remark.}\\
The convolution gives us a strategy for computing the distribution of a sum of independent random variables.
Although the general definition of the discrete convolution has the sum extending over all integer indices from $-\infty$ to $\infty$,
depending on the supports of the random variables involved for some indices the summand may vanish. Let's consider some common scenarios.

Scenario 1: $X$ and $Y$ have pmfs supported on the nonnegative integers.
In this case $p_X(n)=0$ if $n<0$ and $p_Y(k-n)=0$ when $k-n<0$, i.e., when $n>k$. Therefore, for integer $k\ge 0$,
$$(p_X*p_Y)(k) = \sum_{n=-\infty}^{\infty}p_X(n)p_Y(k-n) = \sum_{n=0}^kp_X(n)p_Y(k-n).$$

Scenario 2: $X$ is supported on the integers $x\ge m_1$ for some integer $m_1$, and the pmf of $Y$ is
supported on the integers $y\ge m_2$ for some integer $m_2$.
In this case $p_X(n)=0$ if $n<m_1$ and $p_Y(k-n)=0$ when $k-n<m_2$, i.e., when $n>k-m_2$. Therefore, for integer $k\ge m_1+m_2$,
$$(p_X*p_Y)(k) = \sum_{n=-\infty}^{\infty}p_X(n)p_Y(k-n) = \sum_{n=m_1}^{k-m_2}p_X(n)p_Y(k-n).$$

Scenario 3: $X$ has nonnegative support but $Y$ is supported on all integers. Then $p_X(n)=0$ for $n<0$.  So, for $k\in {\mathbb Z}$,
$$(p_X*p_Y)(k) = \sum_{n=-\infty}^{\infty}p_X(n)p_Y(k-n) = \sum_{n=0}^{\infty}p_X(n)p_Y(k-n).$$

\newpage


\noindent {\bf Example.}(the neg.binom$(r,p)$)\\
Prove that the sum of $r$ independent geometric$(p)$ rvs has a neg.binom$(r,p)$ distribution.\\

\noindent SOLUTION:\\
Suppose $X_1\sim \mbox{geometric}(p)$ and $X_2\sim \mbox{geometric}(p)$ are independent. Recall
$$p(i)=P(X_1=i)=P(X_2=i) = p(1-p)^{i-1}\quad \mbox{for }i=1,2,3,\dots.$$
Let's find the distribution of their sum $X_1+X_2$ as the discrete convolution.  Since the support of the common distribution are integers $\ge m=1$
we fall in scenario 2. In this case, for $k\ge 2$, we have
\begin{eqnarray*}
P(X_1+X_2=k) &=& \sum_{n=1}^{k-1} P(X_1=n)P(X_2=k-n) \\
&=& \sum_{n=1}^{k-1} p(1-p)^{n-1}\cdot p(1-p)^{k-n-1} \\
&=& \sum_{n=1}^{k-1} p^2(1-p)^{k-2} = (k-1)p^2(1-p)^{k-2} \sim \mbox{neg.binom}(2,p). \\
\end{eqnarray*}
Continuing inductively, suppose that for some integer $r\ge 2$, whenever $X_1,X_2,\dots,X_r$ are independent geometric$(p)$ rvs,
$$X_1+X_2+\cdots +X_r\sim \mbox{neg.binom}(r,p).$$
Let $X_1,X_2,\dots,X_r,X_{r+1}$ be independent geometric$(p)$ rvs.  Now, for integer $k\ge r+1$,
\begin{eqnarray*}
P(X_1+\cdots + X_r+X_{r+1}=k) &=& \sum_{n=r}^{k-1} P(X_1+\cdots + X_r=n)P(X_{r+1}=k-n) \\
&=& \sum_{n=r}^{k-1} {n-1\choose r-1} p^r(1-p)^{n-r}\cdot p(1-p)^{k-n-1} \\
&=& \sum_{n=r}^{k-1} {n-1\choose r-1} p^{r+1}(1-p)^{k-(r+1)}\\
&=&{k-1\choose r} p^{r+1}(1-p)^{k-(r+1)} \sim \mbox{neg.binom}(r+1,p),\\
\end{eqnarray*}
where the last equality follows from Fermat's combinatorial identity.  Therefore, the sum of $r$ independent geometric$(p)$ rvs has a neg.binom$(r,p)$ distribution.\\

\vskip .5 in

\noindent {\bf Exercise for the student.}\\
Suppose $X\sim \mbox{Poisson}(a)$ and $Y\sim \mbox{Poisson}(b)$ are independent.  Both pmfs are both supported on the nonnegative integers.
Use the discrete convolution to find the distribution of the sum $X+Y$.\\

\newpage

\noindent {\bf Example.}\\
$X\sim \mbox{Bernoulli}(p)$ and $Y\sim\mbox{Bernoulli}(q)$ are independent. Find the pmf of $X+Y$.\\

\noindent SOLUTION:\\
This can be done easily since the joint pmf of $X$ and $Y$ is easily constructed:
$$\begin{array}{c||cc} & {\bf y=0} & {\bf y=1} \\ \hline\hline
{\bf x=0} & (1-p)(1-q) & (1-p)q \\ \hline
{\bf x=1} & p(1-q) & pq \\ \hline\hline\end{array}$$
The support of $X+Y$ is $\{0,1,2\}$, and

$P(X+Y=0) = (1-p)(1-q)$,

$P(X+Y=1) = p(1-q) + q(1-p)$,

$P(X+Y=2) = pq$.\\

\vskip .2 in

\noindent {\bf Exercise.}\\
Show that discrete convolution is associative: $(p_X*p_Y)*p_Z = p_X*(p_Y*p_Z)$.\\

\vskip 1 in

\noindent {\bf Example.}\label{convobinomialexample}\\
If $X\sim \mbox{binom}(n,p)$ and $Y\sim \mbox{binom}(m,p)$ are independent, show that $X+Y\sim \mbox{binom}(n+m,p)$.\\

\noindent SOLUTION:\\
The support of $X+Y$ is $\{0,1,2,\dots,n+m\}$.  Fix a $u$ in this support.
\begin{eqnarray*}
P(X+Y=u) &=& \sum_{k=-\infty}^{\infty}P(X=k)P(Y=u-k) \\
&=&\sum_{k=0}^{u} {n\choose k}p^k(1-p)^{n-k}\cdot {m\choose u-k}p^{u-k}(1-p)^{m-u+k}\\
&=&p^u(1-p)^{n+m-u}\underbrace{\sum_{k=0}^{u} {n\choose k}{m\choose u-k}}_{={n+m\choose u}\ \mbox{\tiny by normalization}}\\
&=& {n+m\choose u}p^u(1-p)^{n+m-u},
\end{eqnarray*}
which shows $X+Y$ has the pmf of a binom$(n+m,p)$.\\


\newpage


\noindent {\bf The convolution integral.}\label{s:convolutionintegral}\medskip

\noindent Let $X$ and $Y$ be continuous rvs -- $X$ having pdf $f(x)$ and $Y$ having pdf $g(y)$.
We will assume that $X$ and $Y$ are independent so that their joint pdf is $f(x)g(y)$.
We can use the CDF method to find the pdf of $X+Y$.  We start by writing down the CDF of $X+Y$:
$$F_{X+Y}(u) = P(X+Y\le u).$$
To compute this probability we are guided by the picture below:

\includegraphics*[-100,0][200,200]{convo-region.jpg}

\begin{eqnarray*}
F_{X+Y}(u) &=& P(X+Y\le u)\\
&=& \int_{-\infty}^{\infty}\Big(\underbrace{\int_{-\infty}^{u-x} f(x)g(y)\,dy}_{\mbox{a function of $u$ and $x$}}\Big)dx.\\
\end{eqnarray*}
Now we use the Liebniz rule
\begin{eqnarray*}
f_{X+Y}(u) &=& \int_{-\infty}^{\infty} \frac {\partial}{\partial u}\int_{-\infty}^{u-x} f(x)g(y)\,dy\,dx\\
&=&\int_{-\infty}^{\infty}f(x)g(u-x)\,dx =: (f*g)(u),\\
\end{eqnarray*}
and we see the pdf of $X+Y$ is the convolution integral of the individual pdfs.\\

\vskip .3 in

\noindent {\bf Remark.}\\
Just as in an earlier remark regarding the different scenarios in discrete convolutions,
we may be able to restrict the integration from $-\infty$ to $\infty$ based on the support of the densities involved.
For example, if $X$ and $Y$ have the same pdf $f$ whose support is $[0,\infty)$, then $f(x)=0$ for $x<0$ and $f(u-x)=0$ for $u-x<0$, i.e., $u>x$, and
consequently, the convolution reduces to
$$(f*f)(u)=\int_0^u f(x)f(u-x)\,dx$$
in this case.

\newpage

\noindent {\bf Example.}\\
Suppose $X$ and $Y$ are independent $\mbox{N}(0,\sigma^2)$. Recall the pdf is $f(x) = \frac {e^{-\frac {x^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}$ for $-\infty < x<\infty$.
Find the pdf of $X+Y$ using convolutions.\\

\noindent SOLUTION:  The support of $X+Y$ is the entire real line, so let $u\in {\mathbb R}$.\\
\begin{eqnarray*}
f_{X+Y}(u) &=& \int_{-\infty}^{\infty} f(x)f(u-x)\,dx = \int_{-\infty}^{\infty} \frac {e^{-\frac {x^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\frac {e^{-\frac {(u-x)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}\,dx\\\\
&=& \frac 1{2\pi\sigma^2}\int_{-\infty}^{\infty} e^{-\frac {x^2}{2\sigma^2}}e^{-\frac {u^2+x^2-2ux}{2\sigma^2}}\,dx\\
&=& \frac 1{2\pi\sigma^2}\int_{-\infty}^{\infty} e^{-\frac {2x^2-2ux+u^2}{2\sigma^2}}\,dx\\
&=& \frac 1{2\pi\sigma^2}\int_{-\infty}^{\infty} e^{-\frac {2[x^2-ux+\frac {u^2}4 -\frac {u^2}4+\frac {u^2}2]}{2\sigma^2}}\,dx\\
&=& \frac 1{2\pi\sigma^2}\int_{-\infty}^{\infty} e^{-\frac {2[(x-\frac {u}2)^2+\frac {u^2}4]}{2\sigma^2}}\,dx\\
&=& \frac {e^{-\frac {u^2}{4\sigma^2}}}{2\pi\sigma^2}\underbrace{\int_{-\infty}^{\infty} e^{-\frac {(x-\frac {u}2)^2}{\sigma^2}}\,dx}_{=\sqrt{\pi\sigma^2}\mbox{\tiny \ by normalization trick}} = \frac {e^{-\frac {u^2}{4\sigma^2}}}{\sqrt{4\pi\sigma^2}}\sim \mbox{N}(0,2\sigma^2).
\end{eqnarray*}

\vskip .5 in

\noindent {\bf Example.}\\
Suppose $X$ and $Y$ are independent Exp($\lambda$) rvs. Find the pdf of $X+Y$ using convolutions.\\

\noindent SOLUTION: Since $X$ and $Y$ are supported on $[0,\infty)$ we can, for fixed $u>0$, restrict the integration to $[0,u)$:
\begin{eqnarray*}
f_{X+Y}(u) & = & \int_{-\infty}^{\infty} f(x)f(u-x)\,dx \\
&=& \int_0^u \lambda e^{-\lambda x}\lambda e^{-\lambda (u-x)}\,dx\\
&=& \lambda^2\int_0^u e^{-\lambda u}\,dx = \lambda^2 ue^{-\lambda u}.\\
\end{eqnarray*}
So, $$f_{X+Y}(u) = \left\{ \begin{array}{cl} \lambda^2 ue^{-\lambda u} & \mbox{for }u>0 \\ 0 & \mbox{for }u\le 0      \end{array}\right.,$$
which is the pdf of a Gamma$(2,\frac 1{\lambda})$ distribution, i.e., an Erlang$(2,\lambda)$.

\newpage

\noindent {\bf Exercise for the student.}\\
Continue the last example to show inductively that if $X_1,X_2,\dots X_n$ are independent Exp$(\lambda)$ then
$$X_1+X_2+\cdots + X_n\sim \mbox{Gamma}(n,\frac 1{\lambda})=\mbox{Erlang}(n,\lambda).$$

\vskip .4 in

\noindent The next example illustrates the nuances in computing the convolution.\\

\noindent {\bf Example.}\label{uniformpdfconvo}\\
Suppose $X\sim \mbox{unif}(0,2)$ and $Y\sim\mbox{unif}(0,3)$ are independent rvs. Use the convolution to find the pdf of $X+Y$.\\

\noindent SOLUTION:  The support of $X+Y$ is $[0,5]$, so let $0<u<5$.\\
\begin{eqnarray*}
f_{X}(x) &=& \frac 121_{(0,2)}(x)\quad \mbox{for }0<x<2, \quad \mbox{and}\\
f_{Y}(y) & = & \frac 131_{(0,3)}(y) \quad\mbox{for }0<y<3.\\
f_{X+Y}(u) &=& \int_{-\infty}^{\infty}\frac 12 1_{(0,2)}(x)\cdot \frac 13 1_{(0,3)}(u-x)\,dx.
\end{eqnarray*}
The integrand is $\frac 16$ when both $0<x<2$ and $0<u-x<3$ ($u-3<x<u$).  Therefore,
$$\max\{0,u-3\} < x < \min\{2,u\}.$$
When $0<u<2$, we have $0<x<u$, and $f_{X+Y}(u) = \int_0^u\frac 16\,dx = \frac u6$.

\noindent When $2<u<3$, we have $0<x<2$, and $f_{X+Y}(u) = \int_0^2\frac 16\,dx = \frac 13$.

\noindent When $3<u<5$, we have $u-3<x<2$, and $f_{X+Y}(u)=\int_{u-3}^2\frac 16\,dx = \frac {5-u}6$.

$$f_{X+Y}(u) = \left\{ \begin{array}{cl} \frac u6  & \mbox{for }0<u<2 \\ \frac 13 & \mbox{for }2 < u < 3\\ \frac {5-u}6 & \mbox{for }3<u<5\\ 0 & \mbox{elsewhere}   \end{array} \right..$$

\noindent Here's a plot of this pdf:

\includegraphics*[-100,0][260,80]{convo-uniforms-pdf.jpg}

\newpage

\noindent {\bf Remark.}\\
The idea of the convolution can still apply when the rvs are {\em not} independent. I will illustrate when $X$ and $Y$ are jointly continuous with joint pdf $f(x,y)$.

\begin{eqnarray*}
F_{X+Y}(u) &=& P(X+Y\le u) \\
&=& \int_{-\infty}^{\infty}\underbrace{\int_{-\infty}^{u-x}f(x,y)\,dy}_{\mbox{\tiny funtion of $u$}}dx.
\end{eqnarray*}
Now apply the Liebniz rule:
$$f_{X+Y}(u) = \int_{-\infty}^{\infty}f(x,u-x)\,dx.$$

\vskip .5 in

\noindent {\bf Exercise for the student.}\\
Suppose $X$ and $Y$ are jointly continuous with joint pdf $f_{X,Y}(x,y) = (x+y)1_{(0,1)}(x)1_{(0,1)}(y)$.  Find the pdf of $X+Y$.  Hint: since these rvs are {\em not} independent
use the previous remark and when you compute the integral consult the example involving the uniform pdfs on page \pageref{uniformpdfconvo}.\\


%\includegraphics*[-100,0][160,140]{convo-uniforms.jpg}

%\noindent In the yellow region, $u$ ranges from $0$ to $2$.  In the green region $u$ ranges from $2$ to $3$. In the blue region $u$ ranges from $3$ to $5$.
%The bounds of the integral will differ between $u$ in these regions.

%If $0<u<2$, then $f_{X+Y}(u) = \frac 16 \int dx$


\vskip 1 in

\noindent {\bf Important facts about independent rvs.}\label{importantfactindependentrvs}\\
If $X$ and $Y$ are independent, then $h_1(X)$ and $h_2(Y)$ are independent for any functions $h_1$ and $h_2$. More generally, if
$$X_1,X_2,\dots,X_k,X_{k+1},\dots, X_n$$
are independent, then
$$h_1(X_1,X_2,\dots,X_k)\quad\mbox{and} \quad h_2(X_{k+1},\dots,X_n)$$
are also independent for any functions $h_1$ and $h_2$.


\newpage


The law of the unconscious statistician naturally extends to functions of many jointly distributed random variables.  We only present the
result when we have just two jointly distributed rvs the analogous result holds for more than two rvs.\\

\noindent {\bf Law of the Unconscious Statistician re-visited.}\label{s:lotus}\\
Suppose $X$ and $Y$ are jointly distributed and $g:{\mathbb R}^2\to {\mathbb R}$ is a real-valued function. Then
\begin{eqnarray*}
E[g(X,Y)] & = & \sum_{y=-\infty}^{\infty} \sum_{x=-\infty}^{\infty} g(x,y)P(X=x,Y=y) \qquad \mbox{if $X,Y$ are jointly discrete}\\
E[g(X,Y)] & = & \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x,y)f_{X,Y}(x,y)\,dxdy \qquad \mbox{if $X,Y$ are jointly continuous}
\end{eqnarray*}
assuming the expressions shown exist.\\

\vskip .2 in


\noindent {\bf Example.}\\
$X$, $Y$ are jointly continuous with joint pdf $f_{X,Y}(x,y) = e^{-y}$ for $0<x<y$.  Find $E\left(\frac XY\right)$.\\

\noindent SOLUTION:
\begin{eqnarray*}
E\left(\frac XY\right) & = & \int_0^{\infty} \int_0^y \frac xy e^{-y}\,dxdy\\
 & = & \int_0^{\infty} \left. \frac {x^2}{2y} e^{-y}\right|_{x=0}^{x=y}\,dy\\
 &=& \frac 12 \int_0^{\infty} ye^{-y}\,dy = \frac 12.
\end{eqnarray*}

\vskip .2 in
\noindent The following is a very useful {\em\bf corollary:}\label{expectedvalueofaproduct}

\noindent When $X$ and $Y$ are independent and the function $g(x,y)=h_1(x)h_2(y)$ is separable
$$E[h_1(X)h_2(Y)] = E[h_1(X)]E(h_2(Y)].$$

\bigskip

\noindent Proof: We'll prove this in the case where the rvs are independent and jointly continuous so that the joint pdf is $f_X(x)f_Y(y)$. The proof for the jointly discrete case is similar.

\begin{eqnarray*}
E[h_1(X)h_2(Y)] &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} h_1(x)h_2(y)f_X(x)f_Y(y)\,dxdy\\
&=& \int_{-\infty}^{\infty} h_2(y)f_Y(y)\underbrace{\int_{-\infty}^{\infty} h_1(x)f_X(x)\,dx}_{=E[h_1(X)]}dy\\
&=& E[h_1(X)]\int_{-\infty}^{\infty} h_2(y)f_Y(y)dy = E[h_1(X)]E[h_2(Y)].\\
\end{eqnarray*}\hfill$\Box$

\vskip .2 in

\noindent An important and immediate application of this corollary applies to moment generating functions which we discuss now.\\

\newpage

\noindent {\bf Moment generating functions - part 3.}\label{mgfpart3}\\

\noindent Recall that if the MGF of an rv exists it is given by $M_X(\theta)=E(e^{\theta X})$.\\

\vskip .5 in

\noindent {\bf Theorem.}\\
Let $X$ and $Y$ be jointly distributed rvs that each possess an MGF. Then
$$M_{X+Y}(\theta) = M_X(\theta)M_Y(\theta).$$

\bigskip

\noindent Proof:
\begin{eqnarray*}
M_{X+Y}(\theta) &=& E(e^{\theta(X+Y)})\\
 &=& E(e^{\theta X}e^{\theta Y})\\
&=& E(e^{\theta X})E(e^{\theta Y}) \quad \mbox{(previous corollary applied to $h_1(x)=e^{\theta x}, \ h_2(y)=e^{\theta y}$)}\\
&=& M_X(\theta)M_Y(\theta).
\end{eqnarray*}\hfill $\Box$

\vskip .5 in

\noindent {\bf Remark.}\\
The last theorem is often used in conjunction with the following important property of moment generating functions:\medskip

{\em If $U$ and $V$ have moment generating functions that agree and are finite in an open

neighborhood of $\theta=0$, then $U$ and $V$ have the same probability distribution (or law).}\medskip

\noindent We may use the notation $U\stackrel{d}{=}V$ or $U\stackrel{{\cal L}}{=}V$ to mean that $U$ and $V$ have the same distribution/law,\\

\noindent Here are two examples we did using convolutions:\\

\noindent {\bf Example.}\label{mgfbinomialexample}\\
Suppose $X\sim \mbox{binom}(n,p)$ and $Y\sim \mbox{binom}(m,p)$ are independent.  Find the distribution of $X+Y$.\\

\noindent SOLUTION:\\
We know
$$M_X(\theta) = (1-p + pe^{\theta})^n\quad \mbox{and}\quad M_Y(\theta) = (1-p+pe^{\theta})^m.$$
By the theorem,
\begin{eqnarray*}
M_{X+Y}(\theta) &=& M_X(\theta)M_Y(\theta)\\
&=& (1-p + pe^{\theta})^n(1-p + pe^{\theta})^m = (1-p + pe^{\theta})^{n+m},
\end{eqnarray*}
which is the moment generating function for a binom$(n+m,p)$ distribution. Therefore, $X+Y$ must have a binom$(n+m,p)$ distribution.\\

\newpage

\noindent {\bf Example.}\\
Suppose $X_1,X_2$ are independent Exp$(\lambda)$ rvs. Find the distribution of $X_1+X_2$.\\

\noindent SOLUTION:\\
We know
$$M_{X_1}(\theta) = M_{X_2}(\theta)=\left(1-\frac {\theta}{\lambda}\right)^{-1}.$$
By the theorem,
\begin{eqnarray*}
M_{X+Y}(\theta) &=& M_{X_1}(\theta)M_{X_2}(\theta)\\
&=& \left(1-\frac {\theta}{\lambda}\right)^{-1}\left(1-\frac {\theta}{\lambda}\right)^{-1}=\left(1-\frac {\theta}{\lambda}\right)^{-2},
\end{eqnarray*}
which is the moment generating function for a Gamma$(2,\frac 1{\lambda})$ distribution. Therefore, $X_1+X_2$ must have a Gamma$(2,\frac 1{\lambda})$ distribution.\\

\vskip .5 in

\noindent {\bf Remark.}\\
A closer look at the proof of the theorem will show that the theorem remains true for {\em any} finite number of (not just two) rvs:
if $X_1,X_2,\dots,X_n$ are independent Exp$(\lambda)$\footnote{A good mnemonic:
the sum of i.i.d.~exponentials has a Gamma distribution.}, then
$$M_{\sum_{i=1}^nX_i}(\theta) = \prod_{i=1}^n M_{X_i}(\theta) = \prod_{i=1}^n \left(1 - \frac {\theta}{\lambda}\right)^{-1} = \left(1 - \frac {\theta}{\lambda}\right)^{-n},$$
which is the moment generating function of a Gamma$(n,\frac 1{\lambda})$ and, therefore, $\sum_{i=1}^n X_i \sim \mbox{Gamma}(n,\frac 1{\lambda})$.  \smallskip


\vskip .5 in

\noindent {\bf Notation/terminology}:\\
Very often we deal with random variables that are independent {\em and} all have the same probability distribution, and in this case we will say the
random variables are {\bf iid} which is short for {\em independent and identically distributed}\label{iiddef}.\\







\newpage


\noindent {\bf Conditional distributions.}\label{s:conditionaldistributions}\\

\noindent The general problem here is when we have a jointly distributed collection of rvs, we may want to find the distribution
of some function of these given information about the values of a subcollection.\\

\noindent We first discuss the case of two jointly discrete rvs.  We need to recall the {\em conditional probability formula} from
page \pageref{conditionalprobabilityformula}:\\
\begin{center}
{\em For any event $A$, when $P(B) >0$, $P(A|B) = \dfrac {P(A\cap B)}{P(B)}.$}
\end{center}


\vskip .2 in

\noindent What makes this formula so useful is that the conditioning event involving discrete rvs are typically events of positive probability.\\

\vskip .2 in

\noindent {\bf Example.}\\
Suppose $X$ and $Y$ are independent with
$$X\sim\mbox{binom}(N_1,p)\quad \mbox{and}\quad Y\sim\mbox{binom}(N_2,p).$$
Find the conditional distribution of $X$ given $X+Y=n$.\\

\noindent SOLUTION:\\
We are looking for
$P(X=k|X+Y=n)$
as a function of $k$.  We may use the notation
$$p_{X|X+Y}(k|n) := P(X=k|X+Y=n).$$
Since $X+Y=n$ we assume $n\in \mbox{supp}(X+Y)=\{0,1,\dots,N_1+N_2\}$.
It should be clear that $k=0,1,\dots,n$.  Also, from the examples involving the binomial on pages \pageref{convobinomialexample} and \pageref{mgfbinomialexample},
we see the sum $X+Y$ has a binom$(N_1+N_2,p)$ distribution. Now, applying the conditional probability formula above, we obtain
\begin{eqnarray*}
P(X=k|X+Y=n) &=& \frac {P(X=k,X+Y=n)}{P(X+Y=n)}\\
&=& \frac {P(X=k,Y=n-k)}{{N_1+N_2\choose n}p^n(1-p)^{N_1+N_2-n}}\\
&=& \frac {P(X=k)P(Y=n-k)}{{N_1+N_2\choose n}p^n(1-p)^{N_1+N_2-n}}\\
&=& \frac {{N_1\choose k}p^k(1-p)^{N_1-k}{N_2\choose n-k}p^{n-k}(1-p)^{N_2-n+k}}{{N_1+N_2\choose n}p^n(1-p)^{N_1+N_2-n}}\\
&=& \frac {{N_1\choose k}{N_2\choose n-k}}{{N_1+N_2\choose n}}\\
\end{eqnarray*}
which is the hypergeometric distribution!


\newpage

\noindent {\bf Remark.}\\
In the last example, we are given that $X+Y=n$ occurred.
If $P(X+Y=n)=0$ then, since $X+Y$ is discrete, the event $(X+Y=n)$ could not have occurred, and, therefore,
it must be the case that $P(X+Y=n)>0$. This allowed us to use the conditional probability formula.

\vskip .2 in

\noindent We now do an example involving jointly continuous rvs.\\

\noindent {\bf Example.} (uniform distribution over the region $D\subseteq {\mathbb R}^2$)\label{2duniform}\\
Suppose $X$ and $Y$ are jointly continuous and that $(X,Y) \sim \mbox{uniform}(D)$ which means $X,Y$ has the joint pdf
$$f_{X,Y}(x,y) = \left\{ \begin{array}{cl} \frac 1{\mbox{area}(D)} & \mbox{for }(x,y)\in D \\ 0 & \mbox{elsewhere}    \end{array}\right..$$
This is the model where every point $(x,y)\in D$ is equally likely.  To fix ideas let's
suppose that $D$ is the unit disk in the plane centered at $(0,0)$: $D=\{(x,y):x^2+y^2\le 1\}$ -- see figure.
Compute the probability that $Y\ge \frac 12$ given $X\ge \frac 12$.\\

\includegraphics*[-140,0][160,140]{uniformondisk1.jpg}
\begin{center}{\bf Figure.} support of the uniform on the unit disk, blue region is $\{x\ge \frac 12\}\cap \{y\ge \frac 12\}$.
\end{center}


\noindent SOLUTION:\\
We are trying to compute $P(Y\ge \frac 12|X\ge \frac 12)$.  Since $P(X\ge \frac 12)>0$ we apply the conditional probability formula:
\begin{center}$P(Y\ge \frac 12|X\ge \frac 12) = \dfrac {P(X\ge \frac 12,Y\ge \frac 12)}{P(X\ge \frac 12)}.$\end{center}
Much like the example we did on page \pageref{2duniformexample1} when dealing with a uniform distribution of a region, probability is
just the proportion of the region occupied by the event.  From this perspective,
the easiest way to evaluate these probabilities is to exploit the geometry of the situation -- for example,
compute areas of sectors and triangles and use similar triangles to get edge lengths.\\

\noindent I leave for the student to check that
\begin{center}$P(X\ge \frac 12,Y\ge \frac 12) = \frac 1{12} - \frac {\sqrt{3}-1}{4\pi}$ and $P(X\ge \frac 12)=\frac 13-\frac {\sqrt{3}}{4\pi}$,\end{center}
and $P(Y\ge \frac 12|X\ge \frac 12) \approx .128$.\\

\newpage

\noindent In the last example $P(Y\ge \frac 12|X\ge \frac 12)$ was computed using the conditional probability formula as the conditioning event has positive probability.
Let's continue with this example but try to compute
\begin{center}$P(Y\ge \frac 12|X=\frac 12)$\end{center}
instead.  We run into a problem: we {\em cannot} use the conditional probability formula since $P(X=\frac 12)=0$.\\

\noindent We now try to rectify this situation.\\

\noindent {\bf\em Conditional pdfs.}\label{conditionalpdf}

If $X$ and $Y$ have the joint pdf $f_{X,Y}(x,y)$ and marginal pdf $f_X(x)$, then for any $x$ for which $f_X(x) >0$ we define
the {\bf\em conditional pdf $f_{Y|X}(y|x)$ of $Y$ given $X=x$} by
$$f_{Y|X}(y|x) = \frac {f_{X,Y}(x,y)}{f_X(x)}.$$
In this formula we think of the value of $x$ as {\em fixed} and treat this conditional density as a function in the variable $y$.\\

\noindent {\em How to use conditional densities to compute conditional probabilities having conditioning event $X=x$?}\\

\noindent {\bf Fact:} \\
If $X,Y$ are jointly continuous with joint pdf $f_{X,Y}(x,y)$, then for any $a<b$
$$P(a< Y< b|X=x) = \int_a^b f_{Y|X}(y|x)\,dy.$$

\bigskip

\noindent Specifically, the answer to the above question is: we integrate the corresponding conditional pdf as a function of $y$ fixing the value $x$.\\

\noindent {\bf Example.} {\em Continuing with the last example\dots}\\
If we are interested in computing $P(Y\ge \frac 12|X=\frac 12)$ then we recognize that the rv defining the conditioning event is $X$ and the rv in the event of interest is $Y$ and
therefore, we'd need to find the conditional density of $Y$ given $X=x$ with the value $x=\frac 12$.

We first compute the marginal $f_X(x)$: When $-1<x<1$ we have
\begin{center}$f_X(x)= \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}\frac 1{\pi}\,dy = \frac {2\sqrt{1-x^2}}{\pi}.$\end{center}
Notice when $x=\frac 12$, we have $f_X(\frac 12) = \frac {\sqrt{3}}{\pi}>0.$ Therefore, for $-\frac {\sqrt{3}}2 < y<\frac {\sqrt{3}}{2}$,
\begin{center}$f_{Y|X}(y|\frac 12) = \dfrac {f_{X,Y}(\frac 12,y)}{f_X(\frac 12)} = \dfrac {\frac 1{\pi}}{\frac {\sqrt{3}}{\pi}}=\frac 1{\sqrt{3}},$\end{center}
i.e., $Y|X=\frac 12\sim \mbox{uniform}(-\frac {\sqrt{3}}2,\frac {\sqrt{3}}2)$.\\

\newpage

\noindent Finally, to compute $P(Y\ge \frac 12|X= \frac 12)$ we integrate $f_{Y|X}(y|\frac 12)$ from $y=\frac 12$ to $y=\frac {\sqrt{3}}2$
(to stay in the support of this conditional density when $x=\frac 12$):
\begin{center}$P(Y\ge \frac 12|X=\frac 12) = \int_{\frac 12}^{\frac {\sqrt{3}}2} \frac 1{\sqrt{3}}\,dy = \frac {\sqrt{3}-1}{2\sqrt{3}}.$\end{center}

\vskip .2 in

\noindent {\bf Remark.}\\
When the conditioning event is a continuous rv equal to a single value
we {\bf\em must} first find the conditional distribution given this event and then
use this conditional distribution to compute the conditional probability.
On the other hand, if the conditioning event has positive probability, then we use the conditional
probability formula to find the conditional probability.
These two situations are {\em not} the same; for instance, in the last example
\begin{center}$
P(Y\ge \frac 12|X\ge \frac 12) =\frac {\int_{\frac 12}^{\frac {\sqrt{3}}2} \int_{\frac 12}^{\sqrt{1-x^2}} f_{X,Y}(x,y)\,dydx}{\int_{\frac 12}^1 f_X(x)\,dx}$\end{center}
is not the same as
\begin{center}$\int_{\frac 12}^{\frac {\sqrt{3}}2} P(Y\ge \frac 12|X=x)\,dx=\int_{\frac 12}^{\frac {\sqrt{3}}2} \int_{\frac 12}^{\sqrt{1-x^2}} f_{Y|X}(y|x)\,dydx.$\end{center}
The student should show that these two expressions do not evaluate to the same value.
Generally speaking, a conditional pdf is only a pdf in the first variable and it is {\em not} a pdf in the conditioning variable.\\

\vskip .2 in

\noindent {\bf Example.}\\
Continuing from the example on page \pageref{jointpdfexamplemarginals}\dots\\
Suppose $X$ and $Y$ are jointly continuous with joint pdf $f(x,y)=e^{-y}$ for $0<x<y<\infty$.  \\
Compute \\
(a) the conditional density of $X$ given $Y=y$\\
(b) $P(X\ge 1|Y=2)$\\
(c) $P(X\ge 1|Y\ge 2)$\\

\noindent SOLUTION:\\
(a) The marginal pdf of $Y$ from page \pageref{jointpdfexamplemarginals} is $f_Y(y)=ye^{-y}$ for $y>0$ and, therefore,
$$f_{X|Y}(x|y) = \frac {e^{-y}}{ye^{-y}} = \frac 1y\quad\mbox{for }0<x<y,$$
i.e., $X|Y=y\sim \mbox{uniform}(0,y)$.\\
(b) When $y=2$, $X|Y=2\sim \mbox{uniform}(0,2)$.  Thus,
$$P(X\ge 1|Y=2) = \int_1^{\infty}f_{X|Y}(x|2)\,dx = \int_1^{2}\frac 12\,dx = \frac 12.$$

\bigskip

\noindent I leave as an {\bf exercise} for the student to do part (c), the answer is $P(X\ge 1|Y\ge 2)=\frac 23$.


\newpage

\noindent Since the conditional pdf of $X$ given $Y=y$ is
$$f_{X|Y}(x|y) = \frac {f_{X,Y}(x,y)}{f_Y(y)},$$
we can easily solve for the joint pdf of $X$ and $Y$:
$$f_{X,Y}(x,y) = f_{X|Y}(x|y)f_Y(y).$$

\noindent A common practice in probability and statistics (easily in Bayesian statistics) is to model the conditional distribution
of one rv given another, and then specify the
(marginal) distribution of the other like in this next example.\\

\noindent {\bf Example.}\\
Suppose $X|Y=y\sim \mbox{Exp}(y)$ and $Y\sim \mbox{Gamma}(\alpha,1)$.\\
(a) Write down the joint pdf of $X$ and $Y$.\\
(b) From the joint pdf you found in part (a) compute the marginal pdf of $X$.\\

\noindent SOLUTION:\\
(a) We are told $f_{X|Y}(x|y) = ye^{-yx}$ for $x>0$ and $f_Y(y)=\frac {y^{\alpha-1}e^{-y}}{\Gamma(\alpha)}$ for $y>0$. Therefore,
$$f_{X,Y}(x,y) = ye^{-yx}\cdot \frac {y^{\alpha-1}e^{-y}}{\Gamma(\alpha)} = \frac {y^{\alpha}e^{-(1+x)y}}{\Gamma(\alpha)}\quad \mbox{for }x>0,y>0.$$
(b) Fix an $x>0$. Then
$$f_X(x) = \int_0^{\infty} \frac {y^{\alpha}e^{-(1+x)y}}{\Gamma(\alpha)}\,dy=\frac 1{\Gamma(\alpha)}\int_0^{\infty} y^{\alpha}e^{-(1+x)y}\,dy.$$
We recognize the integrand of the last integral as being the functional form of a Gamma pdf having shape parameter $\alpha+1$ and scale parameter $\frac 1{x+1}$.
By the normalization trick it follow that
$$\frac 1{\Gamma(\alpha)}\int_0^{\infty} y^{\alpha}e^{-(1+x)y}\,dy = \frac 1{\Gamma(\alpha)}\left(\frac 1{x+1}\right)^{\alpha+1}\Gamma(\alpha+1)=\frac {\alpha}{(x+1)^{\alpha+1}}.$$
I just so happens that this pdf is recognizable pdf: $X$ is Pareto$(\alpha)$.

\vskip .5 in
\noindent The next example works with rvs of mixed types.\\

\newpage

\noindent {\bf Example.}\label{betabinomialexample}\\
Suppose $X|\Theta \sim \mbox{binom}(n,\Theta)$ and $\Theta\sim \mbox{uniform}(0,1)$.  Derive \\
(a) the joint distribution of $X,\Theta$,\\
(b) the marginal distribution of $X$,\\
(c) the conditional distribution of $\Theta$ given $X=x$.\\

\noindent SOLUTION (sketch):\\
We're told $f_{\Theta}(\theta)=1$ for $0<\theta<1$ and $p_{X|\Theta}(x|\theta) = {n\choose x}\theta^x(1-\theta)^{n-x}$ for $x=0,1,\dots,n$.\\
(a) $p_{X,\Theta}(x,\theta) = {n\choose x}\theta^x(1-\theta)^{n-x}\quad \mbox{for }x=0,1,\dots,n,\ 0<\theta <1.$\\
(b) Fix $x\in \{0,1,\dots,n\}$.
\begin{eqnarray*}
p_X(x) &=& \int_0^1 {n\choose x}\theta^x(1-\theta)^{n-x}\,d\theta\\
&=& {n\choose x}\int_0^1 \theta^x(1-\theta)^{n-x}\,d\theta\\
&=& {n\choose x}\frac {\Gamma(x+1)\Gamma(n-x+1)}{\Gamma(n+2)}\underbrace{\int_0^1 \frac {\Gamma(n+2)}{\Gamma(x+1)\Gamma(n-x+1)}\theta^x(1-\theta)^{n-x}\,d\theta}_{=1\ \mbox{since it is Beta}(x+1,n-x+1)}\\
&=& {n\choose x}\frac {\Gamma(x+1)\Gamma(n-x+1)}{\Gamma(n+2)}\\
&=& \frac {n!}{x!(n-x)!}\frac {x!(n-x)!}{(n+1)!}=\frac 1{n+1},
\end{eqnarray*}
i.e., $X\sim $ discrete uniform\label{discreteuniform1} on $\{0,1,\dots,n\}$.\\
(c) Assuming $x\in \{0,1,\dots,n\}$ is fixed,
$$f_{\Theta|X}(\theta|x) = \frac {p_{X,\Theta}(x,\theta)}{p_X(x)}=\frac {\frac {n!}{x!(n-x)!}\theta^x(1-\theta)^{n-x}}{\frac 1{n+1}} = \frac {\Gamma(n+2)}{\Gamma(x+1)\Gamma(n-x+1)}\theta^x(1-\theta)^{n-x}$$
for $0<\theta < 1$ which says $\Theta|X=x\sim \mbox{Beta}(x+1,n-x+1)$.\\

\vskip .2 in

\noindent {\bf Exercise for the student.}\\
Suppose $X|Y=y\sim \mbox{uniform}(-y,y)$ and $Y\sim \mbox{Gamma}(2,1)$.  Find the pdf of $X$.\\
Do you recognize this distribution?\\

\vskip .4 in

\noindent {\bf\em Law of total probability -- revisited.}\\
Just as we learned earlier in the course, the law of total probability can be an effective tool to compute unconditional probabilities.\\

\noindent {\bf Example.}\\
Let $X,Y,Z\sim \mbox{iid Exp}(1)$. Compute $P(X+Y<Z)$.\\

\newpage

\noindent SOLUTION:\\
On one hand
$$P(X+Y<Z) = \int\int\int_{0<x+y<z}e^{-x}e^{-y}e^{-z}\,dxdydz$$
and we can try to parametrize the region. But, using the law of total probability we also have
\begin{eqnarray*}
P(X+Y<Z) &=&
\int_{-\infty}^{\infty}P(X+Y<Z|Z=z)f_Z(z)\,dz\\
&=&
\int_{0}^{\infty}P(X+Y<Z|Z=z)e^{-z}\,dz.
\end{eqnarray*}
Now, the term
$P(X+Y<Z|Z=z) = P(X+Y<z|Z=z)=P(X+Y<z)$ since $Z$ is independent of $X+Y$.  Recalling that $X+Y\sim \mbox{Gamma}(2,1)$, i.e., has density $f(u) = ue^{-u}$ for $u>0$,
we have
$$P(X+Y<z) = \int_0^zue^{-u}\,du=1-e^{-z}-ze^{-z}\quad \mbox{(details omitted).}$$
We substitute this expression into the integral above
\begin{eqnarray*}
P(X+Y<Z)
&=&
\int_{0}^{\infty}P(X+Y<z)e^{-z}\,dz \\
&=&
\int_{0}^{\infty}(1-e^{-z}-ze^{-z})e^{-z}\,dz \\
&=&
\int_{0}^{\infty}e^{-z}\,dz - \int_0^{\infty}e^{-2z}\,dz - \int_0^{\infty}ze^{-2z}\,dz\\
&=&
1 - \frac 12 - \frac 14 \\
&=& \frac 14,
\end{eqnarray*}
where we used the normalization trick in the last integral.




\newpage

\noindent We come back now to the problem of finding the distribution of a transformation of a continuous random variable or jointly continuous rvs (if there's more than one).
We've already had some exposure to the CDF method, method of convolutions, and the moment generating function method. We now discuss
another method, but initially present the 2-dimensional version.\\

\noindent {\bf Method of Jacobians.}\label{methodofjacobians}\\

Suppose we know the joint pdf $f_{X,Y}(x,y)$ of $X$ and $Y$ and we consider the rvs $U$ and $V$ defined by
$$U=g_1(X,Y)\quad \mbox{and} \quad V=g_2(X,Y).$$
What's the joint pdf $f_{U,V}(u,v)$ of $U$ and $V$?\\

\noindent We assume the mapping $(x,y)\mapsto (u,v):=(g_1(x,y),g_2(x,y))$ is continuously differentiable and one-to-one (so that it is invertible).
In this case it follows
$x=h_1(u,v)$ and $y=h_2(u,v)$ and the answer to the question is given by the following\\

\noindent {\bf Theorem.} (Jacobian transformation in 2-$d$)\\
Suppose $X,Y$ are jointly continuous with joint pdf $f_{X,Y}$ having support ${\cal A}=\mbox{supp}(f_{X,Y})$ and
$$u=g_1(x,y)\quad \mbox{and}\quad v=g_2(x,y)$$
is a continuously differentiable one-to-one transformation of ${\cal A}$ into ${\cal B}$. Then, if we denote the
inverse transformation by
$$x=h_1(u,v)\quad\mbox{and}\quad y=h_2(u,v),$$
the joint pdf of $U=g_1(X,Y)$ and $V=g_2(X,Y)$ is given by
$$f_{U,V}(u,v) = f_{X,Y}(h_1(u,v),h_2(u,v))\cdot |J|,$$
where
$$J:=\mbox{det}\left(\begin{array}{cc} \frac {\partial h_1(u,v)}{\partial u} & \frac {\partial h_1(u,v)}{\partial v} \\
\frac {\partial h_2(u,v)}{\partial u} & \frac {\partial h_2(u,v)}{\partial v}  \end{array}\right) =
\frac {\partial h_1(u,v)}{\partial u} \frac {\partial h_2(u,v)}{\partial v} - \frac {\partial h_1(u,v)}{\partial v} \frac {\partial h_2(u,v)}{\partial u}$$
is the {\bf\em Jacobian determinant}\label{jacobian}.\\

\vskip .5 in

\noindent {\bf Remark.}\\
The CDF method and method of Jacobians are the two most widely used methods for finding the distribution of functions of continuous random variables.\\



\newpage


\noindent {\bf Example.}\\
Suppose $X,Y$ are independent Exp$(1)$ rvs so that $f_{X,Y}(x,y) = e^{-x}e^{-y}$ for $x>0,\ y>0$.  Derive the joint pdf of $U,V$, where
$$U=X+Y\quad\mbox{and\quad }V = Y.$$

\bigskip

\noindent SOLUTION:\\
The transformation $u=x+y$ and $v=y$ is invertible:
$$x = u-v =:h_1(u,v)\quad \mbox{and}\quad y=v=:h_2(u,v).$$
Since $x>0$, $u-v>0\implies u>v$.  Since $y>0$, $v>0$. Therefore, the support of $U,V$ is $0<v<u<\infty$. The Jacobian of the inverse transformation is
$$J = \mbox{det}\left( \begin{array}{cc} 1 & -1 \\ 0 & 1 \end{array}   \right) = 1 \implies |J|=1.$$
Consequently, the joint pdf of $U,V$ is
\begin{eqnarray*}
f_{U,V}(u,v) &=& f_{X,Y}(u-v,v)|J|\\
&=&e^{-(u-v)}e^{-v}\cdot 1 \\
&=& \left\{ \begin{array}{cl}e^{-u} & \mbox{for }0<v<u<\infty \\ 0 & \mbox{elsewhere}\end{array}\right..
\end{eqnarray*}

\vskip .2 in
\noindent {\bf Remark.}\\
At this point we can compute the marginal pdf of $U$:
$$f_U(u) = \int_0^u e^{-u}\,dv = ue^{-u}\quad \mbox{for }u>0.$$

\bigskip

\noindent What if the transformation had been $U=X+Y$ and $V=X-Y$ instead?  Derive the joint pdf of $U,V$ now.\\

\noindent I'll sketch a solution; I'll leave the details to the student. \\
The transformation $u=x+y,\ v=x-y$ has the inverse transformation
$x = \frac 12(u+v)$ and $y=\frac 12(u-v)$.  The support of $U,V$ is $u>0,\ -u<v<u$. The Jacobian is $J=-\frac 12$.
$$f_{U,V}(u,v) = e^{-\frac 12(u+v)}e^{-\frac 12(u-v)}|J|=\frac 12e^{-u}\quad \mbox{for }u>0,\ -u<v<u.$$
Notice that the marginal pdf of $U$ is the same as before (as we should).  Also, the student can check:
$$f_V(v) = \int_{|v|}^{\infty} \frac 12e^{-u}\,du = \frac 12e^{-|v|}\quad\mbox{for }v>0,$$
i.e., $V$ has a double-exponential (Laplace) distribution. \\

\noindent As these last examples show, if the goal was to find a marginal distribution,
then we can still use the method of Jacobians.  This may require us to first find or create artificial variables to
obtain a one-to-one transformation.  It will not matter how we choose the artificial
variable(s) as when we integrate them out we will arrive to the same marginal.



\newpage

\noindent {\bf Example.} (The Beta distribution)\label{betadistributionexample}\\
Suppose $X_1\sim \mbox{Gamma}(\alpha_1,1)$ and $X_2\sim \mbox{Gamma}(\alpha_2,1)$ are {\em independent}.\\
Find the pdf of $U=\frac {X_1}{X_1+X_2}$.\\

\noindent SOLUTION: \\
To apply the method of Jacobians we would need to introduce another rv, say,
$V$ to make the transformation $(x_1,x_2)\mapsto (u,v)$ one-to-one. The choice of $V$ will, in fact, not matter.  So, for now we'll choose $V=X_1+X_2$.\\

The joint pdf of $X_1,X_2$ is $f_{X_1,X_2}(x_1,x_2) = \frac {x_1^{\alpha_1-1}e^{-x_1}}{\Gamma(\alpha_1)}\cdot \frac {x_2^{\alpha_2-1}e^{-x_2}}{\Gamma(\alpha_2)}$ for $x_1>0,x_2>0$.
The inverse transformation is $x_1 = uv$, $x_2=v-uv=(1-u)v$ with Jacobian
$$J = \mbox{det}\left(\begin{array}{cc} v & u \\ -v & 1-u  \end{array}\right) = v.$$ %\implies |J|=|v|=v \quad\mbox{when }v>0.$$
Since $x_1>0$ and $x_2>0$, we have $u>0$, $v=x_1+x_2>0$, and $(1-u)v>0\implies u<1$. Therefore, the support of $U,V$ is $0<u<1$ and $v>0$.  For such $u,v$,
\begin{eqnarray*}
f_{U,V}(u,v) &=& f_{X,Y}(uv,(1-u)v)|v|\\
 &=& \frac {[uv]^{\alpha_1-1}e^{-uv}}{\Gamma(\alpha_1)}\cdot \frac {[(1-u)v]^{\alpha_2-1}e^{-(1-u)v}}{\Gamma(\alpha_2)}\cdot v \quad (\mbox{since }|v|=v>0)\\
 &=& \frac {u^{\alpha_1-1}v^{\alpha_1-1}e^{-uv} (1-u)^{\alpha_2-1}v^{\alpha_2-1}e^{-(1-u)v}v}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\\
 &=& \frac {u^{\alpha_1-1}(1-u)^{\alpha_2-1}\cdot v^{\alpha_1+\alpha_2-1}e^{-v}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\quad \mbox{for }0<u<1,\ v>0.\\
\end{eqnarray*}
The marginal pdf of $U$ is
\begin{eqnarray*}
f_U(u) & = & \int_0^{\infty} \frac {u^{\alpha_1-1}(1-u)^{\alpha_2-1}\cdot v^{\alpha_1+\alpha_2-1}e^{-v}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\,dv\\
& = & \frac {u^{\alpha_1-1}(1-u)^{\alpha_2-1}}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\underbrace{\int_0^{\infty} v^{\alpha_1+\alpha_2-1}e^{-v}\,dv}_{=\Gamma(\alpha_1+\alpha_2)}\\
&=& \frac {\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}u^{\alpha_1-1}(1-u)^{\alpha_2-1}\quad \mbox{for }0<u<1.
\end{eqnarray*}
That is, $U=\frac {X_1}{X_1+X_2}\sim \mbox{Beta}(\alpha_1,\alpha_2)$.  In fact,
$$f_{U,V}(u,v) =
\frac {\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}u^{\alpha_1-1}(1-u)^{\alpha_2-1}\cdot \frac {v^{\alpha_1+\alpha_2-1}e^{-v}}{\Gamma(\alpha_1+\alpha_2)}$$
and we see that $f_{U,V}(u,v)=f_U(u)f_V(v)$ with $V\sim \mbox{Gamma}(\alpha_1+\alpha_2,1)$ and $U$ and $V$ are {\em independent}!\\

\vskip .2 in

\noindent {\bf Exercise for the student.}\\
Please re-do this example by instead choosing $V=X_1$.  Show that you still get the same Beta marginal for $U$, however, $U$ and $V$ will {\em not} be independent in this case.


\newpage

\noindent The method of Jacobians naturally extends from 1- and 2-dimensions to $d$-dimensions. We present the following theorem which may require the reader know a bit of linear algebra to
compute the Jacobian determinant.\\

\noindent {\bf Theorem.} (Jacobian transformation in finite dimensions)\\
Suppose $X_1,X_2,\dots, X_d$ are jointly continuous with joint pdf $f=f_{X_1,\dots,X_d}$ having support ${\cal A}=\mbox{supp}(f)$ and
$$u_1=g_1(x_1,x_2,\dots,x_d), \dots, \  u_d=g_d(x_1,x_2,\dots,x_d)$$
is a continuously differentiable one-to-one transformation of ${\cal A}$ into ${\cal B}$. Then, if we denote the
inverse transformation by
$$x_1=h_1(u_1,\dots,u_d),\ \dots,\  x_d=h_d(u_1,\dots,u_d),$$
the joint pdf of $U_1,U_2,\dots,U_d$ is given by
$$f_{U_1,\dots,U_d}(u_1,\dots,u_d) = f_{X_1,\dots,X_d}(x_1,\dots,x_d)\cdot |J|,$$
where
$$J:=\mbox{det}\left(\begin{array}{cccc} \frac {\partial x_1}{\partial u_1} & \frac {\partial x_1}{\partial u_2} & \cdots & \frac {\partial x_1}{\partial u_d}\\
\frac {\partial x_2}{\partial u_1} & \frac {\partial x_2}{\partial u_2} & \cdots & \frac {\partial x_2}{\partial u_d}\\
\vdots & \vdots & \ddots & \vdots \\
\frac {\partial x_d}{\partial u_1} & \frac {\partial x_d}{\partial u_2} & \cdots & \frac {\partial x_d}{\partial u_d}\end{array}\right)$$
is the {\bf\em Jacobian determinant}\label{jacobian2}.\\

\vskip .5 in

\noindent We now will generalize the last example that led us to the Beta distribution.\\

\noindent {\bf Example.} (The Dirichlet distribution)\\
Let $X_i$, $i=1,2,\dots,d+1$ be independent rvs and, for each $i$, suppose $X_i\sim \mbox{Gamma}(\alpha_i,1)$.  Define
$$U_i = \frac {X_i}{X_1+X_2+\cdots+X_{d+1}}\quad \mbox{for }i=1,2,\dots,d,\quad \mbox{and}\quad U_{d+1} = X_1+X_2+\cdots+X_{d+1}.$$
Show that the $d$-variate marginal pdf of $U_1,\dots,U_d$ is\label{dirichletpdf}
$$f_{U_1,\dots,U_d}(u_1,\dots,u_d) = \frac {\Gamma(\alpha_1+\cdots+\alpha_{d+1})}{\Gamma(\alpha_1)\cdots \Gamma(\alpha_{d+1})}u_1^{\alpha_1-1}\cdots u_d^{\alpha_d-1}(1-u_1-\cdots -u_d)^{\alpha_{d+1}-1}$$
This is the so-called {\bf\em Dirichlet distribution}.\\

\noindent SOLUTION:\\
I'll leave the general case for the reader, I will instead do the case where $d+1=3$. We have the transformation $u_1=\frac {x_1}{x_1+x_2+x_3},\ u_2=\frac {x_2}{x_1+x_2+x_3},\ u_3=x_1+x_2+x_3$. The
inverse transformation:
$$x_1=u_1u_3,\quad x_2=u_2u_3,\quad \mbox{and }\quad x_3=u_3-u_1u_3-u_2u_3=(1-u_1-u_2)u_3.$$


\newpage

\noindent Since $x_1>0, x_2>0,$ and $x_3>0$, $u_1>0, u_2>0, u_3>0$ and since $u_1+u_2=\frac {x_1+x_2}{x_1+x_2+x_3}<1$ we have the support of $U_1,U_2$:
$$u_1>0, u_2>0, u_1+u_2<1.$$
The Jacobian is
\begin{eqnarray*}
J &=& \mbox{det}\left(\begin{array}{ccc} u_3 & 0 & u_1 \\ 0 & u_3 & u_2\\ -u_3 & -u_3 & 1-u_1-u_2  \end{array}\right) \\
&=& \mbox{det}\left(\begin{array}{ccc} u_3 & 0 & u_1 \\ 0 & u_3 & u_2\\ 0 & -u_3 & 1-u_2  \end{array}\right)\\
&=& \mbox{det}\left(\begin{array}{ccc} u_3 & 0 & u_1 \\ 0 & u_3 & u_2\\ 0 & 0 & 1  \end{array}\right)\\
& &\\
&=& u_3^2.
\end{eqnarray*}
In this computation we used the linear algebra result that we do not
change the determinant of a matrix by replacing a row by a multiple of
another row added to it. We went from the first determinant
to the next determinant by adding the first row to the last row.
We went from the second determinant to the third determinant by adding the second row to the third row. The last determinant is a triangular matrix whose determinant is the product of the major
diagonal entries.

Finally, the joint pdf of $U_1,U_2,U_3$ is
\begin{eqnarray*}
f(u_1,u_2,u_3) &=& f_{X_1,X_2,X_3}(u_1u_3,u_2u_3,u_3)|u_3^2|\\
&=& \frac {[u_1u_3]^{\alpha_1-1}e^{-u_1u_3}[u_2u_3]^{\alpha_2-1}e^{-u_2u_3}[(1-u_1-u_2)u_3]^{\alpha_3-1}e^{-(1-u_1-u_2)u_3}}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}\cdot u_3^2 \\
&=& \frac {u_1^{\alpha_1-1}u_2^{\alpha_2-1}(1-u_1-u_2)^{\alpha_3-1}u_3^{\alpha_1+\alpha_2+\alpha_3-1}e^{-u_3}}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)} \\
\end{eqnarray*}
and the (bivariate) marginal pdf of $U_1,U_2$ is
\begin{eqnarray*}
f_{U_1,U_2}(u_1,u_2) &=& \int_0^{\infty}\frac {u_1^{\alpha_1-1}u_2^{\alpha_2-1}(1-u_1-u_2)^{\alpha_3-1}u_3^{\alpha_1+\alpha_2+\alpha_3-1}e^{-u_3}}
{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)} \,du_3\\
&=& \frac {u_1^{\alpha_1-1}u_2^{\alpha_2-1}(1-u_1-u_2)^{\alpha_3-1}}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}
\underbrace{\int_0^{\infty}u_3^{\alpha_1+\alpha_2+\alpha_3-1}e^{-u_3}\,du_3}_{=\Gamma(\alpha_1+\alpha_2+\alpha_3)}\\
&=&\frac {\Gamma(\alpha_1+\alpha_2+\alpha_{3})}{\Gamma(\alpha_1)\Gamma(\alpha_2) \Gamma(\alpha_{3})}u_1^{\alpha_1-1}u_2^{\alpha_2-1}(1-u_1-u_2)^{\alpha_{3}-1}.\\
\end{eqnarray*}

\newpage


\noindent {\bf Ordered statistics.}\label{orderstatistics}\\

\noindent Suppose we have a jointly distributed collection of random variables, say, $X_1,X_2,\dots,X_n$.  Since they are all defined on the same
sample space $\Omega$, we can define the ``new" random variables $Y_j$ to be the $j$th smallest among $X_1,X_2,\dots,X_n$.
The random variables $Y_j$ are called the {\bf\em ordered statistics}
of the sample $X_1,X_2,\dots, X_n$. The ordered statistics are well-defined: for each $\omega\in \Omega$, $X_i(\omega)\in {\mathbb R}$ for $i=1,2,\dots,n$ and, therefore,
for any $j=1,2,\dots,n$, $Y_j(\omega)$ being the $j$th smallest value among them makes complete sense.  It is important to note that the value of $Y_j$ depends on the entire collection of $X_i$'s and, therefore, $Y_j$ is a function of all these rvs $X_1,X_2,\dots,X_n$.  Moreover, the $Y_j$'s satisfy the condition
$Y_1\le Y_2\le \cdots \le Y_n.$\\

\includegraphics*[-60,0][300,180]{orderedstatpicture.jpg}

\bigskip

\noindent For instance, for the rv
$Y_1$, the value $Y_1(\omega)$ is $\min\{X_1(\omega),X_2(\omega),\dots,X_n(\omega)\}$ and this minimum value will come from that $X_i$ which happens
to be the smallest among $X_1,X_2,\dots,X_n$ for the particular $\omega$.
The picture shows $n=3$ rvs $X_1,X_2$ and $X_3$ as mappings from $\Omega$ into ${\mathbb R}$. Abstractly, from left to right, for those $\omega$ in the first section,
$X_3$ is the smallest of the three rvs so $Y_1=X_3$ in this region,
followed next by $X_1$ being the smallest so that $Y_1=X_1$ in this region, followed by $Y_1=X_3$ again, followed by $Y_1=X_2$.\\

\vskip .5 in

\noindent {\bf Simplifying assumption we make when dealing with ordered statistics:}\\
$X_1,X_2,\dots,X_n$ are {\em independent and identically distributed} (iid) with CDF $F(x)$.\\

\vskip .5 in

\noindent {\bf 1.~(univariate) distributions of the minimum $Y_1$ and the maximum $Y_n$}.\\
\noindent The purpose of this section is to first analyze the distributions associated with
the two {\em extreme} ordered statistics: the minimum and the maximum:
$$Y_1 = \min\{X_1,X_2,\dots,X_n\}\quad \mbox{and}\quad Y_n = \max\{X_1,X_2,\dots,X_n\}.$$



\newpage


\noindent {\bf\em useful device when working with $Y_1$ and/or $Y_n$:}\label{pdfofminandmaxofiid}\\
The minimum of a collection is greater than $x$ if and only if {\em every} member of
the collection is greater than $x$:
$$(Y_1>x) = (X_1>x,X_2>x,\dots,X_n>x),$$
and, the maximum of a collection is less than or equal to $x$ if and only if {\em every} member of the collection is less than or equal to $x$:
$$(Y_n\le x) = (X_1\le x,X_2\le x,\dots,X_n\le x).$$



\vskip .5 in

\noindent {\bf\em CDF of the minimum $Y_1$:}\label{cdfofY1}
\begin{eqnarray*}
F_{Y_1}(y) &=& P(Y_1\le y)\\
&=& 1 - P(Y_1> y)\\
&=& 1 - P(X_1> y,X_2>y,\dots,X_n>y)\\
&=& 1 - P(X_1> y)P(X_2>y)\cdots P(X_n>y)\quad (\mbox{using independence})\\
&=& 1 - \big(1-P(X_1\le y)\big)\big(1-P(X_2\le y)\big)\cdots \big(1-P(X_n\le y)\big)\\
&=& 1 - \big(1-F(y)\big)\big(1-F(y)\big)\cdots \big(1-F(y)\big) \quad (\mbox{identically distributed})\\
&=& 1 - (1-F(y))^n.\\
\end{eqnarray*}

\bigskip

\noindent {\bf\em CDF of the maximum $Y_n$:}\label{cdfofYn}
\begin{eqnarray*}
F_{Y_n}(y) &=& P(Y_n\le y)\\
&=& P(X_1\le y,X_2\le y,\dots,X_n\le y)\\
&=& P(X_1\le y)P(X_2\le y)\cdots P(X_n\le y)\quad (\mbox{using independence})\\
&=& P(X_1\le y)P(X_1\le y)\cdots P(X_1\le y)\quad (\mbox{identically distributed})\\
&=& F(y)^n.\\
\end{eqnarray*}

\vskip .2in

\noindent {\bf\em pdfs of $Y_1$ and of $Y_n$ when the iid collection is continuous}\label{pdfofminandmaxofiid}\\
If the iid rvs $X_1,X_2,\dots,X_n$ are absolutely continuous -- so that $F(x)$ possesses a pdf $f(x)$ -- then we can go further and say that
$$f_{Y_1}(y) = n\,f(y)[1-F(y)]^{n-1}$$
$$f_{Y_n}(y) = n\,f(y)[F(y)]^{n-1}.$$

\vskip .5 in

\noindent {\bf Advice:}\\
It is often easier to re-derive the above calculations when trying to find the pdfs for $Y_1$ and $Y_n$ than it is to remember these formulas.\\


\newpage


\noindent {\bf Important Example.} (The minimum of iid exponentials is, again, exponential)\\
Suppose $X_1,X_2,\dots, X_n\sim$\,iid Exp$(\lambda)$.  Show that $Y_1\sim \mbox{Exp}(n\lambda)$.\\

\noindent SOLUTION:\\
Here, $F(x) = 1-e^{-\lambda x}$ for $x>0$, and $f(x) = \lambda e^{-\lambda x}$ for $x>0$. Therefore, for $y>0$,
\begin{eqnarray*}
F_{Y_1}(y) &=& 1 - P(Y_1>y)\\
&=& 1- P(X_1>y)P(X_2>y)\cdots P(X_n>y) \\
&=& 1 - (1-F(y))^{n} \\
&=& 1 - (e^{-\lambda y})^n = 1-e^{-n\lambda y},\end{eqnarray*}
which shows $Y_1$ has the CDF of an Exp$(n\lambda)$.  From here we can easily take the derivative to see
$$f_{Y_1}(y) = n\lambda e^{-n\lambda y}\quad \mbox{for }y>0.$$


\vskip 1 in


\noindent {\bf Application: components in series and in parallel.}\label{seriesparallel}\\
Suppose a system is comprised of a sequence of $n$ identical components in series (so when the first component fails the system fails).
As an example suppose we have a system with $n=4$ identical components hooked up in series and the lifetime of each component follows an exponential distribution with
mean lifetime of 8 years (i.e., a rate $\lambda=1/8$), also assume component failures are independent.

\includegraphics*[-60,0][400,90]{seriesparallel.jpg}

\noindent Let $X_1,X_2,X_3,X_4$ represent the lifetimes of these $4$ components. Then $Y_1=\min\{X_1,X_2,X_3,X_4\}$ represents the time until the first failure.
The last example showed us that $Y_1\sim \mbox{Exp}(1/2)$.  Moreover, the probability the system is functioning after 6 months ($=1/2$ year) is
\begin{center}$P(Y_1>\frac 12) = \int_{\frac 12}^{\infty}\frac 12e^{-y/2}\,dy=e^{-1/4}\approx .7788.$\end{center}
What if, instead of 4 of these components in series, we had only 2 of them in series?  Then the distribution of $Y_1$ would now be Exp$(1/4)$ and
$P(Y_1>\frac 12)$ would now be $\approx .8825$.\\

\noindent Now suppose the 4 components are hooked up in parallel so that the system fails at the time $Y_4$, i.e., when the last component fail (and, therefore, all components have failed). Recall each $X_i$ has CDF $F(x) = 1-e^{-x/8}$ for $x>0$ and, therefore, $F_{Y_4}(y) = (1-e^{-y/8})^4$.
\begin{center}
$P(Y_4>\frac 12) = 1 - F_{Y_4}(\frac 12) = 1 - (1-e^{-1/16})^4 \approx .99987.$
\end{center}


\newpage


\noindent {\bf Exercise for the student.}\\
Suppose $X_1,X_2, X_3,\dots$ is a sequence of iid uniform$(0,1)$. Fix $x>0$ and let $Y_1^{(n)} = \min\{X_1,X_2,\dots,X_n\}$ be the {\em running minimum}.  Find
a formula for $P(Y_1^{(n)} > \frac xn)$.  You may assume $n>x$ so that $\frac xn<1$.   What happens to this expression as $n$ tends to $\infty$?  What can you say about the (limiting) distribution of
$nY_1^{(n)}:=n\,\min\{X_1,X_2,\dots,X_n\}$ as $n \to\infty$, i.e., $F_{nY_1^{(n)}}(x)=P(nY_1^{(n)}\le x)$ as $n\to \infty$? Does his CDF look familiar?\\

\vskip .3 in



\noindent {\bf Example.}\\
We have a balanced $1000$-sided die with faces having distinct numbers from 1 thru 1000. We plan to roll it 500 times.
For $i=1,2,\dots,500$, let $X_i$ be the value showing on the $i$th roll.
Compute the probability the largest value rolled is {\em greater than} 995, i.e., any one of the 5 values from $996,997,998,999,1000$?  Estimate this probability
using a limiting result we learned.\\

\noindent SOLUTION:\\
In this problem $X_1,X_2,X_{500}$ are iid discrete uniform\label{discreteuniform2} on the set $\{1,2,\dots,1000\}$, and the largest value $Y_{500}=\max\{X_1,X_2,\dots,X_{500}\}$.
The probability of interest is
\begin{eqnarray*}
P(Y_{500}>995) &=& 1-P(Y_{500}\le 995)\\
&=& 1 - (\frac {995}{1000})^{500}.
\end{eqnarray*}
We can estimate this probability by invoking the limiting representation of $e^u$ from page \pageref{d:limitrep}:
$$
1 - (\frac {995}{1000})^{500} = 1 - \Big(1 - \frac 5{1000}\Big)^{\frac {1000}2} = 1 - \left(\Big(1 - \frac 5{1000}\Big)^{1000}\right)^{\frac 12} \approx 1-\left(e^{-5}\right)^{\frac 12} \ (\approx .9179).$$

\vskip 1 in


\noindent {\bf 2.~marginal distribution of the $j$th ordered statistic $Y_j$.}\\
\noindent $Y_j$ is the rv that takes the $j$th smallest value produced by $X_1,X_2,\dots,X_n$.
On the last few pages of these notes we discussed the cases $j=1$ and $j=n$ in general, but we now will generalize to arbitrary $j$ from 1 to $n$.
Of course, we continue with the iid assumption, but first present the case where the rvs are continuous before handling the general CDF case.\\

\noindent {\bf 2.1~case of iid continuous rvs.}\\
Since the case where $X_1,X_2,\dots,X_n$ are iid continuous rvs having CDF $F(x)$ and pdf $f(x)$ is of special important we first start with this case.  This case allows for a ``simpler" analysis of the ordered statistics since continuous rvs have a probability 0 of being equal. This means when the joint collection is observed we may assume their values are {\em distinct} (and thus separated by a positive distance). \\


\newpage

\noindent Let $X_1,X_2,\dots,X_n$ be iid continuous rvs having CDF $F(x)$ and pdf $f(x)$ and, for fixed $j=1,2,\dots,n$, consider the $j$th ordered statistic $Y_j$.
For fixed {\em small} $h>0$ -- going to be sent to 0 -- the event $(y-h<Y_j\le y)$
means the $j$th smallest among $X_1,X_2,\dots,X_n$ lies in the interval $(y-h,y]$.
Since the variables are continuous and (presumably) $h$ is small and going to be sent to 0, the event $(y-h<Y_j\le y)$ is equivalent to saying
exactly {\em one} of the $n$ variables $X_1,X_2,\dots,X_n$ lies in the interval $(y-h,y]$, {\em and} $j-1$ of the remaining variables lie in $(-\infty,y-h]$, {\em and}
the remaining $n-j$ variables lie in $(y,\infty)$. See the picture.

\includegraphics*[-80,0][350,60]{marginalofjthorderedstatisticpic.jpg}

\bigskip

\noindent So, the event $(y-h<Y_j\le y)$ is equivalent to
$$\Big\{\mbox{one of the }n\ X_i's \mbox{ in }(y-h,y]\Big\}\cap \Big\{j-1\ X_i's\le y-h\Big\}\cap \Big\{\mbox{remaining }X_i's > y\Big\}.$$
Therefore,
\begin{eqnarray*}
P(y-h<Y_j\le y) &=&
{n\choose j-1,1,n-j}F(y-h)^{j-1}\big(F(y)-F(y-h)\big)\big(1-F(y)\big)^{n-j}\\
\end{eqnarray*}
%{n\choose j-1,1,n-j}\Big(P(X_1\le y-h)\Big)^{j-1}P(y-h<X_1\le y)\Big(P(X_1>y)\Big)^{n-j}\\
 %&=&
and, consequently, if we divide both sides by $h$ and send $h\to 0+$:
\begin{eqnarray*}
f_{Y_j}(y) &= & \lim_{h\to 0+} P(y-h<Y_j\le y) \\
&=& \lim_{h\to 0+} {n\choose j-1,1,n-j}F(y-h)^{j-1}\Big(\dfrac {F(y)-F(y-h)}{h}\Big)\big(1-F(y)\big)^{n-j}\\
&=& {n\choose j-1,1,n-j}F(y-h)^{j-1}f(y)\big(1-F(y)\big)^{n-j}.
\end{eqnarray*}

\vskip 1 in

\noindent Here are some other {\em equivalent} ways of writing this pdf:\label{pdfofYj}\\

$f_{Y_j}(y)= \frac {n!}{(j-1)!(n-j)!}f(y)F(y-h)^{j-1}\big(1-F(y)\big)^{n-j}$\medskip

$f_{Y_j}(y)= \frac {\Gamma(n+1)}{\Gamma(j)\Gamma(n+1-j)}f(y)F(y-h)^{j-1}\big(1-F(y)\big)^{n-j}$\medskip

$f_{Y_j}(y)= j{n\choose j}f(y)F(y-h)^{j-1}\big(1-F(y)\big)^{n-j}$\medskip

$f_{Y_j}(y)= nf(y){n-1\choose j-1}F(y-h)^{j-1}\big(1-F(y)\big)^{n-j}.$

\vskip .4 in



\newpage
\noindent {\bf Remark.}\\
This pdf agree with the pdfs we found for $j=1$ and $j=n$ -- see page \pageref{pdfofminandmaxofiid} -- {\em Check this}!\\

\vskip .5 in


\noindent {\bf Remark.}\\
Although the $X_1,X_2,\dots,X_n$ may be independent, the $Y_j$'s are definitely {\em dependent}; in fact, the $Y_j$ will always satisfy
$$Y_1\le Y_2\le Y_3\le \cdots \le Y_{n-1}\le Y_n.$$

\vskip .5 in

\noindent {\bf Notation.}\\
Sometimes the notation $X_{(j)}$ is used to denote the $j$th ordered statistic instead of $Y_j$.\\

\vskip .7 in

\noindent {\bf Important example.}\\
Suppose $X_1,X_2,\dots,X_n$ are iid uniform$(0,1)$.  Find the pdf of $Y_j$.\\

\noindent SOLUTION:\\
Since $F(y)=y$ and $f(y)=1$ for $0<y<1$, it follows
$$f_{Y_j}(y) = \frac {n!}{(j-1)!(n-j)!}y^{j-1}(1-y)^{n-j}=\frac {\Gamma(n+1)}{\Gamma(j)\Gamma(n-j+1)}y^{j-1}(1-y)^{n-j}\quad\mbox{for }0<y<1,$$
i.e., $Y_j\sim \mbox{Beta}(j,n+1-j)$.


\newpage


\noindent {\bf Application: the sample median}\\
Let $X_1, X_2, \dots, X_{2n+1}$ be iid r.v.s with PDF $f(x)$ and CDF $F(x)$, and let $Y_1, Y_2, \dots, Y_{2n+1}$ be the respective ordered statistics.
We call $Y_{n+1}$ the \textbf{\emph{sample median}}\label{samplemedian}.
The {\bf\em (population) median}\label{populationmedian} is the value $m$ such that
$$\int_{-\infty}^m f(x) \,dx = \frac{1}{2}.$$

\noindent {\bf Example.}\\
Let $X_1, X_2, X_3 \sim $ iid Exp(1). Then $Y_2$ is the sample median. Find the PDF of $Y_2$ and compute $E(Y_2)$.

$$f_{Y_j}(y) = n\,f(y)\begin{pmatrix} n-1 \\ j-1 \end{pmatrix} \left[F(y)\right]^{j-1} \left[1 - F(y) \right]^{n-j}.$$
\begin{align*}
    f_{Y_2}(y) &= 3(e^{-y})\begin{pmatrix} 2 \\ 1 \end{pmatrix} (1 - e^{-y})^{2-1}(e^{-y})^{3-2} \\
    &= 6e^{-y}(1-e^{-y})e^{-y} \\
    &= 6(e^{-2y} - e^{-3y}) \quad\quad \text{for} ~y>0.
\end{align*}

\begin{align*}
    E(y_2) &= \int_0^{\infty} y\cdot 6(e^{-2y} - e^{-3y}) \,dy \\
    &= 6\int_0^{\infty} ye^{-2y} \,dy - 6\int_0^{\infty} ye^{-3y} \,dy \\
    &= 6\left(\frac{1}{2}\right)^2 - 6\left(\frac{1}{3}\right)^2 \\
    &= \frac{6}{4} - \frac{6}{9} \\
    &= \frac{6 \cdot 5}{36} \\
    &= \frac{5}{6} ~\approx~  0.83\Bar{3}.
\end{align*}


\newpage



\noindent {\bf 2.2.~case of a {\em general} iid random sample.}\label{cdfofjthorderedstatistic}\\
In this case $X_1,X_2,\dots,X_n$ are iid having CDF $F(x)$ not necessarily continuous.
We derive the CDF of $Y_j$ by analyzing the event $(Y_j\le y)$.
Since the $j$th smallest is less than or equal to $y$ {\em is not} saying anything about $Y_k$ for $k>j$, we decompose this event into the union of {\em mutually exclusive events} as follows:
\begin{center}
$(Y_j\le y)  =  (Y_n\le y)\cup \bigcup_{k=j}^{n-1}(Y_{k}\le y, Y_{k+1}>y).$\end{center}
\noindent Therefore, the CDF of $Y_j$ is\vskip -.3 in
\begin{eqnarray*}
F_{Y_j}(y)=P(Y_j\le y) & = & P(Y_n\le y)+\sum_{k=j}^{n-1}P(Y_{k}\le y, Y_{k+1}>y) \\
&=& F(y)^n + \sum_{k=j}^{n-1}{n\choose k}F(y)^k(1-F(y))^{n-k}\\
&=& \sum_{k=j}^{n}{n\choose k}F(y)^k(1-F(y))^{n-k}.\\
\end{eqnarray*}


\vskip .2 in




\noindent {\bf Remark.}\\
Let's verify that this CDF agrees with the CDFs we found for $Y_1$ and $Y_n$ on page \pageref{cdfofY1}.\\
When $j=n$, the above CDF reduces to $$\sum_{k=n}^{n}{n\choose k}F(y)^k(1-F(y))^{n-k}=F(y)^n.$$
When $j=1$,
$$\sum_{k=1}^{n}{n\choose k}F(y)^k(1-F(y))^{n-k}=1-{n\choose 0}F(y)^0(1-F(y))^{n-0}=1-(1-F(y))^n.$$

\noindent Moreover, if the CDF $F(x)$ has pdf $f(x)$, then\\

\noindent $f_{Y_j}(y) =\frac d{dy}\sum_{k=j}^n{n\choose k}F(y)^k(1-F(y))^{n-k}$\medskip

\noindent $=\sum_{k=j}^n{n\choose k}kF(y)^{k-1}f(y)(1-F(y))^{n-k} - \sum_{k=j}^{n-1}{n\choose k}(n-k)F(y)^kf(y)(1-F(y))^{n-k-1}$\medskip

\noindent $=\sum_{k=j}^n{n\choose k}kF(y)^{k-1}f(y)(1-F(y))^{n-k} - \sum_{k=j+1}^{n}{n\choose k-1}(n-k+1)F(y)^{k-1}f(y)(1-F(y))^{n-k}$\medskip

\noindent $=j{n\choose j}F(y)^{j-1}f(y)(1-F(y))^{n-j}$

$\qquad +\sum_{k=j+1}^n\underbrace{\left\{k{n\choose k}-(n-k+1){n\choose k-1}\right\}}_{=0}F(y)^{k-1}f(y)(1-F(y))^{n-k}$

\noindent and, therefore, we obtain the pdf of $Y_j$ in another way:
$$f_{Y_j}(y) = j{n\choose j}f(y) F(y)^{j-1}(1-F(y))^{n-j}$$
[cf the pdfs on page \pageref{pdfofYj}].

%\begin{eqnarray*}
%f_{Y_j}(y) &=&\frac d{dy}\sum_{k=j}^n{n\choose k}F(y)^k(1-F(y))^{n-k}\\
%&=&\sum_{k=j}^n{n\choose k}kF(y)^{k-1}f(y)(1-F(y))^{n-k} - \sum_{k=j}^{n-1}{n\choose k}(n-k)F(y)^kf(y)(1-F(y))^{n-k-1}\\
%&=&\sum_{k=j}^n{n\choose k}kF(y)^{k-1}f(y)(1-F(y))^{n-k} \\
%& & \qquad \qquad \qquad \qquad - \sum_{k=j+1}^{n}{n\choose k-1}(n-k+1)F(y)^{k-1}f(y)(1-F(y))^{n-k}\\
%&=&j{n\choose j}F(y)^{j-1}f(y)(1-F(y))^{n-j}\\
%& & \quad +\sum_{k=j+1}^n\underbrace{\left\{k{n\choose k}-(n-k+1){n\choose k-1}\right\}}_{=0}F(y)^{k-1}f(y)(1-F(y))^{n-k} \\
%\end{eqnarray*}


\newpage




\noindent {\bf 3.~The joint distribution of $Y_1,Y_2,\dots,Y_n$.}\\
\noindent The last several pages were all about the univariate marginals (CDFs and, if continuous, pdfs) of iid random samples. In this section we derive the joint pdf
of the entire collection of ordered statistics $Y_1,Y_2,\dots,Y_n$ of an iid random sample $X_1,X_2,\dots,X_n$.  To avoid the issue of random variables taking the same value with a positive probability we will assume in this section that $X_1,X_2,\dots,X_n$ are jointly continuous. \\

\noindent Let $y_1<y_2<\cdots  <y_n$, $0<h_i<<1$ for $i=1,2,\dots,n$ and consider the event
\begin{center}$A:=\Big(y_1-h_1<Y_1\le y_1,y_2-h_2<Y_2\le y_2,\dots,y_n-h_n<Y_n\le y_n\Big).$\end{center}


\noindent We assume the $h_i$'s are small enough so that the intervals $(y_i-h_i,y_i]$ for $i=1,2,\dots,n$ are pairwise disjoint.
Now, the collection of events
\begin{center}$A_{\pi}:=\Big(y_1-h_1<X_{\pi_1}\le y_1,y_2-h_2<X_{\pi_2}\le y_2,\dots,y_n-h_n<X_{\pi_n}\le y_n\Big),$\end{center}
for each permutation
$\pi=(\pi_1,\pi_2,\dots,\pi_n)$ of the integers $\{1,2,\dots,n\}$ are {\em mutually exclusive}.\\

\includegraphics*[-70,0][400,60]{jointorderedstatisticspic.jpg}

\bigskip

\noindent Since the $X_i$'s are iid with common CDF $F(x)$ it follows that, for each permutation $\pi$,
$$P(A_{\pi}) = \prod_{i=1}^n\Big(F(y_i)-F(y_i-h_i)\Big).$$
Finally, since there are $n!$ such permutations we have
$$\frac {P(y_1-h_1<Y_1\le y_1,\dots,y_n-h_n<Y_n\le y_n)}{h_1\cdots h_n} = n!\prod_{i=1}^n\Big(\frac {F(y_i)-F(y_i-h_i)}{h_i}\Big),$$
and, passing to the limit as each $h_i\to 0+$ we arrive to the

\vskip .2 in

\noindent {\bf\em joint pdf of $Y_1,Y_2,\dots,Y_n$:}\label{jointpdfoforderedstatistics}
\begin{center}$f_{Y_1,Y_2,\dots,Y_n}(y_1,y_2,\dots,y_n) = \left\{\begin{array}{cl} n!f(y_1)f(y_2)\cdots f(y_n)&\mbox{for }y_1<y_2<\cdots < y_n\\ 0 & \mbox{elsewhere}\end{array}\right..$\end{center}


%The event $(Y_1\le y_1,Y_2\le y_2,\dots,Y_n \le y_n)$ is equivalent to the event
%$$\bigcup_{\pi\in {\cal S}_n}(X_{\pi_1}\le y_1, X_{\pi_2}\le y_2,\dots, X_{\pi_n}\le y_n).$$
%To understand how we are going to proceed let's first restrict our attention to the case of $n=3$. Since we are assuming $X_1,X_2,X_3$ are jointly continuous,
%we can decompose $\Omega$ into the following $3!$ mutually exclusive and exhaustive possibilities:
%\begin{center}
%${\cal A}_1:=\{X_1<X_2<X_3\}$ on which $X_1=Y_1,\ X_2=Y_2,\ X_3=Y_3$
%
%${\cal A}_2:=\{X_1<X_3<X_2\}$ on which $X_1=Y_1,\ X_2=Y_3,\ X_3=Y_2$
%
%${\cal A}_3:=\{X_2<X_1<X_3\}$ on which $X_1=Y_2,\ X_2=Y_1,\ X_3=Y_3$
%
%${\cal A}_4:=\{X_2<X_3<X_1\}$ on which $X_1=Y_3,\ X_2=Y_1,\ X_3=Y_2$
%
%${\cal A}_5:=\{X_3<X_1<X_2\}$ on which $X_1=Y_2,\ X_2=Y_3,\ X_3=Y_1$
%
%${\cal A}_6:=\{X_3<X_2<X_1\}$ on which $X_1=Y_3,\ X_2=Y_2,\ X_3=Y_1$
%\end{center}

%\newpage

%\noindent {\bf \em bivariate marginals for $Y_i$,$Y_j$.}\label{bivariatemarginalofYiandYj}\\
%\noindent Using exactly the same ideas as in the last section we can show that for any iid sample $X_1,X_2,\dots,X_n$ of {\em continuous} random variables having CDF $F(x)$ and
%pdf $f(x)$ that
%$$f(y_i,y_j) = \frac {n!}{(i-1)!(j-i-1)!(n-j)!}f(y_i)f(y_j)F(y_i)^{i-1}[F(y_j)-F(y_i)]^{j-i-1}[1-F(y_j)]^{n-j}$$
%for $y_i<y_j$.\\
%
%\noindent To see how we get this pdf,
%fix $y_i<y_j$ and $0<h_1,h_2<<1$ so small that $y_1<y_2-h_2$, i.e., the intervals $(y_i-h_i,y_i]$ for $i=1,2$ will be disjoint.
%We look at the event $(y_1-h_1<Y_1\le y_1,y_2-h_2<Y_2\le y_2)$. This event is satisfied exactly in the following way:
%
%\noindent from the collection $X_1,X_2,\dots,X_n$ we select
%
%$i-1$ of them to fall in the interval $(-\infty,y_i-h_1]$,
%
%1 of the remaining to fall in the interval $(y_1-h_1,y_1]$,
%
%$j-i-1$ to fall in the interval $(y_1,y_2-h_2]$,
%
%1 of the remaining to fall in the interval $(y_2-h_2,y_2]$, and
%
%$n-j$ to fall in the interval $(y_2,\infty)$. \medskip
%
%
%\noindent There are
%\begin{equation}\displaystyle {n\choose i-1,1,j-i-1,1,n-j}=\frac {n!}{(i-1)!(j-i-1)!(n-j)!}\label{eq:number}\end{equation}
%ways to do this.
%For each such selection the probability of this happening is
%\begin{equation}F(y_i-h_1)^{i-1}[F(y_i)-F(y_i-h_1)][F(y_j-h_j)-F(y_i)]^{j-i-1}[F(y_j)-F(y_j-h_j)][1-F(y_j)]^{n-j}.\label{eq:prob}\end{equation}
%Therefore, $P(y_1-h_1<Y_1\le y_1,y_2-h_2<Y_2\le y_2)$ is the product of (\ref{eq:number}) and (\ref{eq:prob}). Dividing by $h_1h_2$ and sending $h_i\to 0+$
%we arrive to the above density.\\


\newpage

\noindent {\bf Recap:}\\
\noindent When $X_1, X_2, \dots, X_n \sim $ iid $f(x)$ with CDF$F(x)$, then the ordered statistics of this sample
$$Y_1 \leq Y_2 \leq \dots \leq Y_n$$
has joint PDF
\begin{align*}
    f_{Y_1, Y_2, \dots, Y_n}~(y_1, y_2, \dots, y_n)
    = \left\{
    \begin{array}{l}
         n!\,f(y_1)f(y_2)\dots f(y_n) \quad \quad\text{for } y_1 < y_2 < \dots < y_n  \\
         0 \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \text{elsewhere}
    \end{array} \right.
\end{align*}
and marginals
$$f_{Y_j}(y) = n\,f(y)\begin{pmatrix} n-1 \\ j-1 \end{pmatrix} \left[F(y)\right]^{j-1} \left[1 - F(y) \right]^{n-j}.$$

\vskip .5 in


\noindent {\bf Example (from Sheldon Ross's textbook).}\\
\noindent Suppose 3 people are stranded on a 1 mile length of road completely at random.
What is the probability that no two people are within a distance $d$ of each other? (Here, $d \leq \frac{1}{2}$ mile).\\
%\begin{figure}[h]
%    \centering
%    \includegraphics[width = 0.75\linewidth]{order stats figs/ex1_fig1.png}
%\end{figure}\\

\noindent SOLUTION:\\
Let $X_1, X_2, X_3$ represent the location of the three people along the road. We assume $X_1, X_2, X_3 \sim $ iid uniform$(0, 1)$. If $Y_1, Y_2, Y_3$ are the order statistics of this sample, then no two people are within distance $d$ of each other means that
$$Y_2 - Y_1 > d \quad \text{ and } \quad Y_3 - Y_2 > d.$$


\includegraphics*[-100,0][350,60]{strandedonroad1.jpg}


\begin{eqnarray*}
P(Y_2 - Y_1 > d, Y_3 - Y_2 > d) &=& \int_0^{1-2d}\int_{y_1+d}^{1-d}\int_{y_2+d}^1 3!\,dy_3\,dy_2\,dy_1 \\
    &=& 3! \int_0^{1-2d}\int_{y_1+d}^{1-d} (1 - d - y_2) \,dy_2\,dy_1 \quad
    \left\{ \begin{array}{l}
         u = 1 - d - y_2 \\
         du = -dy_2
    \end{array} \right. \\
    &=& 3! \int_0^{1-2d}\int_0^{1-2d-y_1} u \,du\,dy_1 \\
    &=& 3! \int_{0}^{1-2d} \frac{(1 - 2d - y_1)^2}{2} \,dy_1 \quad\quad\quad\ \ \
    \left\{ \begin{array}{l}
         w = 1 - 2d - y_1 \\
         dw = -dy_1
    \end{array} \right. \\
    &=& \frac{3!}{2} \int_0^{1-2d} w^2 \,dw \\
    & & \\
    &=& (1-2d)^3.
\end{eqnarray*}


%\noindent {\bf Exercise for the student.}\\
%If $X_1, X_2, \dots, X_n \sim $ iid uniform$(0, 1)$ and $d \leq \frac{1}{n-1}$, show
%$$P(Y_2 - Y_1 > d, Y_3 - Y_2 > d, \dots, Y_n - Y_{n-1} > d) = (1-(n-1)d)^n.$$

\newpage


\noindent \textbf{Example.} \\
Let $X_1, X_2, \dots, X_n \sim$ iid uniform(0,1) r.v.s.\\
Define the {\bf \em spacings}\label{spacings}
\begin{align*}
    W_1 &= Y_1 \\
    W_2 &= Y_2 - Y_1 \\
    W_3 &= Y_3 - Y_2 \\
    \vdots & \\
    W_n &= Y_n - Y_{n-1}.
\end{align*}
The inverse transformation is
\begin{align*}
    & Y_1 = W_1 \\
    & Y_2 = W_1 + W_2 \\
    & Y_3 = W_1 + W_2 + W_3 \\
    & \quad \vdots \\
    & Y_{n-1} = W_1 + W_2 + W_3 + \dots + W_{n-1} \\
    & Y_n = W_1 + W_2 + W_3 + \dots + W_{n-1} + W_n.
\end{align*}
And so the Jacobian determinant is
\begin{align*}
    J = \text{det}
    \begin{pmatrix}
    1 & 0 & 0 & 0 & \dots & 0 & 0 \\
    1 & 1 & 0 & 0 & \dots & 0 & 0 \\
    1 & 1 & 1 & 0 & \dots & 0 & 0 \\
    \vdots &&&& \\
    1 & 1 & 1 & 1 & \dots & 1 & 0 \\
    1 & 1 & 1 & 1 & \dots & 1 & 1 \\
    \end{pmatrix} = 1.
\end{align*}
If we agree to set $Y_0=0$, then, for each integer $i\ge 1$, $Y_{i-1}<Y_i$ implies $0<W_i<1$, and $0<Y_n<1$ implies $0<W_1+W_2+\cdots + W_n <1$.
The joint pdf of $W_1,W_2,\dots,W_n$ is
$$f_{W_1,W_2,\dots,W_n}(w_1,w_2,\dots,w_n) = n!\quad\mbox{for }0<w_i<1\ (i=1,2,\dots,n),\ 0<w_1+w_2+\cdots w_n<1.$$



\newpage


\noindent {\bf 4.~(bivariate) marginal of $Y_i, Y_j$ where $i<j$}\label{bivariatemarginalYiYj}\\
Using an idea similar to how we find the univariate marginals $Y_j$ we can find the bivariate marginal, too. Suppose $X_1, X_2, \dots, X_n \sim$ iid $f(x)$ (CDF $F(x)$) and consider their ordered statistics $Y_1 \leq Y_2 \leq \dots \leq Y_n$.
Let $1 \leq i < j \leq n$. Let's find $f_{Y_i, Y_j}(y_i, y_j)$ Let $0 < h_i, h_j << 1$.\\
%\begin{figure}[h!]
%    \includegraphics{order stats figs/ex2_fig1.png}
%    %\caption{Caption}
%    %\label{fig:my_label}
%\end{figure}

\noindent $
P(y_i - h_i < Y_i \leq y_i, \quad y_j - h_j < Y_j \leq y_j) = $\medskip

\noindent $nf(y_i)h_i \cdot (n-1)f(y_j)h_j \cdot {n-2 \choose i-1} \cdot \left[F(y_i - h_i)\right]^{i-1} \cdot {n-i-1 \choose n-j} [1 - F(y_j)]^{n-j} [F(y_j - h_j) - F(y_i)]^{j-i-1}$\\

\noindent Dividing by $h_1h_2$ and sending $h_1 \rightarrow 0$ and $h_2 \rightarrow 0$ gives us\medskip

 $f_{Y_i, Y_j}(y_i, y_j) = $
$$\frac {n!}{(i-1)!(j-i-1)!(n-j)!} \cdot [F(y_i)]^{i-1} \cdot [F(y_j) - F(y_i)]^{j-i-1}  [1 - F(y_j)]^{n-j} f(y_i)f(y_j).$$
\\
\\
\noindent {\bf Example.}\\
$X_1, X_2, \dots, X_n \sim$ iid uniform(0, 1).
Find the PDF of the sample range $R = Y_n - Y_1$.\\

\noindent SOLUTION:\\
Since $R$ is a function of $Y_1$ and $Y_n$ we need to find the joint pdf of $Y_1$ and $Y_n$ to start.  Using the above computation we find that
the joint PDF of $Y_1, Y_n$ is
$$f_{Y_1, Y_n}(y_1, y_n) = n(n-1)(y_n - y_1)^{n-2} \quad\quad\quad \text{for}~ 0 < y_1 < y_n < 1.$$
Applying the method of Jabobians we create the variable $S=Y_1$ and find the inverse transformation:
$$y_1=s\quad \mbox{and}\quad y_n = r+s$$
having Jacobian: $J= \mbox{det}\left( \begin{array}{cc} \partial y_1/\partial r & \partial y_1/\partial s \\ \partial y_n/\partial r & \partial y_n/\partial s   \end{array}\right) =\mbox{det}\left( \begin{array}{cc} 0 & 1 \\ 1 & 1  \end{array}\right) =-1\implies |J|=1$.\\

\noindent The support of the joint density of $R$ and $S$ is
$$0<r<1\ \mbox{and}\ \ 0<r+s<1 \implies 0<s<1-r.$$
Finally,
$$f_{R,S}(r,s) = f_{Y_1,Y_n}(s,r+s)|J|=n(n-1)r^{n-2},$$
and
$$f_R(r) = \int_0^{1-r}n(n-1)r^{n-2}\,dr = n(n-1)r^{n-2}(1-r)\quad \mbox{for }0<r<1,$$
i.e., $R\sim \mbox{Beta}(n-1,2)$.\\


\newpage




\noindent {\bf Application (Poisson process).}\label{poissonprocess}\\
The stochastic process $(N(t): t \geq 0)$ such that
\begin{enumerate}
    \item $P(N(0) = 0) = 1$;
    \item For $t > s \geq 0$, $P(N(t) - N(s) = n) = \frac{e^{-\lambda(t-s)}[\lambda(t-s)]^n}{n!}$; i.e.,
    $$N(t)-N(s) \sim \mbox{Poisson}(\lambda(t-s));$$
    \item For any $0 \leq t_1 \leq t_2 \leq t_3 \leq \dots$, all the elements in the following collection $$N(t_1), N(t_2) - N(t_1), N(t_3) - N(t_2), \dots$$ are independent;
\end{enumerate}
is called a (time-homogeneous) Poisson process with rate $\lambda$.\\
%\begin{figure}[h]
%    \centering
%    \includegraphics[width = 0.75\linewidth]{order stats figs/application_poisson.jpg}
%\end{figure} \\

\noindent We let $S_n$ be the time at which the process first reaches the value $n$: $N(S_n)=n$ and $N(t)<n$ for $t<S_n$.  $S_n$ is called the {\bf\em arrival time}\label{arrivaltime} of the $n$th Poisson event.\\

\includegraphics*[-20,0][400,140]{poissonprocesspath1.jpg}

\bigskip

\noindent Let's show that each $S_n \sim \mbox{Gamma}(n, \frac{1}{\lambda})$.\\

\noindent We use the following important relationship between $N(t)$ and $S_n$:

$$(N(t)\le n-1) = (S_n>t).$$

\noindent This is true because $S_n>t$ means that the $n$th Poisson arrival did not occur by time $t$, i.e., $N(t)\le n-1$.  Therefore, the CDF of $S_n$ is
\begin{eqnarray*}
F_{S_n}(t)& = & 1-P(S_n\le t) \\
& = & 1-P(N(t)\le n-1) \\
& = & 1-\sum_{j=0}^{n-1} P(N(t)=j)\\
&=& 1-\sum_{j=0}^{n-1} e^{-\lambda t}\frac {(\lambda t)^{j}}{j!}\\
&=& 1- e^{-\lambda t} \left( 1 + \lambda t + \frac {\lambda^2 t^2}{2!} + \cdots + \frac {\lambda^{n-1}t^{n-1}}{(n-1)!}\right).\\
\end{eqnarray*}



\newpage

\noindent Taking a derivative in $t$, we have the pdf of $S_n$:\\

\noindent $f_{S_n}(t) = \lambda e^{-\lambda t}\left( 1 + \lambda t + \frac {\lambda^2 t^2}{2!} + \cdots + \frac {\lambda^{n-1}t^{n-1}}{(n-1)!} \right)
-e^{-\lambda t}\left( \lambda + \lambda^2 t + \frac {\lambda^3 t^2}{2!} + \cdots + \frac {\lambda^{n-1}t^{n-2}}{(n-2)!} \right)$\\

\noindent and we arrive at
$$f_{S_n}(t) = \frac {\lambda^n t^{n-1}e^{-\lambda t}}{(n-1)!},$$
which is the pdf of a Gamma$(n,\frac 1{\lambda})$.\\

\vskip .2in
\noindent \textbf{Remark.}\\
An interesting fact about the arrival times of a Poisson process is that conditioned on knowing the number of arrivals by time $T$, the Poisson arrivals occur at times
that are uniformly distributed on the interval $[0,T]$, i.e.,
$(S_1, S_2, \dots, S_n)\mid N(T) = n$ has the same distribution as the ordered statistics of a uniform$(0,T)$ random sample: $(Y_1, Y_2, \dots, Y_n)$.
\\
\noindent Let's show this when $n = 1$, i.e. when $N(T) = 1$. We will show $$P(S_1 \leq t ~|~ N(T) = 1) = \frac{t}{T},$$
the CDF of a uniform$(0,T)$.

\begin{align*}
    P(S_1 \leq t ~|~ N(T) = 1) &= \frac{P(S_1 \leq t, N(T) = 1)}{P(N(T) = 1)} \\
    &= \frac{P(N(T) = 1, N(T) - N(t) = 0)}{P(N(T) = 1)} \\
    &= \frac{P(N(T) = 1) P(N(T) - N(t) = 0)}{P(N(T) = 1)} \\
    &= \frac{e^{\lambda t} \lambda t \cdot e^{-\lambda(T-t)}}{e^{\lambda T}\lambda T} =\frac tT.\\
\end{align*}

\noindent How about $(S_1, S_2)\mid N(T) = 2$?

We will instead compute this conditional pdf in the same way we approached finding the bivariate marginal pdf of two ordered statistics, i.e., via the limit of
$$\dfrac {P(t_1 - h_1 < S_1 \leq t_1, t_2 - h_2 < S_2 \leq t_2 ~|~ N(T) = 2)}{h_1h_2}$$
as $h_1,h_2$ tend to 0.

\includegraphics*[0,0][400,100]{poissonprocess-n=2.jpg}

\begin{align*}
    &P(t_1 - h_1 < S_1 \leq t_1, t_2 - h_2 < S_2 \leq t_2 ~|~ N(T) = 2) \\
    &= \frac{P(t_1 - h_1 < S_1 \leq t_1, t_2 - h_2 < S_2 \leq t_2, N(T) = 2)}{P(N(T)) = 2)} \\
    &= \frac{P(A \cap B \cap C \cap D \cap E)}{P(N(T) = 2)},
\end{align*}
where
$$
\begin{array}{l}
    A: \ N(t_1 - h_1) = 0, \\
    B: \ N(t_1) - N(t_1 - h_1) = 1, \\
    C: \ N(t_2 - h_2) - N(t_1) = 0, \\
    D: \ N(t_2) - N(t_2 - h_2) = 1, \\
    E: \ N(T) - N(t_2) = 0
\end{array}
$$
are all independent by the independent increment property.
Now since
$$
\begin{array}{l}
    P(A) = P(N(t_1 - h_1) = 0) = e^{-\lambda (t_1-h_1)}, \\
    P(B) = P(N(t_1) - N(t_1 - h_1) = 1)=e^{-\lambda h_1}\lambda h_1, \\
    P(C) = P(N(t_2 - h_2) - N(t_1) = 0) = e^{-\lambda(t_2-t_1+h_2)}, \\
    P(D) = P(N(t_2) - N(t_2 - h_2) = 1)=e^{-\lambda h_2}\lambda h_2, \\
    P(E) = P(N(T) - N(t_2) = 0) = e^{-\lambda(T-t_2)},\\
\end{array}
$$

\noindent we have, for $0<t_1<t_2<T$, \\

$\dfrac {P(t_1 - h_1 < S_1 \leq t_1, t_2 - h_2 < S_2 \leq t_2 ~|~ N(T) = 2)}{h_1h_2}$\medskip

\hskip .5 in $= \dfrac {e^{-\lambda (t_1-h_1)}e^{-\lambda h_1}\lambda h_1e^{-\lambda(t_2-t_1+h_2)}e^{-\lambda h_2}\lambda h_2e^{-\lambda(T-t_2)}}{h_1h_2e^{-\lambda T}\left(\frac {\lambda^2T^2}{2!}\right)}
=\dfrac {2!}{T^2}$.\\

\bigskip

\noindent and, therefore, as $h_1$ and $h_2$ tend to 0, the conditional pdf of $S_1,S_2$ given $N(T)=2$ is
$$f_{S_1,S_2|N(T)}(t_1,t_2|2) = 2!\cdot \frac 1T\cdot \frac 1T,$$
which is the joint pdf of the ordered statistics $Y_1,Y_2$ of a uniform$(0,T)$ random sample.\\

\noindent This last argument can be generalized in a straightforward way to show that $$(S_1,S_2,\dots,S_n)\mid N(T)=n$$ has the joint pdf
$\frac {n!}{T^n}$ for $0<t_1<t_2,\cdots <t_n < T$ which is the same
distribution as the ordered statistics $Y_1,Y_2,\dots,Y_n$ of a uniform$(0,T)$ random sample.\\


\vskip .5 in



\newpage

%\noindent {\bf Symmetry and exchangeability.}\label{exchangeability}\\

%In chapter~1 we introduced the idea of exchangeability.  We now expand on this interesting idea.\\

%\noindent A collection of random variables $X_1,X_2,\dots,X_n$ is called {\bf \em exchangeable}\label{exchangeability3} if
%for every permutation $(i_1,i_2,\dots,i_n)$ of the integers $1,2,\dots,n$ we have that the joint distribution of
%$X_{i_1},X_{i_2},\dots,X_{i_n}$ and the joint distribution of $X_1,X_2,\dots,X_n$ are {\em the same}.  We may use the notation:
%$$(X_{i_1},X_{i_2},\dots,X_{i_n})\stackrel{{\cal D}}{=}(X_1,X_2,\dots,X_n),$$
%which just means the two vectors have the same distribution.
%Intuitively, if we have an exchangeable collection (sequence) of random variables then the order in which we observe the sequence
%is irrelevant.\medskip
%
%\noindent We'll show iid sequences are exchangeable, but they are not the only ones.  The following fact is often useful in determining whether or not a collection is
%exchangeable.  Roughly, it says a collection of rvs is exchangeable provided the joint distribution is symmetric.\\
%\vskip .2 in
%
%\noindent {\bf Symmetric functions.}\\
%A function $g=g(x_1,x_2,\dots,x_n)$ is called {\bf\em symmetric}\label{symmetricfunction} if, for every permutation $(i_1,i_2,\dots,i_n)$ of the integers
%$1,2,\dots,n$, $g(x_{i_1},x_{i_2},\dots,x_{i_n})=g(x_1,x_2,\dots,x_n)$, i.e., the value of the function is invariant under permutations of its arguments.\\

%\vskip .2 in
%
%\noindent {\bf Example.}\\
%Which of the following are symmetric functions and which are not?  For the one(s) that are not symmetric briefly explain why they are not.\medskip
%
%\noindent (a) $g(x,y,z) = \left\{ \begin{array}{cl} e^{-2(x+y+z)} & \mbox{for }x>0,y>0,z>0,\\ 0 & \mbox{elsewhere}\end{array}\right.$\\
%(b) $g(x,y) = \left\{\begin{array}{cl}x+y & \mbox{for }0<x<1, 0<y<1\\ 0 & \mbox{elsewhere}\end{array}\right.$.\\
%(c) $g(x,y) = \left\{\begin{array}{cl}2x+y & \mbox{for }0<x<1, 0<y<1\\ 0 & \mbox{elsewhere}\end{array}\right.$.\\
%(d) $g(x,y) = \left\{\begin{array}{cl}2xy & \mbox{for }0<x<1, 0<y<2\\ 0 & \mbox{elsewhere}\end{array}\right.$.\\
%
%\vskip .2 in
%
%\noindent SOLUTION:\\
%(a) and (b) are symmetric functions.\\
%(c) is not a symmetric function since $g(x,y)-g(y,x)=2x+y-(2y+x)=x-y\neq 0$ when $(x,y)$ is in the support of $g$ but $x\ne y$.\\
%(d) is not a symmetric function.  Although the function rule appears symmetric, i.e., $2xy=2yx$,  the support is not symmetric in its arguments, i.e.,
%when $0<x<1$ and $1<y<2$, $g(x,y)\neq 0$ but $g(y,x)=0$.\\
%
%\vskip .2 in
%
%\noindent {\bf Remark.}\\
%In order for a function to be symmetric both its rule and its support must be invariant under permutations of its arguments. In part (c) of the example, the rule was not %symmetric.
%In part (d) of the example, the support was not symmetric.
%
%
%\newpage
%
%
%\noindent {\bf Question: How can we identify if a given sequence is exchangeable?}\medskip
%
%\noindent {\bf Answer:} If the joint pmf or the joint pdf of the sequence is a symmetric function, then the sequence is exchangeable.
%
%\vskip .2 in
%
%\noindent {\bf Properties of exchangeable sequences.}\\
%{\bf 1.} The rvs in an exchangeable sequence are {\bf\em identically distributed}, i.e., for all $i$, $X_i$ and $X_1$ have the same distribution.

%Why?
%For example, if $n=2$ and $X_1,X_2$ is exchangeable, then $(X_2,X_1)$ has the same joint distribution as $(X_1,X_2)$. If these are jointly continuous, say, then
%this means the densities $f_{X_2,X_1}(x,y)=f_{X_1,X_2}(x,y)$ for all $x,y$. Moreover,
%\begin{eqnarray*}
%f_{X_2}(x) &=& \int_{-\infty}^{\infty}f_{X_2,X_1}(x,y)\,dy \\
%& & \\
%&=& \int_{-\infty}^{\infty}f_{X_1,X_2}(x,y)\,dy \\
%& & \\
%&=& f_{X_1}(x),
%\end{eqnarray*}
%which shows $X_1$ and $X_2$ have the same (marginal) distribution. \\
%{\bf 2.} More generally, every subset of $k$ rvs has the same distribution as $X_1,X_2,\dots,X_k$, that is, the $k$-variate marginal distributions
%of an exchangeable sequence are all the same, they only depend on the number of rvs in the subset under consideration.\\
%{\bf 3.} Every subset of an exchangeable sequence is exchangeable.\\
%
%\vskip .1 in



%As a by-product we see that marginal distributions of this collection will only depend on the number of random variables under consideration
%and {\em not} which one(s) nor in which order they are observed.



%\newpage



\newpage

\noindent {\bf Symmetry and Exchangeability}\label{exchangeability}\\

\noindent \textbf{Exchangeable random variables.} \\
$X_1, X_2, \dots, X_n$ are {\bf\em exchangeable}\label{exchangeability3}
if for every permutation $i_1, i_2, \dots, i_n$ of $1, 2, \dots, n$, we have that $(X_{i_1}, X_{i_2}, \dots, X_{i_n})$ has the same distribution as $(X_1, X_2, \dots, X_n)$.
For example, when $n = 3$, $X_1, X_2, $ and $X_3$ are exchangeable if
$$\begin{array}{ccc}
(X_1, X_2, X_3), & (X_1, X_3, X_2), & (X_2, X_1, X_3), \\
(X_2, X_3, X_1), & (X_3, X_1, X_2), & (X_3, X_2, X_1) \\
\end{array}$$
all have \emph{\textbf{the same}} joint distribution.

\vskip .4 in

\noindent \textbf{Symmetric functions.}\label{symmetricfunction} \\
A real-valued function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is called {\bf\em symmetric} if for every permutation $i_1, i_2, \dots, i_n$ of $1, 2, \dots, n$ we have
$$f(x_{i_1}, x_{i_2}, \dots, x_{i_n}) = f(x_1, x_2, \dots, x_n) \quad\quad\text{for all } (x_1, x_2, \dots, x_n) \in \mathbb{R}^n.$$
Here are some symmetric functions:
\begin{eqnarray*}
    1. \quad f(x_1, x_2, \dots, x_n) &=& \sum_{i=1}^{n} x_i,\quad \text{where }(x_1,x_2,\dots,x_n)\in {\mathbb R}^n \\
    2. \quad f(x_1, x_2, \dots, x_n) &=& \prod_{i=1}^{n} x_i,\quad \text{where }x_1>0,x_2>0,\dots,x_n>0.\\
\end{eqnarray*}

\vskip .4 in

\noindent \textbf{Example.} \\
Which (if any) are symmetric functions?
\begin{align*}
    & 1. \quad f(x, y) = \left\{ \begin{array}{ll}
          e^{-x-y} & \text{for }x>0, y>0 \\
          0 &\text{elsewhere}
    \end{array} \right. \\ \\
    & 2. \quad f(x, y, z) = \left\{ \begin{array}{ll}
          xy + z & \text{for }x, y, z>0 \\
          0 &\text{elsewhere}
    \end{array} \right. \\ \\
    & 3. \quad f(x, y) = \left\{ \begin{array}{ll}
          2xy & \text{for }0 < x < 1, ~ 0 < y < 2 \\
          0 &\text{elsewhere}
    \end{array} \right.
\end{align*}
\noindent SOLUTION: \begin{enumerate}
    \item This function is symmetric since $-x-y = -y-x$ for all $x>0, y>0$.
    \item This function is not symmetric. For example, $f(0, 1, 2) = 2 \neq 1 = f(0, 2, 1)$.
    \item This function is not symmetric, since the domain for $x$ and $y$ are not identical. For example, $f(0.9, 1.9) = 3.42 \neq 0 = f(1.9, 0.9)$.
\end{enumerate}



\newpage

\noindent {\bf Remark.}\\
As the last example demonstrates, in order for a function to be symmetric {\em both} its rule and its support need to be permutation invariant. The second function has a support that is permutation invariant but its rule is not; whereas, the third function has a rule that is permutation invariant but its support is not.\\

\vskip .5 in

\noindent  The importance of symmetric functions follows from this\dots\\

\noindent \textbf{Alternate definition of exchangeability.} \\
If $X_1, X_2, \dots, X_n$ are jointly discrete random variables with joint pmf $p$, then they are exchangeable if and only if $p$ is a symmetric function. \\
If $X_1, X_2, \dots, X_n$ are jointly continuous random variables with joint pdf $f$, then they are exchangeable if and only if $f$ is a symmetric function.

\vskip .5 in


\noindent \textbf{Some consequences of exchangeability.} \\
If $X_1, X_2, \dots, X_n$ are exchangeable, then for any $1 \leq k \leq n$, $(X_{i_1}, X_{i_2}, \dots, X_{i_k})$ has the same distribution as $(X_1, X_2, \dots, X_k)$. \\

\noindent When $k = 1$, this says that $X_1, X_2, X_3, \dots, X_n$ all have the same distribution: \emph{exchangeable r.v.s are identically distributed.}\\

\noindent When $k = 2$, this says, for instance,
$$(X_1, X_2), (X_2, X_1), (X_1, X_3), (X_3, X_1), (X_2, X_3), (X_5, X_7), \dots$$
all have the same distribution. \\
For example, if $X_1, X_2, \dots, X_n$ are exchangeable, then
$$E[g(X_3, X_8)] = E[g(X_1, X_2)] = E[g(X_2, X_1)] = \dots$$
provided that the expectation exists. In fact,
$$E[h(X_1, X_2, \dots, X_k)] = E[h(X_{i_1}, X_{i_2}, \dots, X_{i_k})]$$
for any $k$-tuple $(X_{i_1}, X_{i_2}, \dots, X_{i_k})$ of $(X_1, X_2, \dots, X_n).$ \emph{\textbf{The expectation will not change under permutations of its arguments.}}\\

\noindent If $X_1, X_2, \dots, X_n$ are exchangeable, then every subcollection of these is also exchangeable.\\

\newpage


\noindent An important example of an exchangeable sequence is sampling without replacement\dots\label{samplingwithoutreplacement2}\\

\noindent {\bf Theorem.}\\
\noindent Suppose we have a finite population of $N$ distinct objects comprised of $t$ types.  Assume there are $n_k$ objects of type $k$ in this population for $k=1,2,\dots,t$,
where $\sum n_k=N$. The experiment is to draw all $N$ objects without replacement.
On the $i$th draw we observe the rv $X_i$, where $X_i=k$ if we draw a type $k$ object at the $i$th draw. Then, the sequence $X_1,X_2,\dots,X_N$ is exchangeable.\\

\noindent {\bf Proof.}\\
The sample space $\Omega$ of this experiment is the set of all permutations of $N$ objects taken $N$ at a time (assumed equally-likely)
$|\Omega|=N!$, and, if $\omega\in \Omega$, then
$$X_i(\omega)=k\mbox{ iff there is a type $k$ object in the $i$ entry of $\omega$}.$$
Consequently,\vskip -.2 in
$$P(X_1=x_1,X_2=x_2,\dots,X_{N}=x_N) = \frac {n_1!n_2!\cdots n_t!}{N!}$$
for every sequence $x_1,x_2,\dots,x_N$ containing exactly $n_k$ $k's$ for $k=1,2,\dots,t$, and it equals 0 otherwise.
Notice the joint pmf of $X_1,X_2,\dots,X_N$ is a symmetric function of $x_1,x_2,\dots,x_N$, and thus, the collection is {\em exchangeable}.  $\hfill\Box$\\

\vskip .2 in

\noindent {\bf Example.}\\
Suppose we have 4 green balls and 6 blue balls and we draw without replacement. Let $X_i = 1$ if the $i$th draw is green, $X_i = 0$ if otherwise. Then, from the theorem,
$X_1, X_2, \dots, X_{10}$ are exchangeable.  Compute

\noindent (a) $P(X_6 = 0 | X_{10} = 1)$ \\
\noindent (b) $E[X_6 + (X_9 + X_{10})^3]$ \\

\noindent SOLUTION:\\
\noindent (a)

$P(X_6 = 0 | X_{10} = 1)=\frac{P(X_6 = 0, X_{10} = 1)}{P(X_{10} = 1)} = \frac{P(X_2 = 0, X_1 = 1)}{P(X_1 = 1)} = P(X_2=0|X_1=1)=\frac 69=\frac 23.$

\noindent Also,

$P(X_6 = 0 | X_{10} = 1)=\frac{P(X_6 = 0, X_{10} = 1)}{P(X_{10} = 1)} = \frac{P(X_2 = 1, X_1 = 0)}{P(X_1 = 1)}=\frac {\frac 6{10}\cdot\frac {4}{9}}{\frac 4{10}}=\frac 23.$

\noindent (b)

\begin{eqnarray*}
E[X_6 + (X_9 + X_{10})^3]&=& E(X_6) + E[(X_9 + X_{10})^3] = E(X_1) + E[(\underbrace{X_1 + X_2}_{\mbox{\tiny a hypergeom.}})^3]\\
&=& \frac 4{10} + \left( 0^3\cdot \frac {{4\choose 0}{6\choose 2}}{{10\choose 2}} + 1^3\cdot \frac {{4\choose 1}{6\choose 1}}{{10\choose 2}} + 2^3\cdot \frac {{4\choose 2}{6\choose 0}}{{10\choose 2}}\right)\\
&=& 2.\\
\end{eqnarray*}


\newpage

\noindent {\bf Example.}

\noindent Each card in a well-shuffled deck is turned over one after the other.

(a) What's the probability that the last card is a king?

(b) What's the probability that the last two cards are kings?

(c) If the last two cards are kings, what's the probability that the first card is a king?

(d) What's the probability that all the kings happen before all the aces?\\


\noindent {\bf solution.}

\noindent (a) Let a king be a type 1. $P(X_{52}=1)=P(X_1=1)=\frac 1{13}$.\\

\noindent (b) $P(X_{51}=1,X_{52}=1) = P(X_1=1,X_2=1) = P(X_1=1)P(X_2=1|X_1=1) = \frac 1{13}\cdot \frac 3{51}$.\\

\noindent (c) $P(X_1=1|X_{51}=1,X_{52}=1) = \frac {P(X_1=1,X_{51}=1,X_{52}=1)}{P(X_{51}=1,X_{52}=1)}=\frac {P(X_1=1,X_{2}=1,X_{3}=1)}{P(X_{1}=1,X_{2}=1)}=\frac {\frac 14\cdot \frac {3}{51}\cdot\frac {2}{50}}{\frac 14\cdot\frac 3{51}}=\frac 1{25}$.\\

\noindent (d) Let an ace be a type 2. We first compute
\begin{equation}P(X_{i_1}=1,X_{i_2}=1,X_{i_3}=1,X_{i_4}=1,X_{i_5}=2,X_{i_6}=2,X_{i_7}=2,X_{i_8}=2)\label{eq1}\end{equation}
when $1\le i_1<i_2<i_3<i_4<i_5<i_6<i_7<i_8\le 52$.  By exchangeability, for each such ordering of the indices, (\ref{eq1}) is equal to
\begin{equation}P(X_{1}=1,X_{2}=1,X_{3}=1,X_{4}=1,X_{5}=2,X_{6}=2,X_{7}=2,X_{8}=2)=\frac {4!4!}{(52)_8}.\label{eq2}\end{equation}
But, since there are ${52\choose 8}$ ways to choose the indices in this fashion, the answer is
$${52\choose 8}\cdot \frac {4!4!}{(52)_8} = \frac {4!4!}{8!}.$$
The intuition here is that only the relative positions of the 8 cards (the 4 kings and 4 aces) really matter: there are $4!4!$ orderings that keep the 4 kings before the
4 aces out of $8!$ equally likely possibilities.


\vskip .2 in

\noindent {\bf Remark.}\\
A special case of the Theorem is when the population consists of $t=2$ types, say, successes and failures, and we set $X_i=1$ if the $i$th draw is a success and
$X_i=0$ if the $i$th draw is a failure.  Then, for every fixed $n$, $X_1,X_2,\dots, X_n$ is exchangeable.  This is the hypergeometric experiment of page \pageref{d:hypergeom}.
Another special case of the Theorem is when the population consists of $t=N$ types, i.e., one each of $N$ different types.  Again, for every fixed $n$,
$X_1,X_2,\dots,X_n$ is exchangeable. When $N=5$ and $n=3$, this is the example on page \pageref{exchangeability2}.\\




\newpage



\noindent Another important example of exchangeable random variables are iid random variables.\\

\noindent \textbf{Theorem.} (iid random variables are exchangeable) \\
If $X_1, X_2, \dots, X_n \sim$ iid, then $X_1, X_2, \dots, X_n$ are exchangeable. \\


\noindent \textbf{Remark.} \\
It should be immediately clear why: for instance, if $X_1, X_2, \dots, X_n$ are iid continuous, then the joint pdf of $X_1, X_2, \dots, X_n$ is
$$f_{X_1, X_2, \dots, X_n}(x_1, x_2, \dots, x_n) = f(x_1)f(x_2)\dots f(x_n), $$
which is a symmetric function.\\



\noindent \textbf{Remark.} \\
Suppose $X_1, X_2, \dots, X_n$ are exchangeable. If we have an event involving some (or all) of these r.v.s, AND, if all the events with the random variables permuted form an exhaustive collection, then often this leads to simple computations of probabilities.\\


\noindent \textbf{Example.}
Suppose $X_1, X_2, X_3 \sim$ iid uniform(0, 1).
Compute $P(X_3 < X_2).$ \\

\noindent SOLUTION: \\
    Let $a$ denote $P(X_3 < X_2).$ For any continuous distribution, we have
    $$a = P(X_3 < X_2) = \int_{-\infty}^{\infty}\int_{-\infty}^{x_2} f(x_2)f(x_3)\,dx_3 \,dx_2.$$
    However, since $X_2, X_3$ are exchangeable, $P(X_2 < X_3) = P(X_3 < X_2) = a.$
    Also,
    $$\Omega = (X_2 < X_3) \cup (X_3 < X_2) \cup (X_3 = X_2).$$
    Note that since these rvs are continuous, we can ``ignore" the event $(X_3 = X_2)$ of probability zero.  Therefore we have
    $$1 = P(X_2 < X_3) + P(X_3 < X_2) + 0 = 2a,$$
    $$P(X_3 < X_2) = a = \frac{1}{2}.$$

\bigskip
\noindent (\emph{continued}) Compute $P(X_1 < X_2 < X_3).$ \\

\noindent SOLUTION:\\
    The event $(X_1 < X_2 < X_3)$ by exchangeability has the same probability as the event with the rvs permuted in any rearrangement $(X_{i_1} < X_{i_2} < X_{i_3})$ where $(i_1, i_2, i_3)$ is a permutation of 3 indices from $\{1, 2, 3\}$. Also note that all the events with the rvs permuted form a partition of the entire sample space $\Omega$. Therefore we have
    $$3! \cdot P(X_1 < X_2 < X_3) = 1, \quad P(X_1 < X_2 < X_3) = \frac{1}{3!}.$$



\newpage


\noindent \textbf{Remark.} \\
In the example above, exchangeability will still imply that
$$P(X_3 < 2X_2) = P(X_2 < 2X_3) = P(X_1 < 2X_2) = \dots$$
However, the events $(X_3 < 2X_2)$ and $(X_2 < 2X_3)$ are NOT exhaustive.

\vskip .5 in

\noindent \textbf{Example.}\\
Suppose $X, Y \sim$ iid geom($p$). Compute $P(X<Y)$.\\

\noindent SOLUTION:\\
Let $a$ denote $P(X < Y)$; then $P(Y < X) = P(X < Y) = a$, too. Let $b$ denote $P(X = Y)$. Then since $\Omega = (X < Y) \cup (Y < X) \cup (X = Y),$
we have
$1 = a + a + b.$ Moreover, unlike the last example, these r.v.s are discrete, so $b$ may not be zero.
\begin{eqnarray*}
    b &=& P(X = Y) = \sum_{x=1}^{\infty}P(X = x, Y = x)
    = \sum_{x=1}^{\infty}P(X = x)P(Y = x) \\
    &=&\sum_{x=1}^{\infty}(1-p)^{x-1} \cdot p \cdot (1-p)^{x-1} \cdot p \\
    &=& \frac{p^2}{(1-p)^2} \sum_{x=1}^{\infty}(1-p)^{2x}  \\
    &=& \frac{p^2}{(1-p)^2} \cdot \frac{(1-p)^2}{1 - (1-p)^2} \\
    &=& \frac{p^2}{1-(1-p)^2} = \frac{p}{2 - p}.
\end{eqnarray*}
Therefore,
$$P(X<Y) = a = \frac{1-b}{2} = \frac{1-p}{2-p}.$$



\newpage




\noindent \textbf{Example.} \\
Suppose $X_1, X_2, X_3, X_4$ has the joint pdf
\begin{align*}
    f_{X_1, X_2, X_3, X_4}(x_1, x_3, x_3, x_4) = \left\{ \begin{array}{ll}
     \frac 12(x_1+x_2+x_3+x_4) & \quad \text{for } 0 \leq x_i \leq 1, i = 1,2,3,4 \\
     0 & \quad \text{otherwise.}
\end{array} \right. \\ \\
= \frac 12(x_1+x_2+x_3+x_4)1_{(0, 1)}(x_1)1_{(0, 1)}(x_2)1_{(0, 1)}(x_3)1_{(0, 1)}(x_4).
\end{align*}
Find the probability that $X_1$ is the largest, i.e., $P(X_1>X_2,X_1>X_3,X_1>X_4)$.\\

\noindent SOLUTION:\\
The joint pdf is clearly a symmetric function (permutation invariant rule and support), and therefore, the rvs are exchangeable.  It must follow that
$$P(X_1 \text{ is the largest}) = P(X_2 \text{ is the largest})= P(X_3 \text{ is the largest})= P(X_4 \text{ is the largest}).$$
Moreover, the events are exhaustive, one rv will be the largest (ties can be neglected since rvs are continuous), and
$$4 \cdot P(X_1 \text{ is the largest}) = 1, \quad P(X_1 \text{ is the largest}) = \frac{1}{4}.$$

\vskip 1 in

\noindent {\bf Exchangeable events.}\label{exchangeableevents}\\
\noindent If we have events $A_1, A_2, \dots, A_n$ such that the indicators $X_i = 1_{A_i}$ are exchangeable, then we call the events exchangeable. For example, in sampling without replacement from a finite population of successes and failures, the event $A_i (i = 1, \dots, n)$ of drawing a success on $i$th draw are exchangeable events.\\

\noindent Next we introduce a big theorem in exchangeable rvs (students are not responsible for this material in our course).\\

\noindent \textbf{de Finetti's theorem.} \label{definettistheorem}\\
Let $A_1, A_2, \dots, A_n$ be exchangeable events and let $S$ count the number of these events that occur. Then there exists a unique non-negative function $f$ on $[0, 1]$ such that
$$\int_0^1 f(\theta) \,d\theta = 1,$$
and, for any $k$ such that $0 \leq k \leq n$,
$$P(S = k) = \int_0^1 \binom{n}{k} \theta^k(1-\theta)^{n-k} f(\theta) \,d\theta$$



\newpage



\noindent \textbf{Application: P\'{o}lya's urn.}\label{polyaurn} \\
Fix an integer $c\ge -1$.  Suppose an urn initially contains $a$ white balls and $b$ black balls
($a+b$ balls total).  A ball is drawn uniformly at random from these and replaced together with $c$ more balls of the same color (so that the urn will have $a + b + c$ balls in total at the next draw), and this process is repeated.

Let $A_i$ be the event a white ball is drawn on the $i$th trial. Then set
$X_i = 1_{A_i}$,
and
$S_n = \sum_{i = 1}^n X_i$
is the total number of white balls drawn in first $n$ trials.

 William Feller in his famous book {\em Probability Theory and its Applications} (Volume 1 (1968)) showed:

{\em For every $n$, the events
$A_1, A_2, A_3, \dots, A_n$
are exchangeable, and}
$$P(S_n = k) = \binom{n}{k} \frac{a(a+c)\ldots(a+(k-1)c)\cdot b(b+c)\dots(b+(n-k-1)c)}{(a+b)(a+b+c)\dots(a+b+(n-1)c)}.$$
This is called the {\bf\em P\'{o}lya-Eggenberger distribution} with parameters $a, b, $ and $c$.\label{polyaeggenberger}\\

\noindent {\bf Some special cases of P\'{o}lya's urn:}\\
\noindent When $c = 0$, this is a binomial experiment and, for every $n$, $S_n\sim \mbox{binom}\left(n, \frac{a}{a+b}\right)$.

\noindent When $c = -1$, this is a hypergeometric experiment, accordingly, $S_n$ will have a hypergeometric distribution with $N=a+b$, $M=a$.

\noindent When $a = b = c = 1,$ the distribution reduces to
$$P(S_n = k) = \binom{n}{k} \frac{k!(n-k)!}{(n+1)!} = \frac{1}{n+1}.$$
This is the discrete uniform\label{discreteuniform3} distribution on $\{0, 1, 2, \dots, n\}.$ \\
By de Finetti's theorem, we know there exists a pdf $f(\theta)$ such that
\begin{equation}
    \frac{1}{n+1} = \binom{n}{k} \int_0^1 \theta^k (1 - \theta)^{n-k} f(\theta) \,d\theta \quad\quad \text{for each }k,~~ 0 \leq k \leq n. \label{eq: de Finetti}
\end{equation}
So say we take $k = n$, then ($\ref{eq: de Finetti}$) reduces to
$$\frac{1}{n+1} = \int_0^1 \theta^n f(\theta) \,d\theta.$$
We get lucky here because by inspection we see $f(\theta) = 1$ for $0 < \theta < 1$. And, therefore, for our Polya urn with $a = b = c = 1$, it follows from de Finetti's theorem, for each $k = 0, 1, \dots, n$,
$$P(S_n = k) = \binom{n}{k} \int_0^1 \theta^k(1-\theta)^{n-k} \,d\theta.$$
The student may want to compare what we did here with the example on page \pageref{betabinomialexample}.\\

\newpage


\noindent \textbf{Remark.
} \\
For general $a, b, $ and $c$ the required $f = f(\theta)$ from de Finetti's theorem turns out to be
\begin{equation*}
   f(\theta)= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} \quad\quad \text{for } 0 < \theta < 1.
\end{equation*}
where
$$\alpha = \frac{a}{c} \quad \text{and} \quad \beta = \frac{b}{c}.$$
This says the ``mixing" distribution is beta$\left(\frac{a}{c}, \frac{b}{c}\right)$, and
\begin{eqnarray*}
    P(S_n = k) &=& \binom{n}{k} \int_0^1 \theta^k(1-\theta)^{n-k}f(\theta) \,d\theta \\
    &=& \binom{n}{k} \frac{\Gamma(\frac{a+b}{c})}{\Gamma(\frac{a}{c}) \Gamma(\frac{b}{c})}\int_0^1 \theta^{k+\frac{a}{c}-1}(1-\theta)^{n-k+\frac{b}{c}-1} \,d\theta \\
    &=& \binom{n}{k} \frac{\Gamma(\frac{a+b}{c})}{\Gamma(\frac{a}{c}) \Gamma(\frac{b}{c})}\frac {\Gamma(k+\frac{a}{c})\Gamma(n-k+\frac{b}{c})}{\Gamma(n+\frac {a+b}{c})}. \\
\end{eqnarray*}




\newpage

\begin{center}{\bf \Large VI. First- and second-order results.}\end{center}

\newpage




\noindent {\bf Expectations: linearity of expectation.}\label{linearityofexpectation}\\
%\noindent Recall that
%$$E[g(X,Y)] = \left\{\begin{array}{cl} \sum_x\sum_y g(x,y)P(X=x,Y=y) & \mbox{if $X,Y$ are jointly discrete, or }\\
%& \\
% \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x,y)f_{X,Y}(x,y)\,dxdy & \mbox{if $X,Y$ are jointly continuous.}\\   \end{array}\right.$$

%\noindent \textbf{Linearity of expectation} \\

\noindent Assume $X_1, X_2, \dots, X_n$ are r.v.s possessing expected values. Then
\begin{equation*}
    E\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n E(X_i).
\end{equation*}

\noindent {\bf Proof.}\\
\noindent We'll assume the rvs are jointly continuous (essentially the same proof works when the rvs are jointly discrete).
Proceeding by induction we suppose $n = 2$. Then
\begin{eqnarray*}
    E(X + Y) &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x + y) f_{X, Y}(x, y) \,dx \,dy \\
    &=& \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \cdot f_{X, Y}(x, y) \,dx \,dy + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X, Y}(x, y) \,dx \,dy \\
    &=& \int_{-\infty}^{\infty} x \int_{-\infty}^{\infty} f_{X, Y}(x, y) \,dy \,dx + \int_{-\infty}^{\infty} y \int_{-\infty}^{\infty} f_{X, Y}(x, y) \,dx \,dy \\
    &=& \int_{-\infty}^{\infty} x \cdot f_X(x) \,dx + \int_{-\infty}^{\infty} y \cdot f_Y(y) \,dy = E(X)+E(Y).\\
\end{eqnarray*}

\noindent Suppose, for some $n = k \geq 2$, $E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k E[X_i].$
Then,
\begin{align*}
    E\left[\sum_{i=1}^{k+1} X_i\right] &= E\left[\left(\sum_{i=1}^k X_i\right) + X_{k+1}\right] \\
    &= E\left[\sum_{i=1}^k X_i\right] + E(X_{k+1}) & \text{by our base case }k=2;\\
    &= \sum_{i=1}^{k} E(X_i) + E(X_{k+1}) & \text{by our inductive hypothesis};\\
    &= \sum_{i=1}^{k+1} E(X_i),
\end{align*}
which completes the induction. $\hfill \square$

\vspace{0.75cm}

\noindent \textbf{Remark.} \\
Whenever we can recognize a random variable as a sum of random variables, then linearity of expectation is often a useful approach to computing its expected value.

\vspace{0.75cm}

\noindent \textbf{Example.}\label{expectedvalueofbinomial} \\
If $X \sim$ binomial($n, p$), then we know $X = \sum_{i=1}^n X_i$, where $X_1, X_2, \dots, X_n \sim$ iid Bernoulli($p$). Since $E(X_i) = p$ for all $i$, it follows that
\begin{equation*}
    E(X) = E\left(\sum_{i=1}^n X_i \right) = \sum_{i=1}^n E(X_i) = \sum_{i=1}^n p = np.
\end{equation*}




\newpage




\noindent \textbf{Example.}\label{expectedvalueofhypergeometric} \\
If $X \sim$ hypergeometric (the population has $M$ successes, $N-M$ failures, and we draw a random sample of size $n$ {\em without} replacement), then
\begin{equation*}
    X = \sum_{i=1}^n X_i \quad \text{where } X_1, X_2, \dots, X_n \sim \text{Bernoulli}\left(\frac{M}{N}\right).
\end{equation*}
Note that these Bernoullis are dependent and exchangeable. In particular, $E(X_i) = E(X_1)=\frac MN$ for every $i$. Therefore we have
\begin{equation*}
    E(X) = E\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n E(X_i) = \sum_{i=1}^n \frac{M}{N} = n \cdot \frac{M}{N}.
\end{equation*}

\vspace{0.5cm}

\noindent \textbf{Remark.} \\
We can also compute $E(X) = \sum_{k=0}^n k \frac{\binom{M}{k} \binom{N-M}{n-k}}{\binom{N}{n}}$, but this is brutal in comparison to the previous method.\\

\vskip .2 in

\noindent Linearity of expectation can be useful when trying to compute the expected value of a ``counting random variable" since it may be the case that we can
decompose the rv into a sum of ``simpler" rvs.  We illustrate with the next few examples.\\


\noindent \textbf{Example.} \\
We deal 5 cards from a deck of 52. Compute the expected number of suits in your hand.\\

\noindent SOLUTION: Let $X$ count the number of suits in a hand. Additionally, for $i \in \{1, 2, 3, 4\}$, let
\begin{align*}
    X_i = \left\{\begin{array}{ll}
         1 & \text{if we select (at least) one suit $i$} \\
         0 & \text{if not.}
    \end{array} \right.
\end{align*}
Then $X = X_1 + X_2 + X_3 + X_4$, and $E(X) = E(X_1) + E(X_2) + E(X_3) + E(X_4).$
\begin{equation*}
    E(X_i) = P(X_i = 1) = 1 - P(X_i = 0) = 1 - \frac{\binom{39}{5}}{\binom{52}{5}} \quad\quad \text{for } i = 1, 2, 3, 4.
\end{equation*}
So
$$
E(X) = 4 \cdot \left(1 - \frac{\binom{39}{5}}{\binom{52}{5}} \right) \approx 3.11\dots
$$

\newpage


\noindent \textbf{Example.} \\
10 children comprised of 4 boys and 6 girls are randomly seated in 5 pairs of chairs. E.g, if the children are
$$b_1, b_2, b_3, b_4, \quad g_1, g_2, g_3, g_4, g_5, g_6,$$
then
$$(g_3, g_2, b_2, g_1, g_6, b_4, b_1, g_5, g_4, b_3)$$
is one such seating. There are $10!$ seatings in total.

%\begin{figure}[h!]
%    \centering
%    \includegraphics[width = 0.75\linewidth]{linearity_expectations_figs/chairs_example.jpeg}
%\end{figure}

\includegraphics*[-70,0][400,80]{pairs-of-chairs.jpg}

\bigskip

\noindent Let $X$ be the random variable that counts the number of pairs that are occupied by the same gender. Compute $E(X).$\\

\noindent SOLUTION: \\
For $i \in \{1, 2, 3, 4, 5\}$, let
$$X_i = \left\{\begin{array}{ll} 1 & \mbox{if pair $i$ has 2 children of the same gender} \\
0 & \mbox{if otherwise.} \end{array} \right.$$
\noindent Then $X_i \stackrel {\mathscr{D}}{=} X_1$ for all $i$.  Therefore, $X = \sum_{i=1}^5 X_i$ and  $E(X)=\sum_{i=1}^5E(X_i) = 5 \cdot E(X_1)$. We can compute
$$E(X_1) = P(X_1 = 1) = \frac{\binom{4}{2} + \binom{6}{2}}{\binom{10}{2}} = \frac{6+15}{45} = \frac{7}{15},
$$
so
$$E(X) = \sum_{i=1}^5 E(X_i) = 5 \cdot E(X_1) = \frac{7}{3}.$$

\vskip 1 in

\noindent \textbf{Remark.}\\
With the last two examples we were able to decompose the rv of interest into a {\em finite} sum of (exchangeable) Bernoullis.
This worked especially well because the counting rv was bounded.  The next example illustrates a situation where the counting rv of interest
is unbounded.


\newpage


\noindent \textbf{Example.} \\
We have a fair 6-sided die. Compute the expected number of rolls to see every face of the die (at least once).\\


\noindent SOLUTION:
\begin{itemize}
    \item Let $X_1$ be the number of rolls to see the first face ($P(X_1 = 1) = 1$).
    \item Let $X_2$ be the number of additional rolls to see a face distinct from the first face:
    $$X_2 \sim \text{geometric}(\frac{5}{6}),\quad E(X_2) = \frac{1}{\frac{5}{6}} = \frac{6}{5}.$$
    \item Let $X_3$ be the number of additional rolls to see a face distinct from the first 2 faces:
    $$X_3 \sim \text{geometric}(\frac{4}{6}),\quad E(X_3) = \frac{6}{4}.$$
    \item Let $X_4$ be the number of additional rolls to see a face distinct from the first 3 faces:
    $$X_4 \sim \text{geometric}(\frac{3}{6}),\quad E(X_4) = \frac{6}{3}.$$
    \item Similarly,
    $$X_5 \sim \text{geometric}(\frac{2}{6}),\quad E(X_5) = \frac{6}{2},$$
    $$X_6 \sim \text{geometric}(\frac{1}{6}),\quad E(X_6) = \frac{6}{1}.$$
\end{itemize}
Therefore, $X = X_1 + X_2 + X_3+ X_4 + X_5 + X_6,$
and so
\begin{align*}
    E(X) &= E(X_1) + E(X_2) + E(X_3) + E(X_4) + E(X_5) + E(X_6) \\
    &= \frac{6}{6} + \frac{5}{6} + \frac{4}{6} + \frac{3}{6} + \frac{2}{6} + \frac{1}{6} \\
    &= \frac{147}{10} = 14.7.
\end{align*}

\vspace{0.75cm}

\noindent \textbf{Remark.} \\
We see how linearity of expectation (often combined with exchangeability) leads to efficient computation of expected values. It would be nice if we had a similar result for variances. However,
$$\text{Var}\left(\sum_{i=1}^n X_i \right) \text{ does not always equal }\sum_{i=1}^n \text{Var}(X_i)$$
even when the variances of each $X_i$ exist and are finite.
Let's investigate when $n = 2$; $~$i.e. let's compute Var$(X+Y)$ (tacitly assuming that $X, Y$ possess variances).\\

\newpage

\begin{eqnarray*}
    Var(X+Y) &=& E\left[(X+Y)^2\right] - \left[E(X+Y)\right]^2 \\
    &=& E\left[X^2 + 2XY + Y^2\right] - \left[E(X) + E(Y)\right]^2 \\
    &=& E(X^2) + 2E(XY) + E(Y^2) - \{[E(X)]^2 + 2E(X)E(Y) + [E(Y)]^2\} \\
    &=& Var(X) + Var(Y) + 2[E(XY) - E(X)E(Y)],
\end{eqnarray*}
and we see that the variance of a sum \label{varofsum}differs from the sum of the variances by a term involving the expression $E(XY)-E(X)E(Y)$. In probability this expression is called
the covariance between $X$ and $Y$, which we will now investigate.

\vspace{0.75cm}

\noindent \textbf{Covariance between two random variables.} \label{covariance}\\
Let $X$ and $Y$ be jointly distributed random variables that have finite means $\mu_X, \mu_Y$ and finite variances $\sigma_X^2, \sigma_Y^2$. Then the covariance of $X, Y$ is defined as
$$Cov(X,Y) = E[(X - \mu_X)(Y - \mu_Y)].$$
Just as variance has a ``computationally friendly" version so does covariance:
\begin{eqnarray*}
    \text{Cov}(X, Y) &=& E[(X - \mu_X)(Y - \mu_Y)] \\
    &=& E[(X - E(X))(Y - E(Y))] \\
    &=& E[XY - XE(Y) - E(X)Y + E(X)E(Y)] \\
    &=& E(XY) - E[XE(Y)] - E[E(X)Y] + E[E(X)E(Y)] \\
    &=& E(XY) - E(Y)E(X) - E(X)E(Y) + E(X)E(Y) \\
    &=& E(XY) - E(X)E(Y).
\end{eqnarray*}

\vskip .5 in

\noindent Cov$(X, Y)$ can be positive, negative, or zero. If Cov$(X, Y) = 0$, we say that $X$ and $Y$ are {\bf\em uncorrelated}.\label{uncorrelated} \\


\vskip .5 in

\noindent {\bf What does covariance measure?}\\
Covariance between $X$ and $Y$ is measuring the amount of linear association between the random variables.  Loosely speaking
a positive covariance means that if the value of one variable increases (decreases) then, on average, the value of the other also increases (decreases).
A negative covariance would mean if the value of one variable increases (decreases) then, on average, the value of the other decreases (increases).
A zero covariance means there is {\em no} linear association.
For example, if $X$ is the surface area of a strawberry and $Y$ is the number of seeds on the strawberry, then we might suspect $Cov(X,Y)>0$.
On the other hand, if $X$ is the temperature outside and $Y$ is the number of people taking a leisurely stroll in the park, then we might suspect $Cov(X,Y)<0$.



\newpage



\noindent {\bf Properties of covariance.}\label{covarianceproperties}\\

{\bf 1.} Covariance is symmetric: Cov$(X, Y)$ = Cov$(Y, X)$.

{\bf 2.} Cov($X, X$) $= E(X^2) - [E(X)]^2 =$ Var($X$).

{\bf 3.} Covariance is a bilinear \label{covbilinear}form:$$\text{Cov}\left(\sum_{i=1}^n a_i X_i, Y\right) = \sum_{i=1}^n a_i \text{Cov}(X_i, Y);$$
    $$\text{Cov}\left(X, \sum_{j=1}^m b_j Y_j\right) = \sum_{j=1}^m b_j \text{Cov}(X, Y_j).$$

The bilinear property \label{covbilinearity}can also be stated as
$$\text{Cov}\left(\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_j Y_j\right) = \sum_{i=1}^n \sum_{j=1}^m a_ib_j \text{Cov}(X_i, Y_j).$$

\vskip .5 in

\noindent {\bf Variance of a sum of random variables}\label{varianceofasum}\\
\begin{eqnarray*}
\text{Var}\left(\sum_{i=1}^n a_i X_i\right) &=& \sum_{i=1}^n a_i^2 \text{Var}(X_i) + \underbrace{\sum_{i=1}^n\sum_{j=1}^n}_{i \neq j} a_i a_j \text{Cov}(X_i, X_j)\\
&=&\sum_{i=1}^n a_i^2 \text{Var}(X_i) + 2\underbrace{\sum_{i=1}^n\sum_{j=1}^n}_{i < j} a_i a_j \text{Cov}(X_i, X_j).\end{eqnarray*}


\noindent {\bf Proof.}\\
Properties 2 and 3 of covariance combine to give
\begin{eqnarray*}
    \text{Var}\left(\sum_{i=1}^n a_i X_i\right) &=& \text{Cov}\left(\sum_{i=1}^n a_i X_i, \sum_{j=1}^n a_j X_j\right) \\
    &=& \sum_{i=1}^n\sum_{j=1}^n a_i a_j \text{Cov}(X_i, X_j).
\end{eqnarray*}
Now consider the following $n \times n$ matrix:
\begin{equation*}
    \begin{bmatrix}
    a_1^2\text{Cov}(X_1, X_1) & a_1a_2\text{Cov}(X_1, X_2) & \dots & a_1a_n\text{Cov}(X_1, X_n) \\
    a_2a_1\text{Cov}(X_2, X_1) & a_2^2\text{Cov}(X_2, X_2) & \dots & a_2a_n\text{Cov}(X_2, X_n) \\
    \vdots & \vdots  & \ddots & \vdots \\
    a_na_1\text{Cov}(X_n, X_1) & a_na_2\text{Cov}(X_n, X_2) & \dots & a_n^2\text{Cov}(X_n, X_n) \\
    \end{bmatrix}.
\end{equation*}

\noindent Notice that Var$\left(\sum_{i=1}^n a_iX_i\right)$ is the sum of all entries of this matrix.\\
By property 2 of covariance, each entry on the principal diagonal is $a_i^2$Cov$(X_i, X_i) = a_i^2$Var$(X_i)$. By property 1 of covariance, $a_ia_j$Cov$(X_i, X_j) = a_ja_i$Cov$(X_j, X_i)$.  The result follows directly from these observations.$\hfill\Box$


\newpage


\noindent \textbf{Special Cases.}
\begin{eqnarray*}
    \text{Var}(X+Y) &=& \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y)\\
    & & \\
    \text{Var}(X-Y) &=& \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X, Y)\\
\end{eqnarray*}\vskip -.2 in
$$\text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n\text{Var}(X_i) + \underbrace{\sum_{i=1}^n\sum_{j=1}^n}_{i \neq j} \text{Cov}(X_i, X_j),$$
\qquad and when the rvs $X_1,X_2,\dots,X_n$ are exchangeable,
$$\text{Var}\left(\sum_{i=1}^n X_i\right) = n\text{Var}(X_1) + n(n-1) \text{Cov}(X_1, X_2).$$

\vskip .5 in
\noindent {\bf Remark.}\\
From page \pageref{expectedvalueofaproduct} when $X, Y$ are independent, $E[h_1(X)h_2(Y)] = E[h_1(X)]E[h_2(Y)]$ assuming expectations exist and are finite, and therefore,
if $X$ and $Y$ are independent each possessing a variance, then $X$ and $Y$ are uncorrelated, and $E(XY)-E(X)E(Y)=0$ and $Var(X+Y)=Var(X)+Var(Y)$.  In general, if $X_1,X_2,\dots,X_n$ are independent (or just pairwise independent) possessing variances then they are uncorrelated and
$$Var(\sum_{i=1}^nX_i) = \sum_{i=1}^nVar(X_i).$$


%the variance of the sum is the sum of the variances:
%$$Var(\sum_{i=1}^n X_i)= \sum_{i=1}^n Var(X_i)$$
%assuming that the rvs possess a variance. In fact, had the rvs $X_1,X_2,\dots,X_n$ been only uncorrelated, which means $Cov(X_i,X_j)=0$ for all $i\ne j$ (a far weaker condition %than independence!), then the variance of the sum is the sum of the variances.

\vskip .2 in

\noindent \textbf{Remark.}\\
There are uncorrelated random variables that are not independent.\\

\vskip .2 in

\noindent \textbf{Exercise for the student.}\\
Let $X$ be the discrete rv with pmf $P(X = -1) = P(X = 1) = \frac{1}{4}, P(X = 0) = \frac{1}{2}$, and set $Y = X^2$.
\begin{enumerate}
    \item Show $X, Y$ are uncorrelated.
    \item Show $X, Y$ are NOT independent.
\end{enumerate}





\newpage


\noindent {\bf Example.}\\
Suppose $X\sim \mbox{binom}(n,p)$.  Compute $Var(X)$.\\

\noindent SOLUTION:\\
We first note that $X=X_1+X_2+\cdots+X_n$, where $X_1,X_2,\dots,X_n\sim$ iid Bernoulli$(p)$. Moreover, $Var(X_i)=p(1-p)$. Thus,
$$Var(X) = Var(\sum_{i=1}^nX_i) =\sum_{i=1}^nVar(X_i) = \sum_{i=1}^np(1-p)=np(1-p).$$

\vskip .5 in


\noindent \textbf{Example.}\\
Suppose $X \sim$ hypergeometric (the population has $M$ successes, $N-M$ failures, and we draw a random sample of size $n$ without replacement). Compute Var($X$).\\

\noindent SOLUTION:\\
We know $X = X_1 + X_2 + \dots + X_n$, where each $X_i \sim $ Bernoulli($\frac{M}{N}$). So,
$$
    \text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{Var}(X_i) + \underbrace{\sum_{i=1}^n\sum_{j=1}^n}_{i \neq j} \text{Cov}(X_i, X_j)
$$
and, since the rvs are exchangeable, in fact,
We claim that for all $i \neq j$, (since the rvs are identically distributed),
$$
    \text{Var}\left(\sum_{i=1}^n X_i\right) = n\text{Var}(X_1) + n(n-1) \text{Cov}(X_1, X_2).
$$
\begin{eqnarray*}
\text{Cov}(X_1, X_2) &=& E(X_1X_2) - E(X_1)E(X_2) \\
&=& P(X_1 = 1, X_2 = 1) - \frac{M}{N} \cdot \frac{M}{N} \\
&=& P(X_2 = 1 | X_1 = 1) P(X_1 = 1) - \left(\frac{M}{N}\right)^2 \\
&=& \frac{M-1}{N-1} \cdot \frac{M}{N} - \left(\frac{M}{N}\right)^2 \\
&=& \frac{M}{N} \left[\frac{M-1}{N-1} - \frac{M}{N}\right] \\
&=& \frac{M}{N}\left(\frac{-N + M}{N(N-1)}\right) = -\frac{M}{N}\left(\frac{N - M}{N(N-1)}\right).
\end{eqnarray*}

\newpage


\noindent Therefore,
\begin{eqnarray*}
    \text{Var}\left(\sum_{i=1}^n X_i\right) &=& n\text{Var}(X_1) + n(n-1)\text{Cov}(X_1, X_2) \\
    &=& n \cdot \frac{M}{N}\left(\frac{N-M}{N}\right) - n(n-1)\cdot \frac{M}{N}\left(\frac{N - M}{N(N-1)}\right) \\
    &=& n \cdot \frac{M}{N}\left(\frac{N-M}{N}\right) \left(1- \frac{n-1}{N-1}\right) \\
    &=& n \cdot \frac{M}{N} \cdot \frac{N-M}{N} \cdot \frac{N-n}{N-1},
\end{eqnarray*}
which is the variance of a binom$(n,\frac MN)$ times a {\bf\em finite population correction}\label{finitepopulationcorrection} term $\frac {N-n}{N-1}$.
When this factor is close to 1 the hypergeometric and binomial are nearly identical distributions!

\vskip .3 in

\noindent \textbf{Example.}\\
$n$ people wearing hats walk into a room. They take off their hats and the hats get mixed. Each person randomly selects a hat.
Let $X$ denote the number of people who select their own hat. Compute $E(X)$ and Var($X$).\\

\noindent SOLUTION:\\
Let
\begin{align*}
    X_i = \left\{\begin{array}{ll}
         1 & \text{if the $i$th person selects their own hat} \\
         0 & \text{otherwise.}
    \end{array} \right.
\end{align*}
Then $X = \sum_{i=1}^n X_i,$ and
\begin{align*}
    E(X) &= \sum_{i=1}^n E(X_i) = \sum_{i=1}^n P(X_i = 1) \\
    &= \sum_{i=1}^n \frac{1}{n} = n \cdot \frac{1}{n} = 1.
\end{align*}
Also, by the exchangeability of these rvs,
\begin{eqnarray*}
    \text{Var}(X) &=& \sum_{i=1}^n \text{Var}(X_i) + \underbrace{\sum_{i=1}^n\sum_{j=1}^n}_{i \neq j} \text{Cov}(X_i, X_j) \\
    &=& n\cdot\frac{1}{n}\left(1 - \frac{1}{n}\right) + n(n-1) \cdot \left[\frac{1}{n} \cdot \frac{1}{n-1} - \left(\frac{1}{n}\right)^2\right] \\
    &=& 1 - \frac{1}{n} + 1 - \frac{n-1}{n} \\
    & & \\
    &=& 1.
\end{eqnarray*}
Note that $E(X) = $ Var$(X) = 1$.


\newpage


\noindent {\bf The Cauchy--Schwarz inequality and correlation.}\label{cauchyschwarz}\\

\noindent \textbf{The Cauchy--Schwarz inequality.} \label{cauchyschwarz2}
If $X, Y$ are random variables having finite means and variances, then
$$|E(XY)| \leq \sqrt{E(X^2)E(Y^2)} = [E(X^2)E(Y^2)]^{\frac{1}{2}}.$$

\noindent {\bf Proof.} (sketch)\\
For real $c$ define the function $g(c) = E[(X+cY)^2]$. This function is nonnegative; moreover,
$$g(c) = E(X^2) + 2cE(XY) + c^2E(Y^2).$$
The value of $c$ that minimizes $g$ satisfies $g'(c) = 2E(XY)+2cE(Y^2)=0$, namely, $c = -\frac {E(XY)}{E(Y^2)}$.
Substituting this value of $c$ into $g(c)$ and noting that $g(c)\ge 0$ gives:
$$E(X^2) -2\frac {E(XY)^2}{E(Y^2)} + \left(\frac {E(XY)}{E(Y^2)}\right)^2E(Y^2)\ge 0,$$
which reduces to the Cauchy--Schwarz inequality.\\

\vskip .4 in

\noindent {\bf Remark.}\\
If we replace $X$ by the rv $X-\mu_X$ and replace $Y$ by rv $Y-\mu_Y$ in the Cauchy--Schwarz inequality, then we get the equivalent form:
$$|\text{Cov}(X, Y)| \leq [\text{Var}(X)\text{Var}(Y)]^{\frac{1}{2}} = \sigma_X \sigma_Y.$$

\vskip .4 in


\noindent \textbf{Correlation between $X$ and $Y$.}\label{correlation}\\
Define the correlation between $X$ and $Y$ by
$$\rho_{X, Y} = \text{corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}.$$
Correlation measures the ``strength" of linear association between $X$ and $Y$.  From the previous remark, the Cauchy--Schwarz inequality
implies
$$|\rho_{X,Y}|\leq 1.$$
The closer $|\rho_{X, Y}|$ is to $1$, the stronger the linear association.



\newpage








\noindent {\bf Conditional expectation.}\label{conditionalexpectation}\\



\noindent We have several different forms; first for \textbf{discrete r.v.s X}.\\
Conditional expectations where we condition on events:\\
If $A$ is an event with $P(A) > 0$, then
$$E(X | A) = \sum_x x \cdot P(X = x | A) = \sum_x x \cdot \frac{P(\{X = x\} \cap A)}{P(A)}.$$

\noindent \textbf{Example.} \\
Suppose $X \sim$ geometric($\frac{1}{3}$), and $A$ is the event a success happens on or before the third trial, i.e. $A = (X \leq 3)$.
In this example,
$$P(X \leq 3) = \frac{1}{3} + \frac{1}{3} \cdot \frac{2}{3} + \frac{1}{3} \cdot \left(\frac{2}{3}\right)^2 = \frac{19}{27}.$$
\begin{eqnarray*}
    E(X | A) &=& E(X | X \leq 3) \\
    &=& \sum_x x \cdot \frac{P(\{X = x\} \cap \{X \leq 3\})}{P(X \leq 3)} \\
    &=& \frac{1 \cdot \frac{1}{3} + 2 \cdot \frac{1}{3} \cdot \frac{2}{3} + 3 \cdot \frac{1}{3} \cdot \left(\frac{2}{3}\right)^2}{\frac{19}{27}} \\
    &=& \frac{33}{19}.
\end{eqnarray*}

\vspace{0.5cm}

\noindent \textbf{Example.} \\
We toss a fair coin $n = 5$ times. Compute the number of heads given the first and last flips differ in parity (i.e., if the first toss is heads, then the last toss is tails, and conversely).
Let $A$ be the event that the first and last flips differ in parity. Then
$$P(A) = \frac{1}{2}.$$
\begin{eqnarray*}
    E(X | A) &=& \sum_{x = 0}^5 x \cdot \frac{P(X = x, A)}{P(A)} \\
    &=& 2 \sum_{x = 0}^5 x \cdot P(X = x, A) \\
    &=& 2 \cdot (1 \cdot P(X = 1, A) + 2 \cdot P(X = 2, A) + 3 \cdot P(X = 3, A) + 4 \cdot P(X = 4, A)) \\
    &=& 2 \cdot \left( 1 \cdot \frac{2}{32} + 2 \cdot \frac{3 + 3}{32} + 3 \cdot \frac{3 + 3}{32} + 4 \cdot \frac{2}{32}\right) \\
    &=& \frac{80}{32}.
\end{eqnarray*}

%\vspace{0.75cm}

\newpage

\noindent \textbf{Example.} (Cute but specialized) \\
You flip a fair coin $n$ times. \\
{\bf 1.} How many sequences have no consecutive heads? Let $A_n$ denote those sequences with no two consecutive heads. \\
{\bf 2.} Let $X = 1$ if a sequence starts with heads, $X = 0$ otherwise. Compute $E(X)$ and $E(X | A_n)$. \\

\noindent SOLUTION:\\
\noindent {\bf 1} is tricky. It is best analyzed by recursions.  Let's do this now:\\
If we flip a coin $n = 1$ time, then $|A_1| = 2$ since $A_1 = \{h, t\}$. \\
If we flip a coin $n = 2$ times, then $|A_2| = 3$ since $A_2 = \{ht, th, tt\}$.\\
Let $F_n = |A_n|$. We just argued that $F_1 = 2, F_2 = 3$. To analyze general $F_n$ with $n > 2$, we ``break up" $A_n$ into two mutually exclusive pieces:\\

case 1: those sequences of length $n$ that start with $t$:

$$\stackrel{\stackrel{{\large t}}{\_\_\_\_}}{{\tiny 1}}\,\stackrel{\_\_\_\_}{{\tiny 2}}\,\stackrel{\_\_\_\_}{{\tiny 3}}\,\cdots \,\stackrel{\_\_\_\_}{{\tiny n}}$$


\noindent and \\

case 2: those sequences of length $n$ that start with $h$:

$$\stackrel{\stackrel{{\large h}}{\_\_\_\_}}{{\tiny 1}}\,\stackrel{\_\_\_\_}{{\tiny 2}}\,\stackrel{\_\_\_\_}{{\tiny 3}}\,\cdots \,\stackrel{\_\_\_\_}{{\tiny n}}$$

\noindent In case 1, since the sequences start with ``tails", the event of no two consecutive ``heads" is the same as the number of sequences of length $n-1$ with no two consecutive heads and there are $F_{n-1}$ of these.  In case 2, if we want no two consecutive heads, then since the sequence starts with heads the next toss will need to be tails which leaves $n-2$ tosses for no two consecutive heads and there are $F_{n-2}$ of these. Putting this together we have

$$F_n = F_{n-1} + F_{n-2}$$
for $n = 3, 4, 5, 6, 7, \dots$, which is the answer to question 1.  $(F_n)$ is called a {\bf \em Fibonacci sequence}\label{fibonacci}:
$$
\begin{array}{c||c|c|c|c|c|c|c|c} n & \quad 1 \quad & \quad 2 \quad & \quad 3 \quad & \quad 4 \quad & \quad 5 \quad & \quad 6 \quad & \quad \cdots \quad \\ \hline
F_n & 2 & 3 & 5 & 8 & 13 & 21 & \cdots \end{array}$$


\noindent Moreover, $E(X)=\frac 12$ and
\begin{eqnarray*}
    E(X|A_n) &=& 0\cdot P(X=0|A_n)+ 1\cdot P(X = 1 | A_n) = \frac{F_{n-2}}{F_n}.
\end{eqnarray*}
FYI: Here are the 13 sequences when $n = 5$.

those that start with $t$:
$$ththt, thtth, thttt, tthth, tthtt, tttht, tttth, ttttt$$
and those that start with $h$:
$$hthth, hthtt, httht, httth, htttt.$$

\newpage

\noindent {\bf Conditional expectation with a conditioning rv.}\\
A common situation of conditional expectation is when the conditioning event involves another random variable as follows:
\begin{eqnarray*}
    E(X | Y = y) &=& \sum_x x \cdot P(X = x | Y = y) \quad \text{when }X, Y\text{ jointly discrete;} \\
    &=& \int_{-\infty}^{\infty} x \cdot f_{X | Y} (x | y) \,dx  \quad \text{when }X, Y\text{ jointly continuous.}
\end{eqnarray*}
Notice in either case that $E(X | Y = y)$ is a function of $y: E(X | Y = y) = g(y)$.\\
However, if $X$ and $Y$ are independent, it is clear that for all $y$, $E(X | Y = y) = E(X)$. \vspace{0.25cm}\\
More generally, the idea here is that we are taking the expected value of $X$ with respect to the \textbf{\emph{conditional distribution}} of $X$ given $Y = y$.

\vspace{0.5cm}

\noindent \textbf{Example.} \\
Suppose $X, Y$ has joint pmf (marginals shown for convenience):
$$\begin{array}{c||c|c|c|c||c} & y=0 & y=1 & y=2 & y=3 & p_X(x)\\ \hline\hline
x=1 & .1 & .1 & .1 & 0 & .3\\ \hline
x=2 & 0 & .2 & .1 & .1 & .4\\ \hline
x=3 & .1 & .1 & 0 & .1 & .3\\ \hline\hline
p_Y(y) & .2 & .4 & .2 & .2 &   \end{array}$$
Compute $E(X|Y=y)$ for each $y$ and $E(Y|X=x)$ for each $x$.\\


\noindent SOLUTION:\\
I'll construct the conditional pmfs within the given table to illustrate.\\
From our table, to compute $p_{X|Y}(x|y)=P(X=x|Y=y)=\frac {p(x,y)}{p_Y(y)}$ we just take each entry and normalize by the {\em column} sum (i.e., the marginal of $Y$ at $y$):
$$\begin{array}{c||c|c|c|c|} & y=0 & y=1 & y=2 & y=3 \\ \hline\hline
x=1 & .1 \ \color{red}{\frac {.1}{.2}=\frac 12} & .1 \ \color{blue}{\frac {.1}{.4}=\frac 14} & .1 \ \color{green}{\frac {.1}{.2}=\frac 12} &  0 \ \color{purple}{\frac 0{.2}=0}        \\ \hline
x=2 &  0 \ \color{red}{\frac 0{.2}=0}& .2 \ \color{blue}{\frac {.2}{.4}=\frac 12} & .1 \ \color{green}{\frac {.1}{.2}=\frac 12} & .1 \ \color{purple}{\frac {.1}{.2}=\frac 12} \\ \hline
x=3 & .1 \ \color{red}{\frac {.1}{.2}=\frac 12} & .1 \ \color{blue}{\frac {.1}{.4}=\frac 14} &  0 \ \color{green}{\frac 0{.2}=0}& .1 \ \color{purple}{\frac {.1}{.2}=\frac 12} \\ \hline\hline
p_Y(y) & \color{red}{\bf .2}     & \color{blue}{\bf .4}        & \color{green}{\bf .2}        & \color{purple}{\bf .2}        \end{array}$$

\begin{eqnarray*}
    E(X | Y = 0) &=& 1 \cdot {\color{red}{\frac 12}} + 2 \cdot {\color{red}{0}}  + 3 \cdot {\color{red}{\frac 12}} = 2. \\
    E(X | Y = 1) &=& 1 \cdot {\color{blue}{\frac 14}} + 2 \cdot {\color{blue}{\frac 12}} + 3 \cdot {\color{blue}{\frac 14}} = 2. \\
    E(X | Y = 2) &=& 1 \cdot {\color{green}{\frac 12}} + 2 \cdot {\color{green}{\frac 12}} + 3 \cdot {\color{green}{0}} = 1.5. \\
    E(X | Y = 3) &=& 1 \cdot {\color{purple}{0}}  + 2 \cdot {\color{purple}{\frac 12}} + 3 \cdot {\color{purple}{\frac 12}} = 2.5.
\end{eqnarray*}

\newpage

\noindent The conditional pmf of $Y$ given $X=x$ is constructed in a similar manner, we just normalize each entry by the {\em row} sum (i.e., the marginal of $X$ at $x$):
$$\begin{array}{c||c|c|c|c||c} & y=0 & y=1 & y=2 & y=3 & p_X(x)\\ \hline\hline
x=1 & .1\ \color{red}{\frac {.1}{.3}=\frac 13} & .1\ \color{red}{\frac {.1}{.3}=\frac 13} & .1\ \color{red}{\frac {.1}{.3}=\frac 13} &  0\ \color{red}{\frac {0}{.3}=0} & \color{red}{\bf .3}   \\ \hline
x=2 &  0\ \color{blue}{\frac {0}{.4}=0} & .2\ \color{blue}{\frac {.2}{.4}=\frac 12} & .1\ \color{blue}{\frac {.1}{.4}=\frac 14} & .1\ \color{blue}{\frac {.1}{.4}=\frac 14} & \color{blue}{\bf .4}  \\ \hline
x=3 & .1\ \color{green}{\frac {.1}{.3}=\frac 13} & .1\ \color{green}{\frac {.1}{.3}=\frac 13} &  0\ \color{green}{\frac {0}{.3}=0} & .1\ \color{green}{\frac {.1}{.3}=\frac 13} & \color{green}{\bf .3} \\ \hline \end{array}$$



\begin{eqnarray*}
    E(Y | X = 1) &=& 0 \cdot {\color{red}{\frac{1}{3}}} + 1 \cdot {\color{red}{\frac{1}{3}}} + 2 \cdot {\color{red}{\frac{1}{3}}} + 3 \cdot {\color{red}{0}} = 1.\\
    E(Y | X = 2) &=& 0 \cdot {\color{blue}{0}} + 1 \cdot {\color{blue}{\frac{1}{2}}} + 2 \cdot {\color{blue}{\frac 14}} + 3 \cdot {\color{blue}{\frac{1}{4}}} = \frac{7}{4}.\\
    E(Y | X = 3) &=& 0 \cdot {\color{green}{\frac{1}{3}}} + 1 \cdot {\color{green}{\frac{1}{3}}} + 2 \cdot {\color{green}{0}} + 3 \cdot {\color{green}{\frac{1}{3}}} = \frac{4}{3}.
\end{eqnarray*}

\vskip .5 in

\noindent \textbf{Example.}\\
Suppose $X,Y$ are jointly continuous with joint pdf $\displaystyle f(x, y) = \left\{\begin{array}{ll}
        e^{-y} & 0 < x < y < \infty; \\
        0 & \text{elsewhere.}
    \end{array} \right.$.\\
Compute $E(Y|X=x)$.\\

\noindent SOLUTION:\\
Since $E(Y|X=x)$ is just the expectation of the conditional distribution of $Y$ given $X=x$, we compute the
conditional pdf of $Y$ given $X=x$.  When $x>0$ and $y>x$,
\begin{eqnarray*}
f_{Y | X}(y | x) &=& \frac {f(x,y)}{f_X(x)}\\
&=& \frac {e^{-y}}{\int_{x}^{\infty}e^{-y}\,dy} = \frac {e^{-y}}{e^{-x}}\\
&=& \left\{\begin{array}{cl}
        e^{-(y-x)} & \text{for }y > x; \\
        0 & \text{elsewhere.}
    \end{array} \right.
\end{eqnarray*}
This is called a {\bf\em delayed (unit) exponential}\label{delayedexp} (the delay, here, is $x$ and, thereafter $Y | X = x$ behaves as an exp(1) distribution).

Now,
\begin{eqnarray*}
E(Y|X=x) &=& \int_{-\infty}^{\infty}yf_{Y|X}(y|x)\,dy \\
&=& \int_x^{\infty} ye^{-(y-x)}\,dy \qquad \mbox{substitution }u=y-x,\ du=dy\\
&=& \int_0^{\infty} (u+x)e^{-u}\,du = \int_0^{\infty}\!\!\!\!\underbrace{\, ue^{-u}\, }_{\stackrel{\sim\ \ \ \ \ }{\mbox{\tiny Gamma(2,1)}}}\!\!du + x\int_0^{\infty}e^{-u}\,du\\
&=& 1+x.\end{eqnarray*}

\newpage

\noindent \textbf{Exercise for the student.}\\
Continue with the last example and compute $E(X|Y=y)$ where $y>0$. \\
Your answer should be $E(X|Y=y)=\frac y2$.\\

\vskip .5 in


\noindent \textbf{Conditional expectations can be thought of as random variables.} \\
We learned that the conditional expectation of $X$ given $Y = y$ is just a function of $y$, namely,
$$E(X | Y = y) = g(y).$$
Therefore, $E(X | Y)$ can be thought of as $g(Y)$, i.e., $E(X|Y)$ is the random variable that returns the value $g(y)$ when $Y=y$. \\


\noindent The following tells us that $X$ and $E(X | Y)$ have the same expected value.
This is an especially useful and important result in probability. It allows us to compute the (unconditional) expectation of a random variable by way of a conditional distribution whose expectation is computable.\\

\bigskip


\noindent {\bf Property 1: Law of total expectation.}\label{lawoftotalexpectation}\\
$$E[E(X | Y)] = E(X).$$

\bigskip

\noindent \textbf{Proof.} I'll illustrate the proof when $X$ and $Y$ are jointly discrete. We have
\begin{eqnarray*}
    E[E(X | Y)] &=& E[g(Y)] \\
    &=& \sum_y g(y) \cdot P(Y = y) \\
    &=& \sum_y \sum_x (x \cdot P(X = x | Y = y)) \cdot P(Y = y) \\
    &=& \sum_x \sum_y x \cdot P(X = x, Y = y) \\
    &=& \sum_x x \sum_y P(X = x, Y = y) \\
    &=& \sum_x x P(X = x) \\
    &=& E(X).
\end{eqnarray*}
$\hfill\Box$


\vskip .5 in



\noindent \textbf{Exercise for the student.} Prove this theorem when $X$ and $Y$ are jointly continuous.




\newpage




\noindent \textbf{Example.} \\
Suppose $X \sim \mbox{Gamma}(\alpha, \beta)$ and $Y | X \sim \mbox{Gamma}(X, X)$. Derive $E(Y)$.\\

\noindent SOLUTION:\\
One approach is to find the unconditional distribution of $Y$ and then use brute force -- i.e., $E(Y) = \int_{-\infty}^{\infty} y \cdot f_Y(y) dy$.
The joint pdf is $f(x,y) = \frac {x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\cdot \frac {y^{x-1}e^{-y/x}}{x^x\Gamma(x)}$ for $x>0,y>0$, and, for $y>0$,
$$f_Y(y) = \int_0^{\infty} \frac {x^{\alpha-1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}\cdot \frac {y^{x-1}e^{-y/x}}{x^x\Gamma(x)}\,dx,$$
which is not easily computable.  Therefore, it appears the brute force computation of $E(Y)$ is not the way to go.

However, in this example we can employ the law of total expectation:
\begin{eqnarray*}
    E(Y) &=& E[E(Y | X)].\quad \mbox{where }Y|X \sim \mbox{Gamma}(X,X) \\
    &=& E(X^2) \\
    &=& \text{Var}(X) + [E(X)]^2 \\
    &=& \alpha\beta^2 + (\alpha\beta)^2 \\
    &=& \alpha(\alpha + 1)\beta^2.
\end{eqnarray*}


\vskip .5 in

\noindent \textbf{Example.}\\
Your friend tosses a coin (with probability $p$ of coming up heads) $n$ times. Let $X$ denote the number of heads your friend obtains. Then she hands you the same coin, and you get to toss it $X$ times. Let $Y$ denote the number of heads that you obtain. Compute $E(Y)$.\\

\noindent SOLUTION:\\
According to the problem, we know $X \sim \text{binom}\left(n, p\right)$ and $Y | X \sim \text{binom}\left(X, p\right).$
Therefore, $E(Y | X) = Xp$ and
$$E(Y) = E[E(Y | X)] = E\left(Xp\right) = pE(X) = p\cdot np = np^2.$$

\vskip .5 in

\noindent We close this section by stating (and proving) several other important {\bf\em properties that conditional expectations}\label{propertiesofcondexp} enjoy.\\

\vspace{0.75cm}

\noindent \textbf{Property 2.}\\
Let $X$ and $Y$ be jointly distributed r.v.s. and h be any functions, then,
$$E[Xh(Y)|Y] = h(Y)E(X|Y).$$
This says functions of the given rv can be treated as scalars and, thus, can be factored out.

\newpage

\noindent \textbf{Proof.} (only presented when $X$ and $Y$ are jointly discrete)\\
Let $y$ be a value of $Y$.\\
\begin{align*}
E[Xh(Y)|Y=y] &= \sum_x xh(y)P[X=x|Y=y]\\
&= h(y) \sum_x xP[X=x|Y=y]\\
&= h(y)E[X|Y=y]
\end{align*}
Therefore,
$$E(Xh(Y)|Y) = h(Y)E(X|Y).$$
$\hfill\Box$

\vspace{0.75cm}

\noindent \textbf{Property 3}\\
Let $U,V,W$ be any jointly distributed r.v.s. and let $a$ and $b$ be any constants, then
$$E(aU+bV|W)=aE(U|W)+bE(V|W).$$
This says conditional expectation is a linear operation.


\vspace{0.75cm}

\noindent \textbf{Property 4}\\
If $X$ and $Y$ are independent, then $E(X|Y) = E(X).$\\

\noindent Property 4 should be immediate from the fact that the conditional distribution must equal the unconditional distribution by independence.


\vspace{0.75cm}


\noindent \textbf{Property 5}\\
If $c$ is a constant, then $E(c|Y) = c$.



\newpage






\noindent {\bf Conditional variance.}\label{conditionalvariance}\\

\noindent We define the conditional variance as follows:
$$Var(X|Y=y) = E[(X-E(X|Y=y))^2|Y=y].$$
Also,
$$Var(X|Y) = E[(X-E(X|Y)^2|Y],$$
where we think of $Var(X|Y)$ as a random variable.

\vspace{0.5cm}

\noindent \textbf{Remark.} \\
$Var(X|Y)$ is the variance of the conditional distribution of $X$ given $Y$.

\vspace{0.5cm}

\noindent \textbf{Example.} \\
Suppose $X, Y$ are jointly continuous with joint pdf
$$f(x,y) = xe^{-x(1+y)}  \text{     for } x > 0, y > 0.$$
Compute $Var(X|Y)$.\\

\noindent SOLUTION:  We first find the conditional density of $X$ given $Y=y$. Fix $y>0$.
$$f_Y(y) = \int_0^{\infty}xe^{-x(1+y)}\,dx = (1+y)^{-2}\Gamma(2) = \frac {1}{(1+y)^2},$$
therefore, $f_{X|Y}(x|y) = \frac {f(x,y)}{f_Y(y)}=(1+y)^2xe^{-x(1+y)},$ i.e., $X|Y=y\sim \mbox{Gamma}(2,\frac {1}{1+y})$.
Consequently, $Var(X|Y) = \frac {2}{(1+y)^2}$.\\

\vskip .5 in

\noindent {\bf Law of total variance.}\label{lawoftotalvariance}\\
$$Var(X) = E(Var(X|Y)) + Var(E(X|Y)).$$

\bigskip

\noindent \textbf{Proof.}
\begin{eqnarray*}
Var(X) &=& E(\{X-E(X)\}^2)\\
&=& E(\{X-E(X|Y)+E(X|Y)-E(X)\}^2)\\
&=& \underbrace{E(\{X-E(X|Y)\}^2)}_\text{(1)} \\
& & \qquad + \underbrace{E(\{E(X|Y)-E(X)\}^2)}_\text{(2)}\\
& & \qquad\qquad + \underbrace{E(\{X-E(X|Y)\}\{E(X|Y)-E(X)\})}_\text{(3)}\\
&=& E(Var(X|Y)) + Var(E(X|Y)) + 0,
\end{eqnarray*}



\noindent where\\
\vspace{0.05cm}
(1): $E(\{X-E(X|Y)\}^2) = E[E(\{X-E(X|Y)\}|Y]=E[Var(X|Y)]$;\\
\vspace{0.05cm}
(2): Since $E(X) = E[E(X|Y)]$,
$$E(\{E(X|Y)-E(X)\}^2) = E(\{E(X|Y)-E[E(X|Y)]\}^2 = Var(E(X|Y));$$\\
\vspace{0.05cm}
(3):
\begin{eqnarray*}
E(\{X-E(X|Y)\}\{E(X|Y) - E(X)\}) &=& E(E(\{X-E(X|Y)\}\{E(X|Y)-E(X)\}|Y)\\
&=& E[\{E(X|Y)-E(X)\}E(X-E(X|Y)|Y)]\\
&=& 0.
\end{eqnarray*}

\vspace{0.5cm}
\noindent \textbf{Two exercises.} \\
(1) Show that the constant $c$ that minimizes $E[(X-c)^2]$ is $c=E(X)$\\
(2) Show that the function $g(Y)$ that minimizes $E[(X-g(Y))^2]$ is $g(Y) = E(X|Y)$.\\
Hint: start by subtracting and adding $E(X|Y)$ under the square. Use ideas in the proof above.

\vspace{0.5cm}
\noindent \textbf{Example.} \\
A miner is trapped in a room with 3 doors.
\begin{itemize}
\item Door 1 leads the miner to freedom in 3 hours.
\item Door 2 leads the miner back to the room after 5 hours.
\item Door 3 leads the miner back to the room after 7 hours.
\end{itemize}
The miner is equally likely to choose any of the 3 doors at each trial.\\
Compute the expected time until the miner is free and variance of the time.
\vspace{0.25cm}

\noindent SOLUTION:
If $D=$ door chosen, then $P(D=1)=P(D=2)=P(D=3)=\frac{1}{3}$.\\
Also, if $X$ is the time to freedom, then
\begin{eqnarray*}
E(X|D=1) &=& 3\\
E(X|D=2) &=& 5+E(X)\\
E(X|D=3) &=& 7+E(X).
\end{eqnarray*}
Therefore, by the law of total expectation,
\begin{eqnarray*}
E(X) &=& E[E(X|D)]\\
&=& E(X|D=1)P(D=1) + E(X|D=2)P(D=2) + E(X|D=3)P(D=3) \\
&=& 3 \cdot \frac{1}{3} + (5+E(X))\cdot \frac{1}{3} + (7+E(X)) \cdot \frac{1}{3}\\
&=& 1 + \frac{5}{3} + \frac{7}{3} + \frac{2}{3}E(X).\\
\end{eqnarray*}
Hence, $\frac{1}{3}E(X) = 5$, meaning $E(X) = 15$ hours.\\

\noindent For $Var(X)$, we first compute $E(X^2)$
\begin{eqnarray*}
E(X^2) &=& E(X^2|D=1)\cdot \frac{1}{3} + E(X^2|D=2)\cdot \frac{1}{3} + E(X^2|D=2)\cdot \frac{1}{3} \\
&=& 9\cdot\frac{1}{3} + E[(5+X)^2] \cdot \frac{1}{3} + E[(7+X)^2] \cdot \frac{1}{3} \\
&=& 3 + (25+10E(X) +E(X^2))\cdot \frac{1}{3} + (49+14E(X) + E(X^2))\cdot \frac{1}{3} \\
&=& 3 + \frac{25}{3} + \frac{49}{3} + (\frac{10}{3} + \frac{14}{3})E(X) + \frac{2}{3}E(X^2).
\end{eqnarray*}
Therefore, $\frac{1}{3}E(X^2) = \frac{497}{3}\implies E(X^2) = 497$.
Thus,
$$Var(X) = 497-(15)^2=272.$$

\vskip .5 in

\noindent \textbf{Example.} ({\bf\em Compound random variables})\label{compoundrv}\\
In a given year, the number $N$ of claims is a random variable having a mean $\mu_N$ and variance $\sigma_N^2$.
Let $X_i$ be the size of the $i$th claim in the year, so that
$$S_N = \sum_{i=1}^{N} X_i = X_1+X_2+\cdots + X_N$$
represents the total loss incurred in the year. Moreover, we assume

$X_1, X_2, X_3,\cdots$ \text{are iid with mean } $\mu_X$ \text{ and variance } $\sigma_X^2$

$X_i$'s are independent of $N$, and

$N$ \text{ has mean } $\mu_N$ \text{ and variance } $\sigma_N^2$.

\noindent Compute the mean and variance of the total loss incurred in the year, i.e., compute $E(S_N)$ and $Var(S_N)$.\\

\noindent SOLUTION:\\
Notice $S_N$ is a sum of a random number of iid rvs.
\begin{eqnarray*}
E(S_N)=E[\sum_{i=1}^{N}X_i] &=& E\Big[ E\big(\sum_{i=1}^N X_i|N\big)\Big] \hspace{0.75cm} \text{by the law of total expectation}\\
&\stackrel{(*)}{=} & E[N\mu_X]\\
&=& \mu_XE(N)\\
&=& \mu_X\mu_N\\
\end{eqnarray*}
$\stackrel{(*)}{}$ Notice that
$$E\big(\sum_{i=1}^N X_i | N=n\big) = E\big(\sum_{i=1}^n X_i | N=n\big) = E\big(\sum_{i=1}^n X_i\big)  = \sum_{i=1}^n E(X_i) = n\mu_X = g(n),$$
where the second equality follows by property 4 of conditional expectation. Therefore,
$E\big(\sum_{i=1}^N X_i|N\big) = g(N)=N\mu_X$.\\

\noindent As for the variance,
\begin{eqnarray*}
Var(S_N) = Var\Big(\sum_{i=1}^N X_i\Big) &=& E\Big[\big(\sum_{i=1}^N X_i\big)^2\Big] - \mu_X^2\mu_N^2\\
&\stackrel{(1)}{=} & E\Big(\sum_{i=1}^N \sum_{j=1}^N X_i X_j\Big) - \mu_X^2 \mu_N^2\\
&\stackrel{(2)}{=} & E(N\sigma_X^2 + N^2\mu_X^2) - \mu_X^2 \mu_N^2\\
&=& \mu_N\sigma_X^2 + (\sigma_N^2+\mu_N^2)\mu_X^2 + \mu_X^2 \mu_N^2\\
&=& \mu_N \sigma_X^2 + \sigma_N^2\mu_X^2,\\
\end{eqnarray*}
where we used the following facts:\\


\noindent (1):
$$(\sum_{i=1}^N X_i)^2 = (\sum_{i=1}^N X_i)(\sum_{j=1}^N X_j) = \sum_{i=1}^N \sum_{j=1}^N X_iX_j,$$
and\\

\noindent (2): when $N=n$,
\begin{eqnarray*}
E[\sum_{i=1}^N \sum_{j=1}^N X_iX_j | N=n] &=& E[(\sum_{i=1}^n \sum_{j=1}^n X_iX_j | N=n]\\
&=& E[\sum_{i=1}^n \sum_{j=1}^n X_iX_j]\quad \mbox{(since $X_i$'s are independent of $N$)}\\
&=& E[\sum_{i=1}^n X_i^2 + \mathop{\sum_{i=1}^{n}\sum_{j=1}^{n}}_{i \neq j} X_i X_j]\\
&=& \sum_{i=1}^n E(X_i^2) + \mathop{\sum_{i=1}^{n}\sum_{j=1}^{n}}_{i \neq j} E(X_i)E(X_j)\\
&=& n (\sigma_X^2 + \mu_X^2) + n(n-1) \mu_X^2 \\
&=& n\sigma_X^2 + n^2 \mu_X^2=g(n),\\
\end{eqnarray*}
and, therefore,
$E[\sum_{i=1}^N \sum_{j=1}^N X_iX_j | N] = g(N) = N\sigma_X^2 + N^2\mu_X^2.$\\



\noindent {\bf Remark.}\\
We just showed $Var(S_N) = \mu_N \sigma_X^2 + \sigma_N^2\mu_X^2$.
The reader should take notice of how large this variance is compared to $E(N)Var(X_1)=\mu_N\sigma_X^2$. The variability in the
number of terms in the random sum is contributing a huge amount to the overall variability in the total loss.\\


\vskip .25 in



\noindent {\bf The bivariate Normal distribution.}\label{bivariatenormaldist}\\
\noindent We consider a bivariate distribution which is {\em jointly} normally distributed. To motivate let's first suppose that
$X_1\sim \mbox{N}(\mu_1,\sigma_1^2)$ and $X_2\sim \mbox{N}(\mu_2,\sigma_2^2)$ are {\em independent}.  If $Z_1$ and $Z_2$ are independent
standard normal rvs, it follows from the corollary on page \pageref{normalcor} that
\begin{equation}\begin{array}{l}
X_1=\mu_1 + \sigma_1Z_1\\\label{eq:2indepnormals}
X_2=\mu_2+\sigma_2Z_2 \end{array}
\end{equation}
and, in fact, the joint pdf is just the product of the marginals of $X_1$ and $X_2$:
$$f_{X_1,X_2}(x_1,x_2) = \frac {e^{-\frac 12\left( \left(\frac{x_1-\mu_1}{\sigma_1^2}\right)^2 + \left(\frac{x_2-\mu_2}{\sigma_2^2}\right)^2 \right)}}{2\pi\sigma_1\sigma_2}.$$

\noindent Now, let $\rho$ be a constant, $-1\le \rho \le 1$, and consider the (linear) transformation
\begin{equation}\begin{array}{l}
X_1  =  \mu_1 + \sigma_1Z_1\\\label{bivariatenormaltrans}
X_2  =  \mu_2 + \sigma_2\rho Z_1 + \sigma_2\sqrt{1-\rho^2}Z_2\end{array}
= \left[ \begin{array}{c} \mu_1\\\mu_2 \end{array}\right] + \left[ \begin{array}{cc} \sigma_1 & 0 \\ \sigma_2\rho & \sigma_2\sqrt{1-\rho^2}\end{array}\right]\left[ \begin{array}{c} Z_1\\Z_2 \end{array} \right].
\end{equation}
When $\rho=0$, (\ref{bivariatenormaltrans}) reduces to (\ref{eq:2indepnormals}). Moreover, when $\rho$ is {\em strictly} between $-1$ and $1$,
(\ref{bivariatenormaltrans}) is one-to-one and the method of Jacobians will show that the joint pdf of $X_1$ and $X_2$ is
\begin{equation*}\label{bivariatenormalpdf}
    f(x_1, x_2) = \frac{e^{-\frac{1}{2(1-\rho^2)} \cdot \left\{\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2 - 2 \rho \left(\frac{x_1 - \mu_1}{\sigma_1}\right) \! \left(\frac{x_2 - \mu_2}{\sigma_2}\right) + \left(\frac{x_2 - \mu_2}{\sigma_2}\right)^2\right\}}}{2\pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}},
\end{equation*}
\noindent which is the {\bf\em bivariate normal pdf} with parameters $\mu_1, \mu_2, \sigma_1, \sigma_2,$ and $\rho$.\\

\noindent {\bf Fact:}\\
\noindent Let's show that parameters of the bivariate normal have the following meanings:
$$\mu_1=E(X_1),\ \mu_2=E(X_2),\ \sigma_1^2=Var(X_1),\ \sigma_2^2=Var(X_2),\ \mbox{and }\rho=\mbox{corr}(X_1,X_2).$$
From (\ref{bivariatenormaltrans}) and the corollary on page \pageref{normalcor}, $\mu_1=E(X_1)$ and $\sigma_1^2=Var(X_1)$.  Also, by taking the expected value in the second equation it follows that $\mu_2=E(X_2)$, and by taking variance in the first second equation and noting that $Z_1$ and $Z_2$ are independent,
$Var(X_2) = \sigma_2^2\rho^2Var(Z_1) + \sigma_2^2(1-\rho^2)Var(Z_2) = \sigma_2^2.$
Finally, using the properties of covariance
\begin{eqnarray*}
cov(X_1,X_2) & = & cov(\mu_1 + \sigma_1Z_1,\mu_2 + \sigma_2\rho Z_1 + \sigma_2\sqrt{1-\rho^2}Z_2)\\
&=& cov(\sigma_1Z_1,\sigma_2\rho Z_1 + \sigma_2\sqrt{1-\rho^2}Z_2)\\
&=& \sigma_1\sigma_2\rho \underbrace{cov(Z_1,Z_1)}_{\tiny =\,Var(Z_1)=1} + \sigma_1\sigma_2\sqrt{1-\rho^2} \underbrace{cov(Z_1,Z_2)}_{\tiny =\,0}\\
&=&\sigma_1\sigma_2\rho\\
\end{eqnarray*}
from which it follows $\mbox{corr}(X_1,X_2)=\frac{cov(X_1,X_2)}{\sigma_1\sigma_2}=\rho$.$\hfill\Box$\\



\newpage



\noindent \textbf{Remark.} \\
As mentioned already, if $\rho = 0$, i.e. if $X_1$ and $X_2$ are uncorrelated, then the joint pdf becomes
\begin{equation*}
    f(x_1, x_2) = \frac{e^{-\frac{1}{2(1-\rho^2)} \cdot \left\{\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2 - 2 \rho\left(\frac{x_1 - \mu_1}{\sigma_1}\right) \! \left(\frac{x_2 - \mu_2}{\sigma_2}\right) + \left(\frac{x_2 - \mu_2}{\sigma_2}\right)^2\right\}}}{2\pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}}
\stackrel{\rho=0}{=}\frac{e^{-\frac{1}{2}\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2}}{\sigma_1\sqrt{2\pi}} \cdot \frac{e^{-\frac{1}{2}\left(\frac{x_2 - \mu_2}{\sigma_2}\right)^2}}{\sigma_2 \sqrt{2\pi}},
\end{equation*}
and the joint pdf is the product of its marginal pdf. Thus,\\
\begin{center}
{\em \textbf{Jointly distributed normals} that are \textbf{uncorrelated} are \textbf{independent}!}
\end{center}

\vskip .6 in

\noindent \textbf{Remark.}\\
When $|\rho|=1$, then the transformation (\ref{bivariatenormaltrans}) is {\em not} one-to-one. In fact, when, say, $\rho=+1$, (\ref{bivariatenormaltrans}) reduces to
$$\begin{array}{l}
X_1 = \mu_1+\sigma_1Z_1\\
X_2=\mu_2+\sigma_2Z_1\end{array} \Longrightarrow
\begin{array}{l}
X_1 = \mu_1+\sigma_1Z_1\\
X_2=\mu_2+\sigma_2\left(\frac {X_1-\mu_1}{\sigma_1} \right) = \mu_2 - \frac {\sigma_2}{\sigma_1}\mu_1 + \frac {\sigma_2}{\sigma_1}X_1,\end{array}$$
and we see that $X_2$ is just a linear function of $X_1$ with positive slope.  Similarly, at the other extreme, when $\rho=-1$, we can show
$X_2 = \mu_2 + \frac {\sigma_2}{\sigma_1}\mu_1 - \frac {\sigma_2}{\sigma_1}X_1$.  Therefore, when $|\rho|=1$ we have that one of these variables is a direct
linear function of the other and, in these cases, the support of $(X_1,X_2)$ will be a linear submanifold of ${\mathbb R}^2$ of dimension strictly less than 2;
therefore, for this reason, we cannot expect there to exist a joint pdf.\\


\vskip .6 in

\noindent Transformations like (\ref{bivariatenormaltrans}) can often be exploited to simplify computations involving bivariate normals.\\

\noindent\textbf{Example.}\\
Suppose $(X_1,X_2)$ is bivariate normal with parameters $\mu_1,\mu_2,\sigma_1,\sigma_2,\rho$. \\
Derive the conditional distribution of $X_2$ given $X_1$. \\
\emph{Note:} if we try to write down
$$f_{X_2 | X_1}(x_2 | x_1) = \frac{f(x_1, x_2)}{f_{X_1}(x_1)},$$
then it is tedious to recognize what the mean and variance are for the underlying normal even though recognizing it will be normal is not difficult.\\


\newpage


\noindent SOLUTION:\\
Using the transformation (\ref{bivariatenormaltrans}) we see $X_1$ is {\em independent} of $Z_2$ since $X_1$ is only a function of $Z_1$ (which is independent of $Z_2$). Therefore,
\begin{eqnarray*}
    X_2 &=& \mu_2 + \sigma_1 \rho Z_1 + \sigma_2 \sqrt{1 - \rho^2}  Z_2 \\
    &=& \underbrace{\mu_2 + \sigma_2 \rho \left(\frac{X_1 - \mu_1}{\sigma_1}\right)}_{\text{given }X_1\text{, this term is constant}} + \sigma_2 \sqrt{1 - \rho^2} Z_2.
\end{eqnarray*}
Therefore,
\begin{align*}
    X_2 | X_1 \sim \text{N}\!\!\left(\mu_2 + \sigma_2 \rho \left(\frac{X_1 - \mu_1}{\sigma_1}\right), \sigma_2^2 (1 - \rho^2)\right).
\end{align*}
Note that the first parameter is $E(X_2 | X_1)$, and the second parameter is $Var(X_2 | X_1)$.\\

\vskip .6 in

\noindent \textbf{Remark.} (non-uniqueness of the linear transformation defining the bivariate normal)\\
There happens to be {\em many} linear transformations of $Z_1$ and $Z_2$ that lead to the {\em same} bivariate normal distribution.
For example,
\begin{equation}
\begin{array}{l}
X_1 = \mu_1 + \sigma_1\sqrt{1-\rho^2}Z_1 + \sigma_1\rho Z_2\\
X_2 = \mu_2 \hspace{1.14 in} + \ \,\sigma_2Z_2
\end{array}\label{bivariatenormaltrans2}
= \left[ \begin{array}{c} \mu_1\\\mu_2 \end{array}\right] + \left[ \begin{array}{cc} \sigma_1\sqrt{1-\rho^2} & \sigma_1\rho \\ 0 & \sigma_2 \end{array}\right]\left[ \begin{array}{c} Z_1\\Z_2 \end{array} \right]
\end{equation}
is another linear transformation of $Z_1$ and $Z_2$ that leads to the same bivariate normal, i.e., having the exact same parameters.  Please check that with this transformation
$\mu_i=E(X_1),\ \sigma_i^2=Var(X_i)$ and $\rho = \mbox{corr}(X_1,X_2)$.\\

\vskip .5 in

\noindent Alternate linear transformations are useful as hopefully the next exercise illustrates.\\

\noindent\textbf{Exercise for the student.}\\
Suppose $(X_1,X_2)$ is bivariate normal with parameters $\mu_1,\mu_2,\sigma_1,\sigma_2,\rho$. \\
Derive the conditional distribution of $X_1$ given $X_2$, and use this to determine $E(X_1 | X_2)$ and $Var(X_1 | X_2).$\\

\noindent {\em Note about this exercise}:\\
Using the transformation (\ref{bivariatenormaltrans}) would not be ideal because doing so you'd have $Z_1 = \frac {X_2 - \mu_2 - \sigma_2\sqrt{1-\rho^2}Z_2}{\sigma_2\rho}$
and once given information about $X_2$ we'd have information not only about $Z_1$ but {\em also} $Z_2$, in particular, $X_2$ is {\em not} independent of $Z_2$. This is not like in the last example, where $X_1$ would not influence $Z_2$.  Nevertheless, using the transformation (\ref{bivariatenormaltrans2}) we will not run into this problem.
The student should show that
$$X_1|X_2\sim \mbox{N}\left(\mu_1+\sigma_1\rho\left(\frac{X_2-\mu_2}{\sigma_2}\right), \sigma_1^2(1-\rho^2)\right),$$
and the parameters listed here, respectively, are $E(X_1|X_2)$ and $Var(X_1|X_2)$.\\



\newpage

\noindent We end this section some pictures of the bivariate normal distribution.\\

\vskip .5 in

\includegraphics*[0,0][400,350]{bivariatenormalpic1.jpg}

\includegraphics*[10,0][500,150]{bivariatenormalpic2.jpg}


\newpage

\noindent {\bf The multivariate Normal distribution.*}\label{multivariatenormaldist}\\

\noindent {\em This section requires the reader knows some basic linear algebra}.\\

\noindent Suppose $k\ge 2$ is a fixed integer. In a way similar to how we defined the bivariate normal as a linear transformation of iid standard normals,
we now define what it means for the vector $(X_1,X_2,\dots,X_k)^T$ to have a $k$-variate normal distribution.

We say
$$\mathbf{X}:=\left[\begin{array}{c}X_1\\ X_2\\ \vdots \\ X_k \end{array}\right]$$
has a {\bf $k$-variate}, or, simply {\bf multivariate normal distribution}\label{kvariatenormaldef} with parameters $\vec{\mu}$ and
$\mathbf{\Sigma}$, abbreviated $\mathbf{X}\sim {\bf N}_k(\vec{\mu},\mathbf{\Sigma})$,
provided, for some integer $\ell\ge 1$,
\begin{equation}\label{kvariatetransmatrixform}\mathbf{X} = \vec{\mu}+\mathbf{AZ},\end{equation}
where
$$\mathbf{Z}:=\left[\begin{array}{c}Z_1\\ Z_2\\ \vdots \\ Z_{\ell} \end{array}\right],\quad Z_1,Z_2,\dots,Z_{\ell}\sim \mbox{iid N}(0,1),\qquad
\vec{\mu}:=\left[\begin{array}{c}\mu_1\\ \mu_2\\ \vdots \\ \mu_k \end{array}\right]\in {\mathbb R}^k$$
and $\mathbf{A}$ is a $k\times \ell$ real matrix with the property that
$$\mathbf{AA}^T = \mathbf{\Sigma}.$$

\vskip .5 in

\noindent {\bf Example.} The bivariate normal is a $2$-variate normal distribution.\label{bivariatenormal=2variate}\\
Verify these facts for the case $k=2$, i.e., the bivariate normal.\\

\noindent SOLUTION:  From equation (\ref{bivariatenormaltrans}) on page \pageref{bivariatenormaltrans} we see that $\mathbf{X}=\vec{\mu}+\mathbf{AZ}$, where
$$\vec{\mu}=\left[ \begin{array}{c} \mu_1\\ \mu_2 \end{array}\right],\quad \mathbf{A} = \left[ \begin{array}{cc} \sigma_1 & 0 \\ \sigma_2\rho & \sigma_2\sqrt{1-\rho^2} \end{array}\right], \quad\mbox{and}\quad \mathbf{Z} = \left[ \begin{array}{c} Z_1\\ Z_2 \end{array}\right]$$
with $Z_1,Z_2\sim \mbox{iid N}(0,1)$.
Moreover,
$$\mathbf{AA}^T =
\left[
\begin{array}{cc} \sigma_1 & 0 \\ \sigma_2\rho & \sigma_2\sqrt{1-\rho^2} \end{array}
\right]
\left[
\begin{array}{cc} \sigma_1 & \sigma_2\rho \\ 0 & \sigma_2\sqrt{1-\rho^2} \end{array}
\right] =
\left[
\begin{array}{cc} \sigma_1^2 & \rho\sigma_1\sigma_2 \\ \rho\sigma_1\sigma_2 & \sigma_2^2 \end{array}
\right],$$
whose entries are exactly the covariances as required.  Note that if $|\rho|=1$, then $\mathbf{AA}^T=\mathbf{\Sigma}$ is singular and, therefore, not positive definite, so no joint pdf of $X_1,X_2$ will exist in this case.$\hfill\Box$


\newpage

\noindent {\bf Remark} ({\bf The parameters are $\vec{\mu}=E(\mathbf{X})$ and $\mathbf{\Sigma}$ is the matrix of covariances}).\\
\noindent If we write
$$\mathbf{A} = \left[ \begin{array}{cccc} a_{11} & a_{12} & \cdots & a_{1\ell} \\ a_{21} & a_{22} & \cdots & a_{2\ell} \\
\vdots & \vdots & \ddots & \vdots \\
a_{k1} & a_{k2} & \cdots & a_{k\ell} \\ \end{array} \right],$$

\bigskip
\noindent then, working out the matrix multiplication, (\ref{kvariatetransmatrixform}) can be alternately written as

\begin{eqnarray*}
X_1 & = & \mu_1 + a_{11}Z_1 + a_{12}Z_2 + \cdots + a_{1\ell}Z_{\ell}\\
X_2 & = & \mu_2 + a_{21}Z_1 + a_{22}Z_2 + \cdots + a_{2\ell}Z_{\ell}\\
 & \vdots & \\
X_k & = & \mu_k + a_{k1}Z_1 + a_{k2}Z_2 + \cdots + a_{k\ell}Z_{\ell}.\\
\end{eqnarray*}
As each $X_i$ is an affine transformation of independent normals, each $X_i$ is normal; in fact, for each $i$,
$$X_i\sim \mbox{N}(\mu_i, \sum_{m=1}^{\ell}a_{im}^2).$$

\noindent Therefore, $\vec{\mu}$ represents the vector of means or, simply, the {\bf\em mean vector}\label{meanvector}.\\


\noindent More generally,
\begin{eqnarray*}
Cov(X_i,X_j) &=& E[(X_i-\mu_i)(X_j-\mu_j)]\\
&=& E\left[ \sum_{m=1}^{\ell}a_{im}Z_m \sum_{n=1}^{\ell}a_{jn}Z_n\right]\\
&=& \sum_{m=1}^{\ell}\sum_{n=1}^{\ell}a_{im}a_{jn}E\left[Z_m Z_n\right]\\
&=& \underbrace{\sum_{m=1}^{\ell}\sum_{n=1}^{\ell}}_{m=n}a_{im}a_{jm}\underbrace{E\left[Z_m^2 \right]}_{=\,1} \quad + \quad \underbrace{\sum_{m=1}^{\ell}\sum_{n=1}^{\ell}}_{m\ne n}a_{im}a_{jn}\underbrace{E\left[Z_m Z_n\right]}_{=\,0}\\
&=& \sum_{m=1}^{\ell}a_{im}a_{jm}  = (\mathbf{AA}^T)_{ij}.\\
\end{eqnarray*}

\noindent Therefore, since the $(i,j)^{\mbox{\small th}}$ entry of $\mathbf{AA}^T$ is $Cov(X_i,X_j)$, it follows that $\mathbf{AA}^T=\mathbf{\Sigma}$ represents the {\bf \em covariance matrix}\label{covariancematrix} (or variance-covariance matrix).


\newpage

\noindent The previous calculations could have been done succinctly using (\ref{kvariatetransmatrixform}):
$$E(\mathbf{X}) = E(\vec{\mu}+\mathbf{AZ}) = \vec{\mu} + \mathbf{A}\underbrace{E(\mathbf{Z})}_{=\,{\bf 0}} = \vec{\mu}$$
and
\begin{eqnarray*}
E[(\mathbf{X}-\vec{\mu})(\mathbf{X}-\vec{\mu})^T] &=& E[\mathbf{AZ}(\mathbf{AZ})^T]\\
&=& E[\mathbf{AZ}\mathbf{Z}^T\mathbf{A}^T]\\
&=& \mathbf{A}\underbrace{E[\mathbf{ZZ}^T]}_{=\, I_k}\mathbf{A}^T = \mathbf{AA}^T,\\
\end{eqnarray*}
where $I_k$ is the $k\times k$ identity matrix since the $Z_i$'s are iid N(0,1).$\hfill\Box$

\vskip .5 in

\noindent {\bf The $k$-variate normal density.}\label{kvariatenormalpdf}\\
If $\mathbf{\Sigma}$ is {\bf\em positive definite}, then $\mathbf{X}$ has a joint pdf given as
\begin{equation*}f(\mathbf{x}) = \dfrac {e^{-\frac 12 (\mathbf{x}-\vec{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\vec{\mu})}}{(2\pi)^{\frac k2}\mbox{det}(\mathbf{\Sigma})^{\frac 12}}\quad \mbox{for }\mathbf{x}\in {\mathbb R}^k.\end{equation*}

\vskip .2 in

\noindent This follows by method of Jacobians: for $\mathbf{z}\in {\mathbb R}^k$,
$$f_{\mathbf{Z}}(\mathbf{z}) = \dfrac {e^{-\frac 12 \mathbf{z}^T\mathbf{z}}}{(2\pi)^{\frac k2}}.$$
Since $\mathbf{\Sigma}$ is positive definite I claim that $\mathbf{A}$ can be taken to be $k\times k$ with $\mbox{det}(\mathbf{A})>0$ and, thus, invertible: ({\bf\em insert linear algebra result here}).\\

Now, $\mathbf{x}=\vec{\mu} + \mathbf{Az} \implies \mathbf{z} = \mathbf{A}^{-1}(\mathbf{x}-\vec{\mu})$. Moreover,
$$|J| = \left| \mbox{det}\left( \frac {\partial \mathbf{z}}{\partial \mathbf{x}}\right)\right|=\left| \mbox{det}(\mathbf{A}^{-1})\right| = \frac {1}{\mbox{det}(\mathbf{A})}.$$
In fact, since $\mbox{det}(\mathbf{\Sigma})=\mbox{det}(\mathbf{AA}^T)=\mbox{det}(\mathbf{A})\mbox{det}(\mathbf{A}^T)=\mbox{det}(\mathbf{A})^2$, it follows
$$\mbox{det}(\mathbf{A})=\mbox{det}(\mathbf{\Sigma})^{\frac 12}.$$
Therefore,
\begin{eqnarray*}
f_{\mathbf{X}}(\mathbf{x}) &=& f_{\mathbf{Z}}(\mathbf{z})\cdot |J| = \\
&=& f_{\mathbf{Z}}(\mathbf{A}^{-1}(\mathbf{x}-\vec{\mu}))\cdot \frac 1{\mbox{det}(\mathbf{\Sigma})^{\frac 12}}\\
&=&
\dfrac {e^{-\frac 12 (\mathbf{x}-\vec{\mu})^T(\mathbf{A}^{-1})^T\mathbf{A}^{-1}(\mathbf{x}-\vec{\mu})}}{(2\pi)^{\frac k2}\mbox{det}(\mathbf{\Sigma})^{\frac 12}}
=\dfrac {e^{-\frac 12 (\mathbf{x}-\vec{\mu})^T(\mathbf{AA}^T)^{-1}(\mathbf{x}-\vec{\mu})}}{(2\pi)^{\frac k2}\mbox{det}(\mathbf{\Sigma})^{\frac 12}}\\
&=&\dfrac {e^{-\frac 12 (\mathbf{x}-\vec{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\vec{\mu})}}{(2\pi)^{\frac k2}\mbox{det}(\mathbf{\Sigma})^{\frac 12}}.
\end{eqnarray*} $\hfill\Box$

\newpage


\begin{center}{\bf \Large VII. Inequalities and limit theorems.}\end{center}


\newpage

\noindent Inequalities play a special role in probability.  We start with two useful inequalities.\\

\noindent {\bf The Markov inequality.}\label{markovinequality}\\
\noindent Let $X$ be a non-negative random variable. Then, for any constant $a > 0$,
$$P(X \geq a) \leq \frac{E(X)}{a}.$$

\vskip .2 in

\noindent\textbf{Proof.} We assume the random variable is continuous with pdf $f(x)$.
\begin{eqnarray*}
    E(X) = \int_0^{\infty} x f(x) \,dx &=& \underbrace{\int_0^a x f(x) \,dx}_{\text{this term} \geq 0} + \int_a^{\infty} x f(x) \,dx \\
    &\geq & 0 + \int_a^{\infty} x f(x) \,dx \\
    &\stackrel{(*)}{\geq} & \int_a^{\infty} a f(x) \,dx \\
    & = & a \int_a^{\infty} f(x) \,dx  = a \cdot P(X \geq a).
\end{eqnarray*}
Therefore, we've shown $a \cdot P(X \geq a) \leq E(X)$ and, consequently, $P(X \geq a) \leq \frac{E(X)}{a}.$
We point out that in
$\stackrel{(*)}{}$ we used the fact that $xf(x) \geq af(x)$ for all $x\ge a$. $\hfill\Box$\\



\vskip .2 in

\noindent\textbf{Remark.}\\
Since $X \geq 0$, $E(X)$ is either finite or infinitely positive ($E(X) = \infty$). Since $E(X) = \infty$ makes the inequality trivial, we usually restrict the use of this inequality to r.v.s with $E(X) < \infty$. In this case, the Markov inequality roughly says: if a nonnegative random variable has a small mean, then the probability it takes large values must also be small.\\

\vskip .2 in

\noindent\textbf{Remark.}\\
The upper bound provided by Markov's inequality can be quite ``loose".\\

\vskip .2 in

\noindent\textbf{Example.}\\
Suppose $X \sim$ uniform(0, 4), then $E(X) = 2$ and:
\begin{center} \begin{tabular}{ c|c }
Markov's inequality says... & whereas the actual values are... \\
\hline \\
$P(X \geq 2) \leq \frac{2}{2} = 1$ & $P(X \geq 2) = 0.5$ \\ \\
\hline \\
$P(X \geq 3) \leq \frac{2}{3} = 0.6\Bar{6}$ & $P(X \geq 3) = 0.25$ \\ \\
\hline \\
$P(X \geq 4) \leq \frac{2}{4} = 0.5$ & $P(X \geq 4) = 0$ \\ \\
\hline
\end{tabular} \end{center}

\vspace{0.75cm}



\noindent As a corollary to the Markov inequality we have... \vspace{0.75cm}\\
\noindent \textbf{The Chebyshev inequality.}\label{chebyshevinequality}\\
Let $Y$ be any random variable having mean $\mu_Y$ and finite variance $\sigma_Y^2$. Then, for any constant $k > 0,$
\begin{equation} \label{eq:chebyshev1}
    P(|Y - \mu_Y| \geq k) \leq \frac{\sigma_Y^2}{k^2}
\end{equation}
which is equivalent to
\begin{equation} \label{eq:chebyshev2}
    P(|Y - \mu_k| < k) \geq 1 - \frac{\sigma_Y^2}{k^2}
\end{equation}

\vspace{0.75cm}

\noindent\textbf{Remark.} \\
This inequality roughly says: if a random variable has a small variance, then the probability it takes values far away from its mean is also small.

\vspace{0.5cm}

\noindent\textbf{Remark.} \\
If we replace $k$ by $k\sigma_Y$ in form (\ref{eq:chebyshev1}) and (\ref{eq:chebyshev2}) of the Chebyshev's inequality, we get, respectively,\\
\begin{equation} \label{eq:chebyshev3}
    P(|Y - \mu_Y| \geq k\sigma_Y) \leq \frac{1}{k^2}
\end{equation}
which is equivalent to
\begin{equation} \label{eq:chebyshev4}
    P(|Y - \mu_Y| < k\sigma_Y) \geq 1 - \frac{1}{k^2}.
\end{equation}
Inequalities (\ref{eq:chebyshev1}), (\ref{eq:chebyshev2}), (\ref{eq:chebyshev3}), and (\ref{eq:chebyshev4}) are the different forms of Chebyshev inequalities.  In the form (\ref{eq:chebyshev4}), the inequality provides an interesting lower bound: this inequality says that the probability an rv takes values {\em within} $k$ standard deviations from its mean is {\em at least} $1-\frac 1{k^2}$. So, in particular, there's a probability at least $\frac 34$ an rv takes values within 2 standard deviations of its mean and a probability at least $\frac 89$ it takes values within 3 standard deviations of its mean, etc.\\

\vspace{0.75cm}

\noindent\textbf{Proof.} \\
Let $Y$ be any rv with finite mean $\mu_Y$ and finite variance $\sigma_Y$ and let $k > 0$. Then
\begin{eqnarray*}
    P(|Y - \mu_Y| \geq k) &=& P(|Y - \mu_Y|^2 \geq k^2) \quad \text{since }(|Y - \mu_Y|\geq k)=(|Y - \mu_Y|^2\geq k^2\\
    & \leq&  \frac{E(|Y - \mu_k|^2)}{k^2} \quad \text{by Markov's inequality}\\
    &=& \frac{\sigma_Y^2}{k^2}.
\end{eqnarray*}


\newpage

\noindent An interesting application is the weak law of large numbers (WLLN).\\

\noindent {\bf The weak law of large numbers.}\label{wlln}\\
Suppose $X_1, X_2, X_3, \dots$ are i.i.d. each with mean $\mu$. Set $S_n:=\sum_{i=1}^nX_i$ and let $\varepsilon > 0$ be arbitrary. Then
$$\lim_{n\to\infty} P\left(\left|\frac{S_n}{n}  - \mu\right| < \varepsilon\right) = 1.$$
This theorem says: for any fixed tolerance $\epsilon > 0$, the probability that the sample mean is within $\varepsilon$ of $\mu$ approaches 1 as the sample size $n$ tends to $+\infty$.
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.75\linewidth]{bivariate_normal_WLLN_figs/WLLN.jpg}
%\end{figure}

\vskip .4 in

\noindent\textbf{Remark.} (Paraphrasing the summarizing sentence above.) \\
Since $S_n/n$ is really the sample mean of an iid sample $X_1,X_2,\dots,X_n$, another way to think about the WLLN is as follows:
\begin{center}{\em For any $\varepsilon > 0$ and $\delta > 0$, there exists an integer $N$ such that for all $n \geq N$,
$$P\left(\left|\frac{S_n}{n} - \mu\right| < \varepsilon\right) \geq 1 - \delta$$}\end{center}

\quad{\em for all $n\ge N$}.\\


\vskip .5 in

\includegraphics*[0,0][500,250]{wllnpic1.jpg}

%\vspace{0.75cm}
\newpage

\noindent\textbf{Proof.} \\
For the proof we make the simplifying assumption that the identical distribution admits a finite variance $\sigma^2$ (but the result is true without it).
For each $n$,
let $S_n = \sum_{i=1}^n X_i$, where $X_1,X_2,\dots$ is a sequence of iid rvs with mean $\mu$ and variance $\sigma^2<\infty$. Note that
$$E\left(\frac {S_n}n\right)=\frac 1nE\left(\sum_{i=1}^nX_i\right)=\frac 1n\sum_{i=1}^nE\left(X_i\right)=\frac 1n\sum_{i=1}^n\mu = \mu,$$
and
$$Var\left(\frac{S_n}n\right) = \frac 1{n^2}Var(\sum_{i=1}^nX_i) = \frac 1{n^2}\sum_{i=1}^n Var(X_i) = \frac{1}{n^2} \sum_{i=1}^n \sigma^2 = \frac{n\sigma^2}{n^2}= \frac{\sigma^2}{n}.$$
Now, fix an arbitrary $\varepsilon > 0$. By Chebyshev's inequality (\ref{eq:chebyshev2}) using $k = \varepsilon, Y = \frac {S_n}n$,
$$
    P(|\frac {S_n}n - \mu| < \varepsilon) \geq 1 - \frac{Var\left(\frac {S_n}n\right)}{\varepsilon^2} = 1 - \frac{\sigma^2}{n\varepsilon^2}.
$$
Therefore,
$$
    1 - \frac{\sigma^2}{n\varepsilon^2} \leq P\left(\left|\frac {S_n}n - \mu\right| < \varepsilon\right) \leq 1.
$$
Take the limit as $n$ tends to $+\infty$ by Squeeze Theorem. $\hfill\Box$\\



\newpage



\noindent {\bf Application: Monte-carlo method.*}\label{montecarlomethod}\\

\noindent The Monte-Carlo method is much more general, but we will demonstrate the idea with hopefully a simple example.\\

\noindent\textbf{Example.}\\ \vskip -.25 in

\noindent Estimate $\displaystyle {\cal I} := \int_0^1 g(u) \,du$ for some (possibly complicated) function $g(u)$.
Assume $g$ is bounded by a known constant $M$: $|g(u)| \le M$ for all $u \in [0, 1]$. \\

\noindent SOLUTION: \\
Let $U_1, U_2, U_3, \dots$ be iid uniform(0, 1).  Let $X_i = g(U_i)$. Then $X_1,X_2,X_3,\dots$ are also iid and
$$ E(X_i) = E(g(U_i)) = \int_0^1 g(u) \cdot 1 \,du = \int_0^1 g(u) \,du = {\cal I},$$
and
$$ Var(X_i) \le E(g(U_i)^2) = \int_0^1 {g(u)}^2 \,du \leq M^2.$$
Therefore, the conditions of the weak law of large numbers are satisfied with
$$\frac {S_n}n = \frac{1}{n}\sum_{i=1}^ng(U_i),\quad E\left(\frac {S_n}n\right) = {\cal I},\quad\mbox{and}\quad
Var\left(\frac {S_n}n\right) = \frac{Var\left(g(U_1)\right)}{n} \leq \frac{M^2}{n}.$$
Therefore, for any fixed $\varepsilon > 0$,
$$
P\left(\left|\frac{1}{n}\sum_{i=1}^n g(U_i) - \int_0^1 g(u) \,du \right| < \varepsilon\right) \geq 1 - \frac{Var\left(\frac {S_n}n\right)}{\varepsilon^2} \geq 1 - \underbrace{\frac{M^2}{n\epsilon^2}}_{\delta}
$$
for every $n$.  Now, $\frac {M^2}{n\varepsilon^2}<\delta$ when $n\ge \frac {M^2}{\delta\varepsilon^2}$.  This says if we specify a fix tolerance $\varepsilon >0$ and
a probability $\delta >0$, then $P\left(\left|\frac{1}{n}\sum_{i=1}^n g(U_i) - \int_0^1 g(u) \,du \right| < \varepsilon\right)\ge 1-\delta$ for any $n\ge \frac {M^2}{\delta\varepsilon^2}$. We then estimate ${\cal I}$ by
$$\frac 1n\sum_{i=1}^n g(U_i).$$

\vskip .3 in

\noindent {\bf Remark.}\\
The last example says to estimate ${\cal I}$ just simulate $n$ iid uniform(0,1)'s $U_1,U_2,\dots,U_n$, plug each of them into the function $g$ and then average the result!
We'll be within $\varepsilon$ of the actual value of ${\cal I}$ with a (very high) probability of at least $1-\delta$.\\

\vskip .3 in

\noindent \textbf{Exercise for the student.}\\
Take $g(u) = \frac 1{1+u^2}$, $\varepsilon = \delta = .01$.  Find an $n$ so that $\frac 1n\sum_{i=1}^n \frac 1{1+U_i^2}$ will estimate
${\cal I}=\int_0^1 \frac 1{1+u^2}\,du$ to within $\varepsilon$ with probability at least $.99$. Then use a computer to estimate it. The actual value of ${\cal I}$ is $\frac {\pi}4$. How does it do?\\



\newpage








\noindent {\bf The central limit theorem.}\label{clt}\\


\noindent We showed that iid sequences possessing mean $\mu$ satisfy the Weak Law of Large Numbers\footnote{Our proof of the WLLNs required the distribution possess a variance as well, but, in fact, this is not necessary.}: \\

{\em For every $\varepsilon > 0$,
$$\lim_{n\to\infty} P \left(\left| \frac{1}{n} \sum_{i = 1}^n X_i - \mu \right| < \varepsilon \right) = 1.$$
\quad \ \ Equivalently,\footnote{We usually say $\frac{1}{n} \sum_{i=1}^n X_i$ converges to $\mu$ in probability\label{convergesinprob}
 and write $\frac{1}{n}\sum_{i=1}^n X_i \stackrel{P}{\to} \mu$ as $n \to \infty$.}, for $\varepsilon > 0$,
$$\lim_{n\to\infty} P \left(\left| \frac{1}{n} \sum_{i = 1}^n X_i - \mu \right| \geq \varepsilon \right) = 0.$$}

\noindent We now discuss a refinement of the Weak Law...\\

\noindent \textbf{The Central Limit Theorem (CLT).} \\
Let $X_1, X_2, X_3, \dots$ be an iid sequence of random variables having mean $\mu$ and {\em positive} but finite variance $0<\sigma^2<\infty$ (standard deviation $\sigma$). Let $S_n = \sum_{i=1}^n X_i$ for each $n$. Then,
$$\lim_{n\to\infty} P \left( \frac{S_n - n \mu}{\sigma \sqrt{n}} \leq x \right) = \Phi(x).$$
Equivalently,
$$
\lim_{n\to\infty} P \left( \frac{\frac{S_n}{n} - \mu}{\frac{\sigma}{\sqrt{n}}} \leq x \right) = \Phi(x).
$$

\vskip .5 in


\noindent We will prove this theorem under the additional (simplifying) assumption that the identical distribution possesses a moment generating function.
For the proof we will need the following property about MGFs (stated without proof) as well as some reminders about the properties of the MGF and basic calculus.  For students who do not want to see the proof, you may jump to page \pageref{intuitivemeaningofclt}.\\

\noindent \textbf{The Continuity Theorem of Moment-Generating Functions.} \\
If we have a sequence ($Y_n$) of random variables that each have an MGF and $\lim_{n\to\infty} M_{Y_n}(\theta) = M_X(\theta)$ for all $\theta$ in an open interval containing 0, where $M_X(\theta)$ is a MGF of a rv $X$ having a continuous CDF, then for all real $x$:
$$\lim_{n\to\infty} P(Y_n \leq x) = P(X \leq x).$$
i.e., the distribution of $Y_n$ is converging to the distribution of $X$ as $n$ approaches infinity. \\


\newpage



\noindent All MGFs have the following property: \\

\noindent \textbf{Excercise for the student.} (solution below\dots don't peek until you've attempted this!)\\
If $a$ and $b$ are any constants and $X$ has MGF $M(\theta)$, then the MGF of $aX + b$ is
$$
M_{aX + b}(\theta) = e^{\theta b}M(a \theta).
$$

\vspace{0.75cm}

\noindent \textbf{Corollary.} (The MGF of a standardized rv)\label{mgfstandardizedrv}\\
If $X$ has mean $\mu$ and standard deviation $\sigma$, then $$M_{\frac{X - \mu}{\sigma}}(\theta) = e^{-\frac{\mu\theta}{\sigma}}M\!\!\left(\frac{\theta}{\sigma}\right).$$

\noindent \textbf{Proof.}\\
Take $a=\frac 1{\theta}$ and $b = -\frac {\mu}{\theta}$ in the exercise.\\

\vskip 5.5 in

\noindent \textbf{Solution to Exercise.}

\begin{turn}{180}
$
M_{aX+b}(\theta) = E(e^{\theta[aX+b]}) = E(e^{\theta aX + \theta b}) = E(e^{\theta aX} \cdot e^{\theta b}) = e^{\theta b} \cdot E(e^{(\theta a)X}) = e^{\theta b} M(\theta a).
$
\end{turn}

\newpage



\noindent We need to recall this calculus fact from page \pageref{d:limitrep}:\\

{\em When $c$ is a constant,
$$
\lim_{n\to\infty}\left(1 + \frac{c}{n}\right)^n = e^c.
$$
\quad\ \ In fact,
$$
\lim_{n \to\infty}\left(1 + \frac{c}{n} + o\left(\frac{1}{n}\right)\right)^n = e^c.
$$}

\vskip .1 in

\noindent \textbf{Note}: for the ``little oh" notation go to page \pageref{d:littleoh} of these notes.

%\vspace{0.75cm}

%\noindent\textbf{Example.} \\
%If $c > 1$, then $\frac{1}{n^c}$ is $o\left(\frac{1}{n}\right)$ as $n \to \infty$ since
%$$
%\frac{\frac{1}{n^c}}{\frac{1}{n}} = \frac{1}{n^{c-1}} \rightarrow 0 \text{ as } 0 \to \infty.
%$$
%Consequently,
%$$
%\left(1 + \frac{a}{n} + \frac{1}{n^{\frac{3}{2}}}\right)^n \rightarrow e^a \text{ as } n \to \infty,
%$$
%$$
%\left(1 + \frac{a}{n} + \underbrace{\frac{1}{n^{\frac{3}{2}}} + \frac{10}{n^2} + \frac{50}{n^{\frac{5}{2}}} + \dots}_{= o\left(\frac{1}{n}\right)} \right)^n \rightarrow e^a \text{ as } n \to \infty.
%$$

\vspace{0.75cm}

\noindent Finally, we'll also need the

\noindent {\bf The MacLaurin expansion of the MGF} -- see remark on page \pageref{maclaurinexpofmgf}:\label{maclaurinexpofmgf2}
$$M_X(\theta) = 1 + E(X)\theta + E(X^2)\frac {\theta^2}{2!} + E(X^3)\frac {\theta^3}{3!} + \cdots.$$

\vskip .5 in

\noindent\textbf{Proof of CLT.} \\
Suppose $X_1, X_2, \dots \sim$ iid having MGF $M(\theta)$, $E[X_i] = \mu$ and $E[X_i^2] = \sigma^2 + \mu^2$ for all $i$. Let's find the MGF of
$$
Y_n = \frac{\sum_{i=1}^n X_i - n \mu}{\sigma \sqrt{n}} = \frac{\sum_{i=1}^n (X_i - \mu)}{\sigma\sqrt{n}}.
$$
\begin{eqnarray*}
    M_{Y_n}(\theta) &=& E(e^{\theta Y_n}) = E\left(e^{\theta \cdot \frac{\sum(X_i - \mu)}{\sigma\sqrt{\sigma\sqrt{n}}}}\right) = E\left(\prod_{i=1}^n e^{\frac{\theta}{\sigma\sqrt{n}}(X_i - \mu)}\right) \\
    &=& \prod_{i=1}^n E\left(e^{\frac{\theta}{\sigma\sqrt{n}}X_i} \cdot e^{-\frac{\mu\theta}{\sigma\sqrt{n}}}\right) \qquad\mbox{by independence} \\
    &=& \prod_{i=1}^n e^{-\frac{\mu\theta}{\sigma\sqrt{n}}} \cdot M\left(\frac{\theta}{\sigma\sqrt{n}}\right)\\
    &=& \left[ e^{-\frac{\mu\theta}{\sigma\sqrt{n}}} \cdot M\left(\frac{\theta}{\sigma\sqrt{n}}\right) \right]^n
\end{eqnarray*}

\vspace{0.75cm}

\noindent Let's look closer at the last term...
\begin{align*}
    &e^{-\frac{\mu\theta}{\sigma\sqrt{n}}} \cdot M\!\!\left(\frac{\theta}{\sigma\sqrt{n}}\right) \\
    &= \left(1 - \frac{\mu\theta}{\sigma\sqrt{n}} + \frac{1}{2!} \cdot \frac{(\mu\theta)^2}{\sigma^2 n} + o\left(\frac{1}{n}\right)\right) \cdot \left(1 + \frac{\mu\theta}{\sigma\sqrt{n}} + \frac{1}{2!}\cdot\frac{(\sigma^2 + \mu^2)\theta^2}{\sigma^2 n} + o\left(\frac{1}{n}\right)\right) \\
    &= 1 - \frac{\mu\theta}{\sigma\sqrt{n}} + \frac{1}{2!} \cdot  \frac{\mu^2\theta^2}{\sigma^2 n} + \frac{\mu\theta}{\sigma\sqrt{n}} - \frac{\mu^2\theta^2}{\sigma^2n} + \frac{1}{2!}\cdot \frac{(\sigma^2 + \mu^2)\theta^2}{\sigma^2 n} + o\left(\frac{1}{n}\right) \\
    &= 1 + \frac{\theta^2}{2n} + o\left(\frac{1}{n}\right).
\end{align*}
Substituting this back...
\begin{align*}
    M_{Y_n} (\theta) &= \left[1 + \frac{\theta^2}{2n} + o\left(\frac{1}{n}\right)\right]^n \\
    &= \left[1 + \frac{\frac{\theta^2}{2}}{n} + o\left(\frac{1}{n}\right)\right]^n \rightarrow \ \ e^{\frac{\theta^2}{2}} = M_Z(\theta) \text{ as } n\to\infty.
\end{align*}
Therefore, the result follows by the continuity theorem of MGFs.$\hfill\Box$

\vspace{1cm}



\noindent\textbf{Intuitive meaning of the Central Limit Theorem}.\label{intuitivemeaningofclt}\\
For large $n$,
$$\frac{S_n - n \mu}{\sigma \sqrt{n}}\approx Z\sim\mbox{N}(0,1),$$
where $\approx$ means that the distribution of the rv on the left is approximately the distribution of a standard normal, and the larger the $n$ the better the approximation. (more on this later).  Equivalently,
$$S_n\approx \mbox{N}(n\mu,n\sigma^2).$$
Therefore, if we can recognize a random variable $S_n$ as a
large sum of iid rvs with finite mean and finite positive variance, then
by simply replacing the random variable $S_n$ by a N$(n\mu,n\sigma^2)$ rv we should get a reasonable approximation to the desired probability.

\vskip .5 in

\noindent\textbf{Example.}\\
Suppose $U_1, U_2, \dots, U_{1000} \sim \mbox{iid unif}(0,1)$. Estimate $P(480 \leq U_1 + U_2 + \dots + U_{1000} \leq 520)$.\\

\noindent SOLUTION: Let $S = U_1 + U_2 + \dots + U_{1000}$. Since $E(U_i) = \frac{1}{2}$ and $Var(U_i)=\frac 1{12}$,
\begin{center}$E(S) = 1000 \cdot \frac{1}{2} = 500\quad\mbox{and}\quad Var(S) = 1000\cdot\frac{1}{12} \ ( \sigma_S \approx 9.1287).$\end{center}
Therefore, assuming $n=1000$ is large, $S\approx \mbox{N}(500,\frac {1000}{12})$ and
\begin{eqnarray*}
    P(480 \leq S \leq 520) &\approx & P\left(480 \leq Y \leq 520\right). \\
    & = & P\left(\frac{480-500}{9.1287} \le \frac {Y-500}{\sqrt{\frac {1000}{12}}} \le \frac{520-500}{9.1287}\right) \\
    & = & \Phi\left(\frac{520-500}{9.1287}\right) - \Phi\left(\frac{480-500}{9.1287}\right) \\
    & & \\
    &=& 0.9857 - 0.0143 = 0.9714.
\end{eqnarray*}

\vskip .5 in

\noindent \textbf{Exercise for the student.}\\
Suppose $U_1,U_2,U_3\sim\mbox{iid unif}(0,1)$.  Use the CLT to estimate $P(U_1+U_2+U_3<\frac 12)$.  Try to compute this probability exactly. How did your approximation do?

\noindent ANSWER: CLT : $P(S<\frac 12)\approx .0228$, Actual: $\int_0^{\frac 12}\int_0^{\frac 12-x}\int_0^{\frac 12-x-y}1\,dzdydx = \frac 1{48}\approx .0208$. \\
Seems good!


\newpage



\noindent Following up with the last example\dots\\

\noindent\textbf{Example.}\\
Estimate the probability that you need at least 49 uniform(0,1)'s to see $S_n$ exceed 28.\\


\noindent SOLUTION:\\
\noindent The event of interest says that $U_1 + \dots + U_{48}\le 28$, which is equivalent to saying we'll need at least 49 observations to exceed 28.
Since $E(S) = 48 \cdot \frac{1}{2} = 24$, $Var(S) = 48 \cdot \frac{1}{12} = 4$, and $\sigma_S = 2$.
\begin{eqnarray*}
    P(U_1 + U_2 + \dots + U_{48} \leq 28) &=& P(S \leq 28) \\
    &=& P\left(\frac{S - \mu_S}{\sigma_S} \leq \frac{28-24}{2}\right) \\
    &\stackrel{\text{CLT}}{\approx}& \Phi(2) = 0.9772.
\end{eqnarray*}$\hfill\Box$

\vskip .4 in

\noindent \textbf{Example.}(\textbf{The Chi-squared distribution with $n$ degrees of freedom})\label{chisquare2}\\
Let $n \geq 1$ be an integer. Suppose $X \sim X_n^2$, i.e. $X$ has a Chi-squared distribution with $n$ degrees of freedom.
One can define this as the distribution of
$$
X = Z_1^2 + Z_2^2 + \dots + Z_n^2,
$$
where $Z_1, Z_2, \dots, Z_n \sim$ iid Normal(0,1).
The student should show (exercise) that $X \sim$ Gamma$\left(\frac{n}{2}, 2\right)$ using MGFs.
Therefore,
$$
P\left( \frac{X -n}{\sqrt{2n}} \leq x \right) = P(X \leq n + x\sqrt{2n}) \rightarrow \Phi(x) \text{ as } n \to \infty.
$$

%Note: no continuity correction is needed as the underlying population is already continuous. From a standard normal table, we find $\Phi(1.645) \approx 0.95$.
\noindent Note that this is a limiting result (as $n\to\infty$). Practically, we'd expect for large {\em fixed} $n$ that the value of the probability on the left will be ``close" to $\Phi(x)$.




\newpage


\noindent The last examples had the identical distribution being continuous.  However,
the CLT also works when the identical distribution is discrete.\\

\noindent\textbf{Example.} ({\bf Normal approximation to the binomial})\label{normalapproxtobinomial}\\
If $X_1, X_2, X_3, \dots$ are iid Bernoulli($p$) with $0<p<1$, then $S_n = \sum_{i=1}^n X_i \sim \mbox{binom}(n, p)$.
Moreover, since $E(S_n)=np$ and $Var(S_n)= np(1-p)$ exist and are finite, the CLT guarantees for every $u \in \mathbb{R}$,
$$
P \left(\frac{S_n - np}{\sqrt{np(1-p)}} \leq u\right) \rightarrow \Phi(u) \text{ as } n \to \infty.
$$
This means that for sufficiently large $n$ the two CDFs
will be as close as we'd like\footnote{In the case of the binom$(n,p)$ with $p$ fixed, if $np(1-p)\ge 5$, then the CLT gives decent approximations\label{largenbinomialfootnote},
and the larger $np(1-p)$ is from 5, then better the resulting approximation.}.\\



\noindent Let $S_n\sim \mbox{binom}(n,p)$ with fixed $p$. From footnote$^{\ref{largenbinomialfootnote}}$, if $n$ is such that $np(1-p)\ge 5$ then
$$
\frac{S_n - np}{\sqrt{np(1-p)}} \approx Z \quad \implies \quad S_n \approx n\mu + \sqrt{np(1-p)} Z.
$$

\vspace{0.75cm}
\noindent As a concrete illustration suppose
$S_{20} \sim \mbox{binom}(20, 0.5)$. Then, since $20(.5)(1-.5)=5$, the CLT in this case with $n=20$ should give a ``decent" approximation.
We can try to use the CLT to estimate $P(13 \leq S_{20} \leq 15)$.  Now, $E(S_{20})=10$, $Var(S_{20})=5$, and, therefore, $S_{20}\approx 10 + \sqrt{5}Z$, equivalently, $\frac {S_{20}-10}{\sqrt{5}}\approx Z\sim \mbox{N}(0,1)$:
\begin{eqnarray*}
    P(13 \leq S_{20} \leq 15) &=& P\left(\frac{13 - 10}{\sqrt{5}} \leq \frac{S_{20} - 10}{\sqrt{5}} \leq \frac{15-10}{\sqrt{5}}\right) \\
    &\approx& P(1.34 \leq Z \leq 2.24) \\
    & & \\
    &=& \Phi(2.24) - \Phi(1.34) = 0.9875 - 0.9099 = 0.0776.
\end{eqnarray*}

\vskip .6 in

\noindent \textbf{Remark.}\\
\noindent We should note that, in the last example, we can compute $P(13\le S_{20}\le 15)$ {\em exactly}:
\begin{equation*}
    P(13 \leq S_{20} \leq 15) = \binom{20}{13} \left(\frac{1}{2}\right)^{20} + \binom{20}{14} \left(\frac{1}{2}\right)^{20} + \binom{20}{15} \left(\frac{1}{2}\right)^{20} = 0.125679\dots,
\end{equation*}
and it appears the CLT approximation is not so sharp.  Nevertheless, when dealing with binomial random variables (in fact, {\em any} discrete {\em integer-valued} rv) we may be able to improve upon the result of the CLT by using a so-called {\em continuity correction}\label{continuitycorrection1}.\\


\newpage


\noindent\textbf{Continuity corrections in using CLT for integer-valued random variables.}\label{continuitycorrection2} \\
When the identical distribution is supported by integer values,
the result of the CLT can often be improved by appropriately adding/subtracting 0.5 before invoking the CLT.  We now explain the idea.

The issue that arises in using the CLT for approximating probabilities involving discrete rvs is that the exact probability is
computed by {\em adding probability masses} in the event whereas
the CLT approximation would be {\em integrating probability density} -- i.e., finding area (under a pdf) -- and these two operations are inherently {\em different}!
Nevertheless, when the discrete random variable happens to be integer-valued, it turns out we can reconcile this difference to a great degree because, in this case, adding probability masses can be ``nicely" converted into an area problem.  See the picture below.

\includegraphics*[-40,0][500,180]{contcorrpic1.jpg}


\noindent From the CLT, $S_{20}\approx \mbox{N}(10,5)$ but the $\mbox{N}(10,5)$ pdf will not perfectly interpolate the actual pmf values.  The actual value
of $P(13\le S_{20}\le 15)$ is the sum of the 3 masses/heights at 13, 14, and 15. These heights are typically {\em unknown} but are being approximated by the pdf.
The actual probability can now be interpreted as the area within the 3 red rectangles of width 1 centered at the values we are interested in:
since these rectangles have width 1, each rectangle has area exactly equal
to the pmf value there.  From the picture it now seems a bit clearer that area under the pdf between $12.5$ and $15.5$ does a better job at estimating the area of these 3 red rectangles than had we found the area under the pdf between $13$ and $15$.  This is the essence of the idea behind the {\bf\em 0.5 continuity correction}.\\

\bigskip



\noindent Continuing with the last example, we try to improve upon the na\"{i}ve CLT by making a .5 continuity correction before invoking the CLT.\\

\noindent {\bf Example (continued).}\\
\noindent Since $S_{20}$ is an integer-valued rv,
\begin{eqnarray*}
P(13\le S_{20}\le 15) & = & P(12.5\le S_{20}\le 15.5)\\
& = & P\left( \frac {12.5-10}{\sqrt{5}} \le \frac {S_{20}-10}{\sqrt{5}} \le \frac {15.5-10}{\sqrt{5}}\right)\\
& \stackrel{\text{CLT}}{\approx} & P(1.11 \le Z\le 2.46) = \Phi(2.46)-\Phi(1.11) = .1266,
\end{eqnarray*}
which compares quite favorably to the actual value of $.125679\dots$.




\newpage






\noindent {\bf Example.}\\
Suppose $S\sim \mbox{binom}(100,.2)$.  Use the CLT to estimate $P(S=20)$.\\

\noindent SOLUTION:  First note that $Var(S)= 100(.2)(.8)=16 \ge 5$ so the CLT should provide a decent approximation. In fact, $S\approx \mbox{N}(20,16)$.
Na\"ively, without a continuity correction, $P(S=20)\approx P(\frac {S-20}{4}=0)\approx P(Z=0)=0$ since $Z$ is a continuous rv whereas the actual
value is $P(S=20) = {100\choose 20}.2^{20}.8^{80}= .0993\dots$.

A continuity correction can offer a great improvement:
\begin{eqnarray*}
P(S=20) &=& P(19.5\le S\le 20.5) \\&=& P(\frac {19.5-20}{4}\le \frac {S-20}{4} \le \frac {20.5-20}{4}) \\
&\stackrel{\text{CLT}}{\approx}&(-.125\le Z\le .125) = .0994.
\end{eqnarray*}$\hfill\Box$


\vskip 1 in


\noindent \textbf{Example.}\\
If $X_n \sim$ Poisson($n$), then we can think of $X_n$ as a sum of iid Poisson($1$) rvs:
$$
X_n = Y_1 + Y_2 + \dots + Y_n,
$$
where $Y_1, Y_2, \dots \sim$ iid Poisson($1$).
$$
E(X_n) = n, Var(X_n) = n \implies \sigma_{X_n} = \sqrt{n}.
$$
With $n=100$, say, the CLT gives
\begin{eqnarray*}
    P(X_{100} \leq 110) &=& P(X_{100} \leq 110.5) \\
    &=& P\left( \frac{X_{100} - \mu_{X_{100}}}{\sigma_{X_{100}}} \leq \underbrace{\frac{110.5 - 100}{10}}_{1.05} \right)\\
    &\stackrel{\text{CLT}}{\approx} & \Phi(1.05) = 0.8531.
\end{eqnarray*}
Note: we employed a continuity correction because the Poisson$(100)$ is an integer-valued rv.
In fact, the exact value can be found in this case (up to rounding error) to be $0.852862652\dots$, and it appears the CLT did a good job here.\\



\newpage


\noindent \textbf{Example.}\\
Estimate the probability we need at least 50 rolls of a fair die to see the sum exceed 180.\\

\noindent SOLUTION:
Let $X_1, X_2, \dots, X_n \sim$ iid discrete uniform$\{1,2,3,4,5,6\}$.\label{discreteuniform4} \\
Set $S_n = X_1 + X_2 + \dots + X_n$. Since $E(X_i)=\frac 72=3.5$, $Var(X_i)=\frac {35}{12}=2.916\overline{6}$, $\sigma_{X_i}=1.707825\dots$,
we have
\begin{equation*}
\mu_S = 49\cdot\frac{7}{2} = 171.5,\quad \sigma_S^2 = 49\cdot\frac{35}{12} = 142.916\dots, \quad \mbox{and}\quad \sigma_S = \sqrt{142.916} \approx 11.955.
\end{equation*}
The event that 50 (or more) rolls are needed to see the sum exceed 180 is the same as saying that
the sum $S_{49}$ is less than or equal to 180.
Therefore, we want to estimate $P(S_{49}\le 180)$. Using a continuity correction (since $S_{49}$ is discrete integer-valued):
\begin{eqnarray*}
    P(S_{49} \leq 180) &=& P(S_{49} \leq 180.5) \\
    &=& P \left( \frac{S - \mu_S}{\sigma_S} \leq \frac{180.5 - 171.5}{11.955} \right) \ \ \stackrel{\text{CLT}}{\approx} \ \  \Phi(0.75) = 0.7734.\\
\end{eqnarray*}


\vskip .5 in

\noindent {\bf Example.}\\
Suppose now we only roll the die 3 times. Use the CLT to estimate the probability the total sum is 9.  Compare your answer with the actual probability.\\

\noindent SOLUTION: $E(S_3)=3\cdot\frac 72=10.5$, $Var(S_3)=3\cdot\frac {35}{12}=8.75$ and $\sigma_{S_3} = 2.958$.
\begin{eqnarray*}
P(S_3=9) & = & P(8.5\le S_3\le 9.5) = P\left(\frac {8.5-10.5}{2.958} \le \frac {S_3-10.5}{2.958} \le \frac {9.5-10.5}{2.958}\right)\\
& \stackrel{\text{CLT}}{\approx} & P( -.68\le Z\le -.34) = .1186.\end{eqnarray*}
The actual probability is $P(S_3=9) = \frac {25}{216}=.1157\dots$ (details omitted).  In this case (even with $n$ as small as 3) the CLT did a pretty good job with the estimation!
This begs the question: can we always expect the CLT to give such a wonderful approximation for $n$ as small as 3?  The answer, of course, is {\em NO}! We will discuss this next, but state here that the reason this approximation worked so well in this example is because the identical distribution happens to be symmetric about its mean.

\includegraphics*[-50,0][300,140]{discreteunif1-6.jpg}

\newpage


\noindent For fixed constants $a$ and $b$ we ask:
\begin{center}
{\em How large does $n$ need to be in order for $\Phi(b) - \Phi(a)$ to be ``close" to $P\left( a < \frac{S_n - n\mu}{\sigma\sqrt{n}} \leq b \right)$?}\end{center}
The answer depends on how symmetric the population distribution is about its mean.  We present the following rules of thumb.\\

\noindent \textbf{Heuristic.}
\begin{itemize}
    \item If the population is symmetric about its mean, the $n$ as small as 10 gives good results (or, as the last example shows, $n$ as small as 3).
    \item If the population is only mildly skewed (or if there is no reason to believe the population is heavily skewed) then $n\ge 30$ usually suffices for the CLT to give decent results, however,\dots
    \item The more skewed the population distribution, the larger $n$ will need to be. For instance, if we have a Bernoulli($p$) population, so that $S_n \sim \mbox{binom}(n, p)$, then (see footnote$^{\ref{largenbinomialfootnote}}$ on page \pageref{largenbinomialfootnote})
    $$n \geq \frac{5}{p(1-p)}$$
    will give a reasonable approximation. Notice, in the case of the binomial, the closer $p$ is to 0 (or to 1) the larger $n$ will need to be; for example, if $p=10^{-6}$ then (by this heuristic) $n$ would need to be over 5 million for the CLT to give a ``decent" approximation.
\end{itemize}

\vskip 1 in

\noindent The following result is usually of more theoretical value than of practical value.  It states the precise rate of convergence of the CDF to $\Phi$.\\

\noindent \textbf{The Berry-Essen Theorem.} \label{berryesseen}(simplified version to fit our statement of the CLT) \\
There exists a universal constant $C$ such that:\\
if $X_1, X_2, \dots$ are iid mean $\mu$, variance $\sigma^2$, and $E(|X_i - \mu|^3) = \tau$, then with $S_n = \sum_{i=1}^n \frac{X_i - \mu}{\sigma}$,
$$E(S_n) = n\mu, Var(S_n) = n\sigma^2, \text{ and}$$
$$
\left| P\left( \frac{S_n - n\mu}{\sigma\sqrt{n}} \leq x \right) - \Phi(x)\right| \leq \frac{C \cdot \tau}{\sigma^3 \sqrt{n}} = \frac{C \cdot E\left(\left|\frac{X_i - \mu}{\sigma}\right|^3\right)}{\sqrt{n}}
$$
for all $x$ and $n$.\\





\newpage

\noindent The following interesting application doesn't {\em directly} follow from the statement of the central limit theorem the way we've stated it and I'll mention why.\\

\noindent \textbf{Application: Stirling's approximation} (see page \pageref{stirling})\\
For large integers $n$,
$$n! = \sqrt{2\pi n}\,n^ne^{-n}(1+o(1))\quad \mbox{as }n\to \infty,$$
which means that the relative error in approximating $n!$ by
$\sqrt{2\pi n}n^ne^{-n}$, namely,
$$\frac {n! - \sqrt{2\pi n}\,n^ne^{-n}}{\sqrt{2\pi n}\,n^ne^{-n}}$$
goes to 0 as $n$ tends to infinity.\\

\noindent Let's consider $P(X_n=n)$, where $X_n\sim \mbox{Poisson}(n)$.

On one hand,
\begin{equation}\label{exactpoissonvalue}P(X_n=n) = e^{-n}\cdot \frac {n^n}{n!},\end{equation}
since we just need to evaluate the Poisson$(n)$ pmf at the value $n$.

On the other hand, by the CLT, $\frac {X_n-n}{\sqrt{n}}\approx Z$ and employing a continuity correction (since $X_n$ is integer-valued):
\begin{eqnarray*}
P(X_n=n) & = & P(n-\frac 12 \le X_n \le n+\frac 12) \ \  = \ \  P(-\frac 1{2\sqrt{n}} \le \frac {X_n-n}{\sqrt{n}} \le \frac 1{2\sqrt{n}}) \\
& \stackrel{\text{CLT}^*}{\approx } & \Phi(\frac 1{2\sqrt{n}}) = \Phi(-\frac 1{2\sqrt{n}}) = \int_{-\frac 1{2\sqrt{n}}}^{\frac 1{2\sqrt{n}}} \frac {e^{-z^2/2}}{\sqrt{2\pi}}\,dz\\
& \approx &  \frac{e^0}{\sqrt{2\pi}} \cdot \frac{1}{\sqrt{n}} = \frac 1{\sqrt{2\pi n}},
\end{eqnarray*}
i.e., by the CLT$^*$,
\begin{equation}\label{approxpoissonvalue} P(X_n=n)\approx \frac 1{\sqrt{2\pi n}}.\end{equation}

\medskip

\noindent Therefore, from expressions (\ref{exactpoissonvalue}) and (\ref{approxpoissonvalue}) we have
$$e^{-n}\cdot \frac {n^n}{n!} = P(X_n=n) \approx \frac 1{\sqrt{2\pi n}},$$
which simplifies to
$$n! \approx \sqrt{2\pi n}\,n^ne^{-n}.$$$\hfill\Box$

\noindent $^*$ The issue with applying the CLT here is that, strictly speaking,
the endpoints need to be fixed constants $a$ and/or $b$, something like this: $a\le \frac {X_n-n}{\sqrt{n}} \le b$, so that the endpoints
remain fixed as $n\to \infty$. However, in this application,
the endpoints are functions of $n$; indeed, we have $-\frac 1{2\sqrt{n}}\le \frac {X_n-n}{\sqrt{n}} \le \frac 1{2\sqrt{n}}$ and this interval
is collapsing to a single point as $n\to\infty$!
Therefore, the CLT, by the way it has been developed,
doesn't directly apply here.  What is needed to make what we did rigorous is something called the {\bf\em local central limit theorem}\label{localclt} which we will not state.



\newpage

\noindent {\bf The CLT implies the WLLN}\label{cltimplieswlln}.\\
Suppose the $X_1,X_2,X_3,\dots$ satisfy the conditions of the central limit theorem, and set $S_n=\sum_{i=1}^n X_i$. Then
\begin{eqnarray*}
    P\left( \left|\frac{S_n}{n} - \mu \right| < \varepsilon \right) &=& P(-n\varepsilon < S_n - n\mu < n\varepsilon) \\
    &=& P\left( \frac{-n\varepsilon}{\sigma\sqrt{n}} < \frac{S_n - n\mu}{\sigma\sqrt{n}} < \frac{n\varepsilon}{\sigma\sqrt{n}}\right) \\
    &=& P\left(\frac{-\varepsilon\sqrt{n}}{\sigma} < \frac{S_n - n\mu}{\sigma\sqrt{n}} < \frac{\varepsilon\sqrt{n}}{\sigma}\right).
\end{eqnarray*}
\noindent Notice that, for any fixed $M > 0$,
$$
\left( -M < \frac{S_n - n\mu}{\sigma\sqrt{n}} < M \right) \subseteq \left( \frac{-\varepsilon\sqrt{n}}{\sigma} < \frac{S_n - n\mu}{\sigma\sqrt{n}} < \frac{\varepsilon\sqrt{n}}{\sigma} \right)
$$
when $n$ is sufficiently large; and, in this case,
\begin{eqnarray*}
    P\left( \left| \frac{S_n}{n} - \mu \right| < \varepsilon \right) &\geq& P\left( -M < \frac{S_n - n\mu}{\sigma\sqrt{n}} < M\right) \\
    & \xrightarrow{\text{CLT}}& \Phi(M) - \Phi(-M) \text{ as } n \to \infty \\
    & \rightarrow& 1 \text{ as } M \to \infty
\end{eqnarray*}
and, we see the CLT implies the WLLN!\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage


\noindent \textbf{Convex functions.}\label{convexfunction}


\noindent Let $-\infty\le a < b\le +\infty$.  A function $h:(a, b) \to \mathbb{R}$ is called \textbf{\emph{convex}} if for every $x, y \in (a, b)$,
$$
h(\lambda x + (1 - \lambda)y) \leq \lambda h(x) + (1 - \lambda)h(y)
$$
for all $0 \leq \lambda \leq 1$.
In other words, a function $h$ is convex if the line segment connecting $h(x)$ and $h(y)$ lies above the graph of the function there (see figure below).



\includegraphics*[-50,0][400,210]{convexfunctiongenericpic.jpg}

\begin{center} {\bf Figure}. Graph of generic convex function:
for every choice of $x$ and $y$, the graph of the line segment connecting $h(x)$ and $h(y)$ {\em lies above} the graph of the function itself.
\end{center}

\vskip .5 in

\noindent Here are pictures of some common convex functions defined on subintervals of $\mathbb{R}$:

\includegraphics*[0,0][400,280]{convexfunctionspic1.jpg}


\newpage

\noindent \textbf{Jensen's Inequality.}\label{jenseninequality}\\
Let $X$ be a random variable and $h(x)$ a convex function (defined on the support of $X$). Then
$$
h(E(X)) \leq E(h(X)).
$$

\bigskip

\noindent We will not present the proof of Jensen's inequality, but, instead, we will illustrate how it follows immediately
from the definition of a convex function in the case of random variables taking only two values.
For example, suppose $X$ takes only the two values $x_1$ and $x_2$, where
$$P(X = x_1) = p\quad\mbox{and}\quad P(X = x_2) = 1-p.$$
Notice that $E(X) = px_1 + (1-p)x_2$
is a convex combination of $x_1$ and $x_2$.
Now, since $h$ is convex, we have
$$h(E(X)) = h(px_1 + (1-p)x_2) \leq p h(x_1) + (1-p) h(x_2)=E[h(X)]$$
by the law of the unconscious statistician.
Therefore, Jensen's inequality follows directly from the definition of a convex function in the case of random variables taking only two values.\\

\vskip .5 in


\noindent\textbf{Example.} \textbf{Finite second moment implies finite first moment.}\\
Let $X$ be any random variable with finite second moment. Since $h: \mathbb{R} \to \mathbb{R}$ defined by $h(x) = x^2$ is convex, Jensen's inequality says
\begin{eqnarray*}
    [E(X)]^2 =: h(E(X)) &\leq& E[h(X)] := E(X^2).
\end{eqnarray*}
Therefore, if $E(X^2)<\infty$, we must also have $[E(X)]^2<\infty$, which implies $E(X)<\infty$.\\

\vskip 1 in

\noindent Not only does a finite second moment imply a finite first moment, but in fact, a {\em much} more general statement is true:\\

\noindent \textbf{Liapounov's inequality.}\label{liapounov}\\
Let $0<p<q$ be real numbers and let $X$ be a random variable.  Then
$$[E(|X|^p)]^{1/p} \le [E(|X|^q)]^{1/q}.$$
That is, a finite $q$th moment implies a finite $p$th moment for any $0<p<q$.\\

\noindent \textbf{Proof.} Left as an \textbf{exercise for the student.} Hint: $h(x)=|x|^{q/p}$ is a convex function. Now apply Jensen's inequality using this $h$ and the
random variable $|X|^p$.

\newpage




\noindent {\bf The strong law of large numbers*}\label{slln}\\

\noindent Let $X_1, X_2, X_3, \dots$ be an iid sequence having finite mean $E(X_i) = \mu.$
The {\bf\em strong law of large numbers} says
\begin{equation} \label{eq:SLLN1}
    P \left( \lim_{n\to\infty} \frac{\sum_{i=1}^n X_i}{n} = \mu \right) = 1,
\end{equation}
or, equivalently, as
\begin{equation} \label{eq:SLLN2}
    P \left( \lim_{n\to\infty} \frac{\sum_{i=1}^n X_i}{n} \neq \mu \right) = 0.
\end{equation}
In words, this result says that, except for an event $N\subseteq \Omega$ of probability zero, if $\omega\in N^c$,
then the {\em infinite sequence} $X_1(\omega), X_2(\omega), X_3(\omega),\dots$ will have the property
$$\lim_{n\to\infty}\frac 1n\sum_{i=1}^n X_i(\omega) = \mu.$$
It is certain that when we observe an infinite sequence of iid rvs that the running average of their values will converge to the mean $\mu$,
the set of sequences that don't have this property has probability zero.

\vskip .5 in

\noindent \textbf{Remark.}\\
Please understand that the event we are dealing with in the strong law of large numbers involves an experiment, i.e., a sample space, that is {\em infinite-dimensional}\,!\\

\vskip .5 in

\noindent \textbf{Exercise for the student.}\\
\noindent Show that the strong law implies the weak law.  The following equivalent forms of the strong law might help:
\begin{equation} \label{eq:SLLN3}
    P \left( \lim_{n\to\infty} \left| \frac{\sum_{i=1}^n X_i}{n} - \mu \right| = 0 \right) = 1,
\end{equation}

\begin{equation} \label{eq:SLLN4}
    P \left( \lim_{n\to\infty} \left| \frac{\sum_{i=1}^n X_i}{n} - \mu \right| > 0 \right) = 0.
\end{equation}

\vskip 1 in



\noindent \textbf{Proof.} We will prove the SLLN under the additional assumption $E(X_i^4) < \infty$.\\
We first prove the result when $E(X_i) = 0$ (i.e. when $\mu = 0$).
Set $S_n = \sum_{i=1}^n X_i$. We want to show
$$
P\left(\lim_{n\to\infty} \frac{S_n}{n} = 0 \right) = 1.
$$
The strategy of the proof is to show that the random variable $\sum_{n=1}^{\infty} \left(\frac{S_n}{n}\right)^4$ has a finite expected value, i.e.,
\begin{equation}\label{eq:SLLN_proof_step}
    E\left(\sum_{n=1}^{\infty} \left(\frac{S_n}{n}\right)^4\right) < \infty.
\end{equation}
From this it would have to follow that $\left(\frac{S_n}{n}\right)^4$ must be finite with probability 1, i.e.,
$$
P\left( \sum_{n=1}^{\infty} \left(\frac{S_n}{n}\right)^4 < \infty\right) = 1;
$$
otherwise, $\sum_{n=1}^{\infty} \left(\frac{S_n}{n}\right)^4$ would be infinite with positive probability and thus cannot have a finite expectation.
Now, since convergent series must have terms that go to zero, it would follow that
$$
P\left( \lim_{n\to\infty} \left(\frac{S_n}{n}\right)^4 = 0 \right)
$$
implying
$$
P\left( \lim_{n\to\infty} \frac{S_n}{n} = 0 \right)
$$
giving our result.\\

\noindent So, we proceed to verify equation (\ref{eq:SLLN_proof_step}): since
$$
\left( \frac{S_n}{n} \right)^4 \geq 0,
$$
we can swap the expected value with the infinite series: (this is the Fubini-Tonelli theorem)
$$
E\left( \sum_{n=1}^{\infty} \frac{S_n^4}{n^4} \right) = \sum_{n=1}^{\infty} \left( \frac{E\left(S_n^4\right)}{n^4} \right).
$$
Now we analyze
$$
S_n^4 = \left(\sum_{i=1}^n X_i\right)\left(\sum_{j=1}^n X_j\right)\left(\sum_{k=1}^n X_k\right)\left(\sum_{l=1}^n X_l\right) = \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n X_i X_j X_k X_l.
$$
This sum will have terms like
$$
\begin{array}{cccc}
    X_1 X_1 X_1 X_1 & X_1 X_1 X_2 X_1 & \dots & X_n X_n X_n X_1 \\
    X_1 X_1 X_1 X_2 & X_1 X_1 X_2 X_2 & \dots & X_n X_n X_n X_2 \\
    X_1 X_1 X_1 X_3 & X_1 X_1 X_2 X_3 & \dots & X_n X_n X_n X_3 \\
    \vdots & \vdots & \ddots & \vdots \\
    X_1 X_1 X_1 X_n & X_1 X_1 X_2 X_n & \dots & X_n X_n X_n X_n
\end{array}
$$
which is in one-to-one correspondence with the order of the indices
$$
\begin{array}{cccc}
    (1, 1, 1, 1) & (1, 1, 2, 1) & \dots & (n, n, n, 1) \\
    (1, 1, 1, 2) & (1, 1, 2, 2) & \dots & (n, n, n, 2) \\
    (1, 1, 1, 3) & (1, 1, 2, 3) & \dots & (n, n, n, 3) \\
    \vdots & \vdots & \ddots & \vdots \\
    (1, 1, 1, n) & (1, 1, 2, n) & \dots & (n, n, n, n)
\end{array}
$$
which is the sample space in rolling an ``$n$-sided die" 4 times. \\

\vspace{0.25cm}

\noindent \textbf{Question: } How many 4-of-a-kinds are there? \\
\textbf{Answer:} There are $n$, namely, $(i,i,i,i)$ for $i=1,2,\dots,n$, and $E(X_i^4) = c_4$. \\

\vspace{0.25cm}

\noindent \textbf{Question: } How many 2 pairs are there? \\
\textbf{Answer:} There are ${n\choose 2}{4\choose 2}=3n(n-1)$, and for $I\ne j$,
$E(X_i^2 X_j^2) = \underbrace{E(X_i^2)}_{c_2} E(X_j^2) = c_2^2. $\\

\vspace{0.25cm}

\noindent \textbf{Question: } How many 3-of-a-kinds are there? \\
\textbf{Answer:} There are ${4\choose 3}n(n-1)=4n(n-1)$, and for $i\ne j$, $E(X_i^3 X_j) = E(X_i^3) \underbrace{E(X_j)}_{=0} = 0. $ \\

\vspace{0.25cm}

\noindent \textbf{Question: } How many exactly one pair with the other two distinct are there? \\
\textbf{Answer:} There are ${4\choose 2}n(n-1)(n-2)$ and for distinct $i,j,k$, $E(X_i^2X_jX_k)=E(X_i^2)E(X_j)E(X_k)=0.$ \\

\vspace{0.25cm}

\noindent \textbf{Question: } How many have all 4 rolls distinct? \\
\textbf{Answer:} There are $n(n-1)(n-2)(n-3)$ and for distinct $i,j,k,l$,
$E(X_i X_j X_k X_l) = E(X_i)E(X_j)E(X_k)E(X_l) = 0. $ \\

\vspace{0.75cm}

\begin{eqnarray*}
    E(S_n^4) &=& E\left( \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n X_i X_j X_k X_l \right) \\
    &=& \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \sum_{l=1}^n E(X_i X_j X_k X_l) \\
    & & \\
    &=& n E(X_1^4) \\
    & & \\
    &  & \quad + 3n(n-1) E(X_1^2)E(X_2^2) \\
    & & \\
    &  & \quad + \underbrace{\binom{4}{3} n(n-1)E(X_1^3)E(X_2)}_{=0} \\
    & & \\
    &  & \quad + \underbrace{\binom{4}{2}n(n-1)(n-2)E(X_1^2)E(X_2)E(X_3)}_{= 0} \\
    & & \\
    &  & \quad + \underbrace{n(n-1)(n-2)(n-3) E(X_1)E(X_2)E(X_3)E(X_4)}_{= 0} \\
    & & \\
    &=&  c_4n + 3c_2^2n(n-1).
\end{eqnarray*}
Thus,
\begin{eqnarray*}
    \frac{E(S_n^4)}{n^4} &=& \frac{c_4n + 3c_2^2n(n-1)}{n^4} \\
    &=& c_4 \cdot \frac{1}{n^3} + 3 c_2^2 \cdot \frac{1}{n^2} - 3 c_2^2 \cdot \frac{1}{n^3}
\end{eqnarray*}

\noindent Finally,
\begin{eqnarray*}
    \sum_{n=1}^{\infty} \frac{E(S_n^4)}{n^4} &=& c_4 \sum_{n=1}^{\infty} \frac{1}{n^3} + 3c_2^2 \sum_{n=1}^{\infty} \frac{1}{n^2} - 3c_2^2\sum_{n=1}^{\infty} \frac{1}{n^3} \\
    & & \\
    & < &\infty \quad \text{ since these are all $p$-series with }p > 1.
\end{eqnarray*}
Therefore, we've shown
$$
P\left( \lim_{n\to\infty} \left| \frac{\sum_{n=1}^{\infty} X_n}{n} \right| = 0 \right) = 1 \text{ when }\mu = 0.
$$

\vskip .2 in

\noindent Now let's suppose $X_1, X_2, X_3, \dots $ are iid with mean $\mu$. Then set $Y_i = X_i - \mu$. \\
Notice $Y_1, Y_2, Y_3, \dots$ are iid with mean $0$. Consequently,
\begin{eqnarray*}
    1 &=& P\left( \lim_{n\to\infty} \frac{\sum_{i=1}^n Y_i}{n} = 0 \right) \\
    &=& P\left( \lim_{n\to\infty} \frac{\sum_{i=1}^n (X_i - \mu)}{n} = 0 \right) \\
    &=& P\left( \lim_{n\to\infty} \frac{\sum_{i=1}^n X_i - n\mu}{n} = 0 \right) \\
    &=& P\left( \lim_{n\to\infty} \frac{\sum_{i=1}^n X_i}{n} = \mu \right),
\end{eqnarray*}
 completing the proof.$\hfill\Box$





\newpage



\noindent \textbf{the $\limsup$ and $\liminf$ events.}\\
\noindent Let $A_1, A_2, A_3, \dots$ be an infinite sequence of events. We define two new events:
$$
\limsup_{n\to\infty} A_n = (A_n ~i.o.) = \bigcap_{k=1}^{\infty} \bigcup_{n=k}^{\infty} A_n,
$$
and
$$
\liminf_{n\to\infty} A_n = (A_n ~ev.) = \bigcup_{k=1}^{\infty} \bigcap_{n=k}^{\infty} A_n.
$$

\vspace{0.75cm}


\noindent Notice that $\omega\in (A_n~i.o.)$ means that for every $k\ge 1$, $\omega\in \bigcup_{n=k}^{\infty} A_n$, which in turn means
for every $k\ge 1$, $\omega$ belongs to at least one of the event $A_k,A_{k+1},A_{k+2},\dots$.  To summarize, $\omega\in (A_n~i.o.)$ means
$\omega$ belongs to infinitely manys of the $A_n$'s.  $i.o.$~stands for {\em infinitely often}.\\

\noindent Similarly, $\omega\in (A_n~ev.)$ means that $\omega$ belongs to at least one of $\bigcap_{n=k}^{\infty} A_n$, which in turn means
for some $k\ge 1$, $\omega$ belongs to {\em all} the events $A_k,A_{k+1},A_{k+2},\dots$.  To summarize, $\omega\in (A_n~ev.)$ means
$\omega$ belongs to all the $A_n$'s from some $n=k$ onward, i.e., $\omega$ belongs to all the $A_n$'s eventually.  $ev.$~stands for {\em eventually}.\\

\vskip .2 in

\noindent {\bf Exercise for the student.}\\
Let $A_1,A_2,A_3,\dots$ be any sequence of events.
Show that $(A_n~ev.)\subseteq (A_n~i.o.).$\\


\vskip .3 in

\noindent\textbf{The Borel-Cantelli Lemma.} \label{borelcantelli}\\
Let $A_1, A_2, \dots$ be a sequence of events.
\begin{enumerate}
    \item If $\sum_{n=1}^{\infty}P(A_n) < \infty,$ then $P(A_n ~i.o.) = 0.$
    \item If $A_1, A_2, A_3, \dots$ are independent, and $\sum_{n=1}^{\infty} P(A_n) = \infty$, then $P(A_n ~i.o.) = 1$.
\end{enumerate}

\vspace{0.75cm}

\noindent\textbf{Proof.}
\begin{enumerate}
    \item Since, for every $k \geq 1$,
    $$ (A_n ~i.o.) \subseteq \cup_{n=k}^{\infty} A_n, $$
    it follows by monotonicity of probability and subadditivity (Boole's inequality)
    $$ P(A_n ~i.o.) \leq P\left( \cup_{n=k}^{\infty} A_n \right) \leq \sum_{n=k}^{\infty}P(A_n). $$
    But, since $\sum_{n=1}^{\infty}P(A_n) < \infty$ it follows
    $$ \sum_{n=k}^{\infty}P(A_n) \to 0 \text{ as } k \to \infty.$$
    Therefore, $P(A_n ~i.o.) = 0.$

    \item $$(A_n ~i.o.)^c = \bigcup_{k=1}^{\infty} \bigcap_{n=k}^{\infty} A_n^c.$$\\
    By continuity of probability, it follows
    $ P\left( (A_n ~i.o.)^c \right) = \lim_{k\to\infty} P\left( \bigcap_{n=k}^{\infty} A_n^c \right). $
    \begin{eqnarray*}
        P\left( \bigcap_{n=k}^{\infty} A_n^c \right) & \leq & P \left( \bigcap_{n=k}^N A_n^c \right) \quad \text{ for any finite }N \geq k. \\
        &=& \prod_{n=k}^N P(A_n^c) \\
        &=& \prod_{n=k}^N (1 - P(A_n)) \\
        &\leq& \prod_{n=k}^N e^{-P(A_n)} \quad\quad (*) \\
        & & \\
        &=& e^{-\sum_{n=k}^N P(A_n)}.
    \end{eqnarray*}
    Note that by the MacLaurin expansion,
    $$ e^{-C} = 1 - C + \frac{C^2}{2!} - \frac{C^3}{3!} + \dots \geq 1 - C \text{ when } 0 \leq C < 1. $$
    So
    $$ 1 - P(A_n) \leq e^{-P(A_n)}, $$
    which allows us to perform step ($*$).\\

    Therefore,
    $$ P\left( \bigcap_{n=k}^{\infty}A_n^c \right) \leq \lim_{N\to\infty}e^{-\sum_{n=k}^N P(A_n)} = 0 \text{ since } \sum_{n=k}^{\infty}P(A_n) = \infty. $$
    Consequently,
    $$ P((A_n ~i.o.)^c) = 0 \text{, or equivalently } P(A_n ~i.o.) = 1.$$
\end{enumerate}$\hfill\Box$

\vskip .5 in

\noindent We immediately have the following\\

\noindent\textbf{Corollary.}\\
If $A_1, A_2, A_3, \dots$ is a sequence of independent events, then either $P(A_n ~i.o.) = 0$ or $1$ according to whether $\sum_{n=1}^{\infty}P(A_n) < \infty$ or $\sum_{n=1}^{\infty}P(A_n) = \infty$, respectively. \\



\newpage



\noindent\textbf{Remark.}\\
What we did earlier in the proof of the SLLN under the assumption of finite 4th moment was to show
$$ \sum_{n=1}^{\infty} \frac{E(S_n^4)}{n^4} < \infty.$$
Assume $\mu = 0$.  By the Markov inequality for every $n$, for small but arbitrary $\varepsilon > 0$,
$$ P\left( \left|\frac{S_n}{n}\right| \geq \varepsilon \right) = P\left( \left|\frac{S_n}{n}\right|^4  \geq \varepsilon^4 \right) \leq \frac{E(S_n^4)}{\varepsilon^4 n^4}. $$
Thus,
$$ \sum_{n=1}^{\infty} P\left( \left|\frac{S_n}{n}\right|  \geq \varepsilon \right) \leq \sum_{n=1}^{\infty} \frac{E(S_n^4)}{\varepsilon^4 n^4} = \frac{1}{\varepsilon^4}\sum_{n=1}^{\infty}\frac{E(S_n^4)}{n^4} < \infty.$$

\bigskip
\noindent Therefore, the Borel-Cantelli Lemma implies, for each integer $j=1,2,3,\dots$,
$$ P\left( \left| \frac{S_n}{n} \right| \geq \frac{1}{j} ~i.o. \right) = 0. $$
The events
$$ D_{j} = \left( \left| \frac{S_n}{n} \right| \geq \frac{1}{j} ~i.o. \right) $$
are increasing as $j$ increases. I.e.
$$ D_1 \subseteq D_2 \subseteq D_3 \subseteq \dots \subseteq \bigcup_{k=1}^{j} D_k \subseteq \dots$$
and each of these has probability $0$. Therefore, by the continuity of probability,
$$ P\left( \bigcup_{j=1}^{\infty} D_{j} \right) = 0. $$
But then this says
$$ P\left( \left|\frac{S_n}{n}\right| > 0 ~i.o.\right) = 0. $$
Equivalently,
$$ P\left( \left|\frac{S_n}{n}\right| = 0 ~ev.\right) = 1, $$
i.e.,
$$ P\left( \lim_{n\to\infty} \left|\frac{S_n}{n}\right| = 0 \right) = 1. $$$\hfill\Box$



























\newpage







\begin{center}{\Large INDEX}\end{center}

\noindent {\bf A}\\
\noindent anagram \pageref{d:anagram}, \pageref{countinganagrams}\\
\noindent arrival time, Poisson process \pageref{arrivaltime}\\
\noindent Axioms of probability \pageref{axioms1}\\

\noindent {\bf B}\\
\noindent Bayes rule \pageref{bayesrule1}\\
\noindent Bernoulli \pageref{e:bernoulli}, \pageref{d:bernoullip}, \pageref{e:bernoullipmean}, \pageref{bernoullipmoments}\\
\noindent Berry-Esseen theorem \pageref{berryesseen}\\
\noindent Beta distribution \pageref{betadistributionexample}\\
\noindent binomial coefficient \pageref{d:binomialcoeff}\\
\noindent binomial distribution \pageref{binomdist1}, \pageref{d:binomialnp}\\
\noindent binomial distribution, expected value of \pageref{e:binomialnpmean}, \pageref{expectedvalueofbinomial}

\quad normal approximation to the binomial \pageref{normalapproxtobinomial}

\noindent bivariate normal \pageref{bivariatenormaldist}, \pageref{bivariatenormal=2variate}\\
\noindent bivariate normal pdf \pageref{bivariatenormalpdf}\\
\noindent Boole's inequality \pageref{subadd1}\\
\noindent Boole-Bonferroni inequalities \pageref{boolebonferroni}\\
\noindent Borel-Cantelli lemma \pageref{borelcantelli}\\


\noindent{\bf C}\\
Calculus facts:

binomial theorem \pageref{d:binomialtheorem}

geometric progression, sums and series \pageref{d:geomseries1}, \pageref{d:geomseries}

Liebniz rules \pageref{liebnizrule}

limit representation for $e^u$ \pageref{d:limitrep}

little oh notation \pageref{d:littleoh}

MacLaurin series for $e^u$ \pageref{d:maclaurinseries1}, \pageref{d:maclaurinseries}

multinomial theorem \pageref{multinomialtheorem}

\noindent Cauchy distribution \pageref{Cauchypdf}, \pageref{Cauchypdf2}\\
\noindent Cauchy--Schwarz inequality \pageref{cauchyschwarz2}\\
\noindent CDF method \pageref{cdfmethod1}\\
\noindent CDF of the $j$th ordered statistic \pageref{cdfofjthorderedstatistic}\\
\noindent CDF of the minimum of iid sample \pageref{cdfofY1}\\
\noindent CDF of the maximum of iid sample \pageref{cdfofYn}\\
\noindent Central Limit theorem (CLT) \pageref{clt}

\quad implies the weak law of large numbers \pageref{cltimplieswlln}

\noindent Chi-square distribution \pageref{chisquare1}, \pageref{chisquare2}\\
\noindent Classical probability measure \pageref{classicalprobmeasure1}, \pageref{classicalprobmeasure2}\\
\noindent combinations \pageref{sec:combinations}\\
\noindent complementary rule \pageref{complementaryrule}\\
\noindent compound random variables \pageref{compoundrv}\\
\noindent conditional expectation \pageref{conditionalexpectation}\\
\noindent conditional pdf \pageref{conditionalpdf}\\
\noindent conditional probability formula \pageref{conditionalprobabilityformula}\\
\noindent conditional variance \pageref{conditionalvariance}\\
\noindent continuity correction \pageref{continuitycorrection1}, \pageref{continuitycorrection2}\\
\noindent Continuity of Probability \pageref{continuityofprobability}\\
\noindent converges in probability \pageref{convergesinprob}\\
\noindent convex function \pageref{convexfunction}\\
\noindent convolution, convolution integral \pageref{convolutionintegral}, \pageref{s:convolutionintegral}\\
\noindent convolution, discrete convolution \pageref{discreteconvolution}\\
\noindent correlation \pageref{correlation}\\
\noindent covariance \pageref{covariance}\\
\noindent covariance, bilinearity \pageref{covbilinear}, \pageref{covbilinearity}\\
\noindent covariance matrix of $k$-variate normal \pageref{covariancematrix}\\
\noindent covariance, properties of \pageref{covarianceproperties}\\
\noindent cumulative distribution function (CDF) \pageref{cdfs}\\

\noindent {\bf D}\\
\noindent decreasing sequence of events (nested) \pageref{nesteddecreasing}\\
\noindent delayed exponential \pageref{delayedexp}\\
\noindent DeMorgan's rules \pageref{demorgan}\\
\noindent dependent events \pageref{dependentevents}\\
\noindent dependent random variables \pageref{dependentrvs}\\
\noindent Dirichlet distribution \pageref{dirichletpdf}\\
\noindent Discrete probability measure \pageref{discreteprobmeasure1}\\
\noindent Discrete rv \ref{e:discreterv},\pageref{d:discreterv}\\
\noindent Distributions:

Bernoulli \pageref{e:bernoulli}, \pageref{d:bernoullip}

Beta \pageref{betadistributionexample}

binomial \pageref{d:binomialnp}

bivariate normal \pageref{bivariatenormaldist}, \pageref{bivariatenormalpdf}, \pageref{bivariatenormal=2variate}

Cauchy \pageref{Cauchypdf}, \pageref{Cauchypdf2}

Chi-square \pageref{chisquare1}, \pageref{chisquare2}

delayed exponential \pageref{delayedexp}

Dirichlet \pageref{dirichletpdf}

discrete uniform \pageref{discreteuniform1}, \pageref{discreteuniform2}, \pageref{discreteuniform3}, \pageref{discreteuniform4}

Erlang \pageref{erlangdist}

exponential \pageref{expdist}

Gamma \pageref{gammapdf}

Gaussian \pageref{normaldist}

geometric \pageref{geomfaircoin}, \pageref{d:geometricp}

hypergeometric \pageref{d:hypergeom}

log-normal \pageref{lognormal}

multinomial \pageref{multinomialdist}

multivariate hypergeomtric \pageref{multivariatehypergeom1}, \pageref{multivariatehypergeom}

multivariate normal \pageref{multivariatenormaldist}, \pageref{kvariatenormalpdf}

negative binomial \pageref{d:negbinom}

normal \pageref{normaldist}

Poisson \pageref{d:poissonlambda}

Polya-Eggenberger \pageref{polyaeggenberger}

standard Normal \pageref{stdnormaldist}

uniform \pageref{d:uniformabdist}

uniform on $D$, where $D\subseteq {\mathbb R}^2$ \quad \pageref{2duniform}

Zeta \pageref{zetadist}\\

\noindent {\bf E}\\
\noindent Erlang distribution \pageref{erlangdist}\\
\noindent Euler's Gamma function \pageref{d:eulergamma}\\
\noindent exchangeability \pageref{exchangeability1}, \pageref{exchangeability2}, \pageref{exchangeability3}\\
\noindent exchangeable events \pageref{exchangeableevents}\\
\noindent expectation, expectation value \pageref{expectationvalue}\\
\noindent expected value \pageref{d:expectedvaluediscrete}\\
\noindent experiment \pageref{d:experiment}\\

\noindent {\bf F}\\
\noindent factorial \pageref{factorial}\\
\noindent falling factorial \pageref{fallingfactorial}\\
\noindent Fibonacci sequence \pageref{fibonacci}\\
\noindent finite population correction \pageref{finitepopulationcorrection}\\
\noindent functional form of pmf \pageref{e:functionalformpmf}, \pageref{pmffunctionalform}\\

\noindent {\bf G}\\
\noindent Gamma distribution \pageref{gammapdf}\\
\noindent Gamma function \pageref{d:eulergamma}\\
\noindent Gamma scale parameter \pageref{gammascale}\\
\noindent Gamma shape parameter \pageref{gammashape}\\
\noindent Gaussian distribution \pageref{normaldist}\\
\noindent geometric distribution \pageref{geomfaircoin}, \pageref{d:geometricp}\\
\noindent geometric series \pageref{d:geomseries1}, \pageref{d:geomseries}\\
\noindent geometric progression \pageref{d:geomprog}\\
\noindent geometric ratio \pageref{d:geomratio}\\

\noindent {\bf H}\\
\noindent hazard rate function \pageref{hazardratefunction}\\
\noindent hypergeometric \pageref{hypergeometric1}, \pageref{d:hypergeom}\\
\noindent hypergeometric, expected value of \pageref{expectedvalueofhypergeometric}\\

\noindent {\bf I}\\
\noindent iid: independent and identically distributed \pageref{iiddef}\\
\noindent image of a random variable \pageref{d:imageofrv},\pageref{f:imagepreimage}\\
\noindent inclusion-exclusion rules \pageref{inclusionexclusionrules}\\
\noindent increasing sequence of events (nested) \pageref{nestedincreasing}\\
\noindent independence of 2 events \pageref{d:2eventsindependent}\\
\noindent independence of 3 events \pageref{d:3eventsindependent}\\
\noindent independent events \pageref{d:independentevents}\\
\noindent independent random variables \pageref{independentrvs}\\
\noindent index of an element in a multiset \pageref{d:index}\\
\noindent indicator function \pageref{indicatorfunction}\\
\noindent Inequalities:

\noindent \quad Cauchy--Schwarz \pageref{cauchyschwarz}, \pageref{cauchyschwarz2}

\noindent \quad Chebyshev \pageref{chebyshevinequality}

\noindent \quad Jensen \pageref{jenseninequality}

\noindent \quad Liapounov \pageref{liapounov}

\noindent \quad Markov \pageref{markovinequality}

\noindent inverse image \pageref{d:inverseimage}\\

\noindent {\bf J}\\
\noindent Jacobian \pageref{jacobian},\ \pageref{jacobian2}\\
\noindent Jensen's inequality \pageref{jenseninequality}\\
\noindent jointly continuous \pageref{jointlycontinuous}\\
\noindent jointly distributed rvs \pageref{jointlydistributed}\\
\noindent joint probability density function (joint pdf) \pageref{jointpdf}\\
\noindent joint probability mass function (joint pmf) \label{jointpmf}\\

\noindent{\bf L}\\
\noindent Law of the Unconscious Statistician (LOTUS)-discrete rv case \pageref{lotus}\\
\noindent Law of the Unconscious Statistician (LOTUS)-continuous rv case \pageref{lotuscontinuous}\\
\noindent Law of total expectation \pageref{lawoftotalexpectation}\\
\noindent Law of total probability \pageref{lotp}\\
\noindent Liapounov's inequality \pageref{liapounov}\\
\noindent Liebniz rule \pageref{liebnizrule}\\
\noindent likelihoods (Bayes rule) \pageref{likelihoodsbayes}\\
\noindent local central limit theorem, local CLT \pageref{localCLT}\\
\noindent log-normal distribution \pageref{lognormal}\\

\noindent {\bf M}\\
\noindent MacLaurin series \pageref{d:maclaurinseries1}, \pageref{d:maclaurinseries}\\
\noindent marginal pmf \pageref{marginalpmf}\\
\noindent Markov inequality \pageref{markovinequality}\\
\noindent mean, mean value \pageref{mean}\\
\noindent mean vector \pageref{meanvector}\\
\noindent median, population \pageref{populationmedian}\\
\noindent median, sample \pageref{samplemedian}\\
\noindent memoryless property \pageref{memorylessexp}\\
\noindent Method of Jacobian \pageref{methodofjacobians}\\
\noindent moment generating function (MGF) \pageref{d:mgf}

MacLaurin expansion of MGF \pageref{maclaurinexpofmgf}, \pageref{maclaurinexpofmgf2}

\noindent moments of a rv/distribution \pageref{moments}\\
\noindent monotonicity of probability measure \pageref{monotonicity}\\
\noindent Monte-Carlo method \pageref{montecarlomethod}\\
\noindent multinomial coefficient \pageref{multinomialcoeff2}\\
\noindent multiplicative rule of conditional probability \pageref{multrulecondprob}\\
\noindent multiplicity (in a multiset) \pageref{d:multiplicity}\\
\noindent multisets \pageref{multisets1}, \pageref{d:multiset}\\
\noindent multinomial distribution \pageref{multinomialdist}\\
\noindent multinomial theorem \pageref{multinomialtheorem}\\
\noindent multivariate hypergeometric distribution \pageref{multivariatehypergeom1}, \pageref{multivariatehypergeom}\\
\noindent multivariate normal distribution, pdf \pageref{multivariatenormaldist}, \pageref{kvariatenormalpdf}\\

\noindent {\bf N}\\
\noindent negative binomial \pageref{d:negbinom}\\
\noindent no-clumping criterion \pageref{noclumping}\\
\noindent normal approximation to the binomial \pageref{normalapproxtobinomial}\\
\noindent normal distribution \pageref{normaldist}\\

\noindent {\bf O}\\
\noindent ordered statistics \pageref{orderstatistics}\\
\noindent \quad bivariate marginal pdf \pageref{bivariatemarginalYiYj}\\
\noindent \quad joint pdf of $Y_1,Y_2,\dots,Y_n$ \pageref{jointpdfoforderedstatistics}\\
\noindent \quad univariate pdf of $Y_j$ \pageref{pdfofYj}\\

\noindent {\bf P}\\
\noindent parallel, components hooked-up in \pageref{seriesparallel}\\
\noindent Pascal's identity \pageref{pascalsidentity}\\
\noindent pdf \pageref{pdf}\\
\noindent pdf of minimum, maximum ordered statistics \pageref{pdfofminandmaxofiid}\\
\noindent pdf of $j$th ordered statistic \pageref{pdfofYj}\\
\noindent Poisson \pageref{poissoncalcfacts}, \pageref{d:poissonlambda}, \pageref{poissonpmf}\\
\noindent Poisson assumptions \pageref{poissonassumptions}\\
\noindent posterior probabilities \pageref{posteriorprob}\\
\noindent Poisson process \pageref{poissonprocess}\\
\noindent Poisson process arrival time \pageref{arrivaltime}\\
\noindent Polya-Eggenberger distribution \pageref{polyaeggenberger}\\
\noindent Polya's urn \pageref{polyaurn}\\
\noindent preimage \pageref{d:preimage}, \pageref{f:imagepreimage}, \pageref{e:preimage}\\
\noindent prior probabilities \pageref{priorprob}\\
\noindent probability density function (pdf) \pageref{pdf}\\
\noindent probability mass function (pmf) \pageref{e:pmf},\pageref{d:pmf}\\
\noindent properties of conditional expectation \pageref{propertiesofcondexp}\\

\noindent {\bf R}\\
\noindent random variable \pageref{d:rv}\\
\noindent reduction formula for Gamma function \pageref{reductionformula}\\
\noindent Riemann zeta function \pageref{riemannzeta}\\

\noindent {\bf S}\\
\noindent sample median \pageref{samplemedian}\\
\noindent sample point \pageref{d:samplepoint}\\
\noindent sample space \pageref{d:samplespace}\\
\noindent sampling with replacement \pageref{sec:samplingwithreplacement}\\
\noindent sampling without replacement \pageref{sec:permutations}, \pageref{samplingwithoutreplacement2}\\
\noindent scale parameter - Gamma \pageref{gammascale}\\
\noindent series, components hooked-up in \pageref{seriesparallel}\\
\noindent shape parameter - Gamma \pageref{gammashape}\\
\noindent spacings \pageref{spacings}\\
\noindent specifying sets \pageref{specify}\\
\noindent standard Normal distribution \pageref{stdnormaldist}\\
\noindent standard Normal table \pageref{standardnormaltable}\\
\noindent stars and bars counting \pageref{starsandbars1}\\
\noindent Stirling's approximation \pageref{stirling}\\
\noindent strictly monotone \pageref{strictlymonotone}\\
\noindent Strong law of large numbers (SLLN) \pageref{slln}\\
\noindent Subadditivity of probability measures (Boole's inequality) \pageref{subadd1}
\noindent support \pageref{d:pmfsupport}\\
\noindent survival function \pageref{survivalfunction}\\
\noindent symmetric function \pageref{symmetricfunction}\\

\noindent {\bf T}\\
\noindent tabular form \pageref{e:tabularform}, \pageref{pmftabularform}\\

\noindent {\bf U}\\
\noindent uncorrelated \pageref{uncorrelated}\\
\noindent Uniform distribution - uniform$(a,b)$, \pageref{d:uniformabdist}\\
\noindent Uniform distribution - uniform$(D)$, where $D\subseteq {\mathbb R}^2$ \quad \pageref{2duniformexample1}, \pageref{2duniform}\\
\noindent uniformly at random \pageref{uniformlyatrandom1}\\
\noindent unit exponential \pageref{unitexp}\\

\noindent {\bf V}\\
\noindent Variance \pageref{variance}, \pageref{variancecomputation}\\

\noindent {\bf W}\\
\noindent Weak law of large numbers (WLLN) \pageref{wlln}

\quad implied by the CLT \pageref{cltimplieswlln}

\noindent Weibull distribution \pageref{weibulldist}\\

\noindent {\bf Z}\\
\noindent Zeta distribution \pageref{zetadist}\\
\noindent $z$-score \pageref{d:z-score}, \pageref{zscore2}\\



\end{document}
